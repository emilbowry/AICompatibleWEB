{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1d20da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "embedding_models = {\n",
    "\t\"Legacy_1\": \"models/embedding-001\",\n",
    "\t\"Legacy_2\": \"models/text-embedding-004\",\n",
    "\t\"Current\": \"models/gemini-embedding-001\",\n",
    "}\n",
    "\n",
    "chat_models = {\n",
    "\t\"FLASH_LATEST\": \"gemini-2.5-flash-preview-09-2025\",\n",
    "\t\"PRO\": \"gemini-2.5-pro\",\n",
    "}\n",
    "\n",
    "\n",
    "class GeminiModel:\n",
    "\tDEFAULT_CHAT_MODEL = chat_models[\"FLASH_LATEST\"]\n",
    "\tDEFAULT_EMBEDDING_MODEL = embedding_models[\"Current\"]\n",
    "\n",
    "\tdef __init__(self, *, api_key=None):\n",
    "\t\tif api_key is None:\n",
    "\t\t\tload_dotenv()\n",
    "\t\t\tGOOGLE_API_KEY = self.getEnvAPIKey()\n",
    "\t\telse:\n",
    "\t\t\tGOOGLE_API_KEY = api_key\n",
    "\t\tself.client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "\tdef getEnvAPIKey(self):\n",
    "\t\tload_dotenv()\n",
    "\t\treturn os.environ.get(\"GEMINI_API_KEY\")\n",
    "\n",
    "\tdef startChat(self, model_name=None):\n",
    "\n",
    "\t\tif model_name is None:\n",
    "\t\t\tmodel_name = self.DEFAULT_CHAT_MODEL\n",
    "\n",
    "\t\tchat = self.client.chats.create(model=model_name)\n",
    "\t\treturn chat\n",
    "\n",
    "\tdef send_prompt(self, chat, prompt):\n",
    "\t\tresponse = chat.send_message(prompt)\n",
    "\t\treturn response.text\n",
    "\n",
    "\tdef getSemanticEmbedding(self, semantic_datum, *, model_name=None, task_type=None):\n",
    "\t\tif task_type is None:\n",
    "\t\t\t# task_type = \"SEMANTIC_SIMILARITY\"\n",
    "\t\t\ttask_type = \"FACT_VERIFICATION\"\n",
    "\t\tif model_name is None:\n",
    "\t\t\tmodel_name = self.DEFAULT_EMBEDDING_MODEL\n",
    "\t\tresult = self.client.models.embed_content(\n",
    "\t\t\tmodel=model_name,\n",
    "\t\t\tcontents=semantic_datum,\n",
    "\t\t\tconfig=types.EmbedContentConfig(task_type=task_type),\n",
    "\t\t)\n",
    "\t\treturn result.embeddings[0].values\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef preprocessStatement(statement):\n",
    "\t\tprefix = \"Does the privacy policy affirm\"\n",
    "\t\treplacement = \"The privacy policy affirms\"\n",
    "\t\tprocessed_text = (\"The privacy policy affirms\" + statement[len(prefix) :])[:-1] + \".\"\n",
    "\n",
    "\t\treturn processed_text\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef preprocessStatement_alt(statement):\n",
    "\t\tprefix = \"Does the privacy policy affirm that \"\n",
    "\t\tprocessed_text = (statement[len(prefix) :].capitalize())[:-1] + \".\"\n",
    "\t\treturn processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4327070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Qs:\n",
      "---\n",
      "Does the privacy policy affirm that personal data transfers are automatically consented to by using the service?\n",
      "Does the privacy policy affirm that user data transfers are automatically consented to by using the service?\n",
      "Does the privacy policy affirm that personal data processing is automatically consented to by using the service?\n",
      "---\n",
      "\n",
      "Preprocessing A:\n",
      "---\n",
      "The privacy policy affirms that personal data transfers are automatically consented to by using the service.\n",
      "The privacy policy affirms that user data transfers are automatically consented to by using the service.\n",
      "The privacy policy affirms that personal data processing is automatically consented to by using the service.\n",
      "---\n",
      "\n",
      "Preprocessing B:\n",
      "---\n",
      "Personal data transfers are automatically consented to by using the service.\n",
      "User data transfers are automatically consented to by using the service.\n",
      "Personal data processing is automatically consented to by using the service.\n",
      "---\n",
      "\n",
      "Original Q comparison:\n",
      "---\n",
      "1 vs 1 prime: 0.9608952811844776\n",
      "1 vs 2: 0.8984818644348227\n",
      "delta = 0.06241341674965495\n",
      "---\n",
      "\n",
      "Preprocessing A comparison:\n",
      "---\n",
      "statement 1 vs 1 prime: 0.9725804679510963\n",
      "statement 1 vs 2: 0.9169212399341644\n",
      "delta = 0.055659228016931905\n",
      "---\n",
      "\n",
      "Preprocessing B comparison:\n",
      "---\n",
      "statement 1 vs 1 prime: 0.9619928913981085\n",
      "statement 1 vs 2: 0.8990657574417134\n",
      "delta = 0.06292713395639515\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "question_1 = \"Does the privacy policy affirm that personal data transfers are automatically consented to by using the service?\"\n",
    "question_1_prime = \"Does the privacy policy affirm that user data transfers are automatically consented to by using the service?\"\n",
    "question_2 = \"Does the privacy policy affirm that personal data processing is automatically consented to by using the service?\"\n",
    "print(\"Original Qs:\")\n",
    "print(\"---\")\n",
    "print(question_1)\n",
    "print(question_1_prime)\n",
    "print(question_2)\n",
    "print(\"---\")\n",
    "print()\n",
    "\n",
    "s_1 = GeminiModel.preprocessStatement(question_1)\n",
    "s_1_p = GeminiModel.preprocessStatement(question_1_prime)\n",
    "s_2 = GeminiModel.preprocessStatement(question_2)\n",
    "print(\"Preprocessing A:\")\n",
    "print(\"---\")\n",
    "print(s_1)\n",
    "print(s_1_p)\n",
    "print(s_2)\n",
    "print(\"---\")\n",
    "print()\n",
    "\n",
    "\n",
    "s_1_2 = GeminiModel.preprocessStatement_alt(question_1)\n",
    "s_1_p_2 = GeminiModel.preprocessStatement_alt(question_1_prime)\n",
    "s_2_2 = GeminiModel.preprocessStatement_alt(question_2)\n",
    "\n",
    "print(\"Preprocessing B:\")\n",
    "print(\"---\")\n",
    "print(s_1_2)\n",
    "print(s_1_p_2)\n",
    "print(s_2_2)\n",
    "print(\"---\")\n",
    "print()\n",
    "\n",
    "model = GeminiModel()\n",
    "\n",
    "\n",
    "def process_sim(emb_a, emb_b):\n",
    "\tarr_1 = np.array(emb_a)\n",
    "\tarr_2 = np.array(emb_b)\n",
    "\tdot_product = np.dot(arr_1, arr_2)\n",
    "\tnorm1 = np.linalg.norm(arr_1)\n",
    "\tnorm2 = np.linalg.norm(arr_2)\n",
    "\tsimilarity_score = dot_product / (norm1 * norm2)\n",
    "\treturn similarity_score\n",
    "\n",
    "\n",
    "task_type = None\t# Preprocessing B comparison 0.028234191625243987, however None give 1,1_prime of 99.2%\n",
    "# task_type = \"QUESTION_ANSWERING\"\t# Preprocessing B  0.06292713395639515 , similar to None = 0.06241341674965495\n",
    "# task_type = (\n",
    "# \t\"FACT_VERIFICATION\"\t# Preprocessing B 0.06857297559057307, None  0.04625901551552625\n",
    "# )\n",
    "# # task_type = \"CLASSIFICATION\"\t# Preprocessing B 0.051221128737101496, None\n",
    "# # task_type = \"CLUSTERING\"\t# B 0.02915895877534147, A is very bad 0.009454020561961651\n",
    "# # task_type = \"RETRIEVAL_DOCUMENT\"\t# None  0.03241810464957673\n",
    "# task_type = \"RETRIEVAL_QUERY\"\t# B  0.062026106845692985\n",
    "q_1_embed = model.getSemanticEmbedding(question_1, task_type=task_type)\n",
    "q_1_prime_embed = model.getSemanticEmbedding(question_1_prime, task_type=task_type)\n",
    "q_2_embed = model.getSemanticEmbedding(question_2, task_type=task_type)\n",
    "\n",
    "sim_a = process_sim(q_1_embed, q_1_prime_embed)\n",
    "sim_b = process_sim(q_1_embed, q_2_embed)\n",
    "print(\"Original Q comparison:\")\n",
    "print(\"---\")\n",
    "\n",
    "print(f\"1 vs 1 prime: {sim_a}\")\n",
    "print(f\"1 vs 2: {sim_b}\")\n",
    "print(f\"delta = {sim_a-sim_b}\")\n",
    "print(\"---\")\n",
    "print()\n",
    "\n",
    "print(\"Preprocessing A comparison:\")\n",
    "print(\"---\")\n",
    "\n",
    "s_1_embed = model.getSemanticEmbedding(s_1, task_type=task_type)\n",
    "s_1_prime_embed = model.getSemanticEmbedding(s_1_p, task_type=task_type)\n",
    "s2_embed = model.getSemanticEmbedding(s_2, task_type=task_type)\n",
    "sim_c = process_sim(s_1_embed, s_1_prime_embed)\n",
    "sim_d = process_sim(s_1_embed, s2_embed)\n",
    "print(f\"statement 1 vs 1 prime: {sim_c}\")\n",
    "print(f\"statement 1 vs 2: {sim_d}\")\n",
    "print(f\"delta = {sim_c-sim_d}\")\n",
    "print(\"---\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Preprocessing B comparison:\")\n",
    "print(\"---\")\n",
    "s_1_embed_2 = model.getSemanticEmbedding(s_1_2, task_type=task_type)\n",
    "s_1_prime_embed_2 = model.getSemanticEmbedding(s_1_p_2, task_type=task_type)\n",
    "s2_embed_2 = model.getSemanticEmbedding(s_2_2, task_type=task_type)\n",
    "sim_c_2 = process_sim(s_1_embed_2, s_1_prime_embed_2)\n",
    "sim_d_2 = process_sim(s_1_embed_2, s2_embed_2)\n",
    "print(f\"statement 1 vs 1 prime: {sim_c_2}\")\n",
    "print(f\"statement 1 vs 2: {sim_d_2}\")\n",
    "print(f\"delta = {sim_c_2-sim_d_2}\")\n",
    "print(\"---\")\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
