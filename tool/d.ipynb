{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "693fb80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading Data...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from itertools import product\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import rankdata, expon, beta\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. DATA LOADING\n",
    "# ==============================================================================\n",
    "\n",
    "QUESTIONS_FILE = \"./data/questions_filter_after.json\"\n",
    "\n",
    "\n",
    "def _loadJson(filepath):\n",
    "\tif not os.path.exists(filepath):\n",
    "\t\tprint(f\"Warning: File not found: {filepath}\")\n",
    "\t\treturn {}\n",
    "\ttry:\n",
    "\t\twith open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "\t\t\treturn json.load(f)\n",
    "\texcept json.JSONDecodeError:\n",
    "\t\tprint(f\"Error decoding JSON: {filepath}\")\n",
    "\t\treturn {}\n",
    "\n",
    "\n",
    "print(\">>> Loading Data...\")\n",
    "qdata = _loadJson(QUESTIONS_FILE)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DISTANCE & ARTIFACT PREPARATION\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def getMahalanobisDistances(vectors_a, vectors_b):\n",
    "\tnorms = np.linalg.norm(vectors_a, axis=1, keepdims=True)\n",
    "\tcleaned_vectors = vectors_a / (norms + 1e-10)\n",
    "\n",
    "\tlw = LedoitWolf()\n",
    "\tlw.fit(cleaned_vectors)\n",
    "\tprecision_matrix = lw.precision_\n",
    "\n",
    "\tdist_matrix = cdist(\n",
    "\t\tcleaned_vectors, cleaned_vectors, metric=\"mahalanobis\", VI=precision_matrix\n",
    "\t)\n",
    "\treturn dist_matrix, precision_matrix\n",
    "\n",
    "\n",
    "Distance_Processors = {\n",
    "\t\"cosine\": lambda emb_a, emb_b: 1.0\n",
    "\t- (emb_a @ emb_b.T)\n",
    "\t/ (\n",
    "\t\tnp.linalg.norm(emb_a, axis=1, keepdims=True)\n",
    "\t\t@ np.linalg.norm(emb_b, axis=1, keepdims=True).T\n",
    "\t\t+ 1e-10\n",
    "\t),\n",
    "\t\"l1\": lambda emb_a, emb_b: np.sum(np.abs(emb_a[..., np.newaxis] - emb_b.T), axis=1),\n",
    "\t\"l2\": lambda emb_a, emb_b: np.linalg.norm(emb_a[..., np.newaxis] - emb_b.T, axis=1),\n",
    "\t\"dot\": lambda emb_a, emb_b: emb_a @ emb_b.T,\n",
    "\t\"mahalanobis\": lambda emb_a, emb_b: getMahalanobisDistances(emb_a, emb_b),\n",
    "}\n",
    "\n",
    "\n",
    "def _prepareModelArtifact(\n",
    "\traw_vectors,\n",
    "\tsemantic_data,\n",
    "\ttruncation_dim=256,\n",
    "\tdistance_metric=\"mahalanobis\",\n",
    "\tdebug=True,\n",
    "):\n",
    "\tdata_matrix = np.array(raw_vectors)\n",
    "\tinput_dim = data_matrix.shape[1]\n",
    "\n",
    "\tif input_dim < truncation_dim and debug:\n",
    "\t\tprint(\n",
    "\t\t\tf\"Warning: Vector dimension ({input_dim}) < limit ({truncation_dim}). No truncation.\"\n",
    "\t\t)\n",
    "\n",
    "\tdata_truncated = data_matrix[:, :truncation_dim]\n",
    "\n",
    "\tdist_output = Distance_Processors[distance_metric](data_truncated, data_truncated)\n",
    "\n",
    "\tprecision_matrix = None\n",
    "\tif distance_metric == \"mahalanobis\":\n",
    "\t\tdist_matrix, precision_matrix = dist_output\n",
    "\telse:\n",
    "\t\tdist_matrix = dist_output\n",
    "\n",
    "\t# Calculate NN indices (excluding self) for cache usage\n",
    "\td_temp = dist_matrix.copy()\n",
    "\tnp.fill_diagonal(d_temp, float(\"inf\"))\n",
    "\tnn_indices = np.argmin(d_temp, axis=1)\n",
    "\n",
    "\treturn {\n",
    "\t\t\"dist_matrix\": dist_matrix,\n",
    "\t\t\"vectors\": data_truncated,\n",
    "\t\t\"precision\": precision_matrix,\n",
    "\t\t\"semantic_data\": semantic_data,\n",
    "\t\t\"metric\": distance_metric,\n",
    "\t\t\"nn_indices\": nn_indices,\n",
    "\t}\n",
    "\n",
    "\n",
    "def prepareModelArtifacts(\n",
    "\tdata_set, vector_keys, truncation_dim=256, distance_metric=\"mahalanobis\", debug=True\n",
    "):\n",
    "\tsemantic_data = list(data_set.keys())\n",
    "\tmodel_artifacts = {}\n",
    "\traw_vectors = {}\n",
    "\tfor key in vector_keys:\n",
    "\t\traw_vectors[key] = [data_set[s][key] for s in semantic_data]\n",
    "\n",
    "\texecutor = concurrent.futures.ThreadPoolExecutor(max_workers=len(vector_keys))\n",
    "\tfutures = dict()\n",
    "\tfor key in vector_keys:\n",
    "\t\tif debug:\n",
    "\t\t\tprint(f\"Processing {key}...\")\n",
    "\t\tfutures[key] = executor.submit(\n",
    "\t\t\t_prepareModelArtifact,\n",
    "\t\t\traw_vectors[key],\n",
    "\t\t\tsemantic_data,\n",
    "\t\t\ttruncation_dim,\n",
    "\t\t\tdistance_metric,\n",
    "\t\t\tdebug,\n",
    "\t\t)\n",
    "\texecutor.shutdown(wait=True)\n",
    "\tfor key in vector_keys:\n",
    "\t\tmodel_artifacts[key] = futures[key].result()\n",
    "\treturn model_artifacts\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. ACGC CONSENSUS SEARCH (COLD START)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def clusterAndGetArtifacts(dist_matrix, threshold):\n",
    "\tmodel = AgglomerativeClustering(\n",
    "\t\tn_clusters=None,\n",
    "\t\tdistance_threshold=threshold,\n",
    "\t\tmetric=\"precomputed\",\n",
    "\t\tlinkage=\"complete\",\n",
    "\t)\n",
    "\tlabels = model.fit_predict(dist_matrix)\n",
    "\tgroups = {}\n",
    "\tfor idx, label in enumerate(labels):\n",
    "\t\tgroups.setdefault(label, []).append(idx)\n",
    "\treturn [g for g in groups.values() if len(g) > 1], labels\n",
    "\n",
    "\n",
    "def getNNPairsFromGroups(groups, nn_indices):\n",
    "\tpairs = set()\n",
    "\tfor group in groups:\n",
    "\t\tif len(group) < 2:\n",
    "\t\t\tcontinue\n",
    "\t\tgroup_set = set(group)\n",
    "\t\tfor idx in group:\n",
    "\t\t\tnn_idx = nn_indices[idx]\n",
    "\t\t\tif nn_idx in group_set:\n",
    "\t\t\t\tpairs.add(tuple(sorted((idx, nn_idx))))\n",
    "\treturn pairs\n",
    "\n",
    "\n",
    "def calculateNTrue(labels_array, target_pairs):\n",
    "\tif not target_pairs or labels_array is None:\n",
    "\t\treturn 0\n",
    "\tinvolved = {idx for pair in target_pairs for idx in pair}\n",
    "\tif not involved:\n",
    "\t\treturn 0\n",
    "\treturn len({labels_array[idx] for idx in involved})\n",
    "\n",
    "\n",
    "def createClusteringCache(model_artifacts, tau_range):\n",
    "\tcache = {name: {} for name in model_artifacts.keys()}\n",
    "\t# print(f\"Building ACGC Cache ({len(tau_range)} steps)...\")\n",
    "\tfor name, artifact in model_artifacts.items():\n",
    "\t\tprev_groups = []\n",
    "\t\tfor t in tau_range:\n",
    "\t\t\tgroups, labels = clusterAndGetArtifacts(artifact[\"dist_matrix\"], t)\n",
    "\t\t\t# Optimization: Only store if groups changed\n",
    "\t\t\tif len(groups) != len(prev_groups) or groups != prev_groups:\n",
    "\t\t\t\tnn_pairs = getNNPairsFromGroups(groups, artifact[\"nn_indices\"])\n",
    "\t\t\t\tcache[name][t] = [groups, labels, nn_pairs]\n",
    "\t\t\t\tprev_groups = groups\n",
    "\treturn cache\n",
    "\n",
    "\n",
    "def findConsensusViaACGC(clustering_cache, model_keys):\n",
    "\tthreshold_axes = [list(clustering_cache[m].keys()) for m in model_keys]\n",
    "\tbest_score = -1\n",
    "\tbest_p_true = set()\n",
    "\n",
    "\tfor thresholds in product(*threshold_axes):\n",
    "\t\tcurrent_config = dict(zip(model_keys, thresholds))\n",
    "\t\tpair_sets = [clustering_cache[m][t][2] for m, t in current_config.items()]\n",
    "\n",
    "\t\tp_true = set.intersection(*pair_sets)\n",
    "\t\tif not p_true:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tn_true_sum = 0\n",
    "\t\tfor m, t in current_config.items():\n",
    "\t\t\tlabels = clustering_cache[m][t][1]\n",
    "\t\t\tn_true_sum += calculateNTrue(labels, p_true)\n",
    "\n",
    "\t\tcurrent_score = n_true_sum / len(model_keys)\n",
    "\t\tif current_score > best_score:\n",
    "\t\t\tbest_score = current_score\n",
    "\t\t\tbest_p_true = p_true\n",
    "\n",
    "\treturn best_p_true\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. PROBABILISTIC MODELING (EVT & MIXTURE MODELS)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# def computeLocalDensityRanks(dist_matrix, k_neighbors=50):\n",
    "# \t# method='min' ensures ties get the lower rank\n",
    "# \tranks = rankdata(dist_matrix, axis=1, method=\"min\")\n",
    "# \t# Subtract 1 so the diagonal (self) is 0.0\n",
    "# \tranks = ranks - 1.0\n",
    "# \tnormalized_ranks = ranks / k_neighbors\n",
    "# \treturn np.clip(normalized_ranks, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def computeLocalDensityRanks(dist_matrix, k_neighbors=50, max_valid_dist=None):\n",
    "\t\"\"\"\n",
    "\tComputes ranks but penalizes neighbors that are 'physically' too far.\n",
    "\n",
    "\tArgs:\n",
    "\t    max_valid_dist: If provided, any distance > this value is treated\n",
    "\t                    as Background Noise (Rank 1.0), regardless of being a NN.\n",
    "\t\"\"\"\n",
    "\t# 1. Standard Ranking\n",
    "\tranks = rankdata(dist_matrix, axis=1, method=\"min\") - 1.0\n",
    "\tnormalized_ranks = ranks / k_neighbors\n",
    "\n",
    "\t# 2. Distance Gating (The Fix)\n",
    "\tif max_valid_dist is not None:\n",
    "\t\t# Create a mask where the raw distance is too high\n",
    "\t\t# We treat these as \"Infinite Rank\" (Probability 0 of being signal)\n",
    "\t\tnoise_mask = dist_matrix > max_valid_dist\n",
    "\t\tnormalized_ranks[noise_mask] = 1.0\t# Force to max entropy\n",
    "\n",
    "\treturn np.clip(normalized_ranks, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def computeFusedEVTStatistic(rank_matrices_dict):\n",
    "\tmatrices = list(rank_matrices_dict.values())\n",
    "\tstacked_matrices = np.stack(matrices, axis=0)\n",
    "\treturn np.min(stacked_matrices, axis=0)\n",
    "\n",
    "\n",
    "def _get_off_diagonal_samples(matrix):\n",
    "\tmask = ~np.eye(matrix.shape[0], dtype=bool)\n",
    "\treturn matrix[mask]\n",
    "\n",
    "\n",
    "def _initializeParametersViaNaiveQuantile(fused_statistic_matrix, signal_quantile=0.01):\n",
    "\tdata = _get_off_diagonal_samples(fused_statistic_matrix)\n",
    "\tthreshold = np.quantile(data, signal_quantile)\n",
    "\tsignal_data = data[data <= threshold]\n",
    "\tnoise_data = data[data > threshold]\n",
    "\n",
    "\tlamb = 1.0 / (np.mean(signal_data) + 1e-6)\n",
    "\tmu, var = np.mean(noise_data), np.var(noise_data)\n",
    "\n",
    "\tif var >= mu * (1 - mu):\n",
    "\t\talpha, b_param = 1.0, 1.0\n",
    "\telse:\n",
    "\t\tcommon = (mu * (1 - mu) / (var + 1e-9)) - 1\n",
    "\t\talpha = max(mu * common, 1.0)\n",
    "\t\tb_param = max((1 - mu) * common, 1.0)\n",
    "\n",
    "\treturn {\"pi\": signal_quantile, \"lambda\": lamb, \"alpha\": alpha, \"beta\": b_param}\n",
    "\n",
    "\n",
    "def _initializeParametersViaConsensus(fused_statistic_matrix, consensus_pairs):\n",
    "\tif not consensus_pairs:\n",
    "\t\tprint(\"Warning: ACGC found no pairs. Falling back to naive quantile.\")\n",
    "\t\treturn _initializeParametersViaNaiveQuantile(fused_statistic_matrix)\n",
    "\n",
    "\tsignal_values = [fused_statistic_matrix[i, j] for i, j in consensus_pairs]\n",
    "\tflat_all = _get_off_diagonal_samples(fused_statistic_matrix)\n",
    "\n",
    "\tnoise_mean = np.mean(flat_all)\n",
    "\tnoise_var = np.var(flat_all)\n",
    "\n",
    "\tpi = len(consensus_pairs) / len(flat_all)\n",
    "\tlamb = 1.0 / (np.mean(signal_values) + 1e-6)\n",
    "\n",
    "\tcommon = (noise_mean * (1 - noise_mean) / (noise_var + 1e-9)) - 1\n",
    "\talpha = max(noise_mean * common, 1.0)\n",
    "\tbeta_param = max((1 - noise_mean) * common, 1.0)\n",
    "\n",
    "\treturn {\"pi\": pi, \"lambda\": lamb, \"alpha\": alpha, \"beta\": beta_param}\n",
    "\n",
    "\n",
    "def fitWeibullBetaMixture(\n",
    "\tfused_statistic_matrix, initial_params=None, max_iter=100, tol=1e-4\n",
    "):\n",
    "\tdata = _get_off_diagonal_samples(fused_statistic_matrix)\n",
    "\tdata = np.clip(data, 1e-6, 1.0 - 1e-6)\n",
    "\n",
    "\tif initial_params is None:\n",
    "\t\tparams = _initializeParametersViaNaiveQuantile(fused_statistic_matrix)\n",
    "\telse:\n",
    "\t\tparams = initial_params\n",
    "\n",
    "\tpi, lamb, a, b_p = params[\"pi\"], params[\"lambda\"], params[\"alpha\"], params[\"beta\"]\n",
    "\tlog_likelihood_old = -np.inf\n",
    "\n",
    "\tfor _ in range(max_iter):\n",
    "\t\t# E-Step\n",
    "\t\tpdf_signal = expon.pdf(data, scale=1.0 / lamb)\n",
    "\t\tpdf_noise = beta.pdf(data, a, b_p)\n",
    "\n",
    "\t\tweighted_signal = pi * pdf_signal\n",
    "\t\tweighted_noise = (1 - pi) * pdf_noise\n",
    "\t\ttotal_evidence = weighted_signal + weighted_noise + 1e-10\n",
    "\t\tgamma = weighted_signal / total_evidence\n",
    "\n",
    "\t\t# M-Step\n",
    "\t\tN_s = np.sum(gamma)\n",
    "\t\tpi = N_s / len(data)\n",
    "\n",
    "\t\tweighted_sum_x = np.sum(gamma * data)\n",
    "\t\tlamb = 1.0 / (weighted_sum_x / (N_s + 1e-10))\n",
    "\n",
    "\t\tw_noise = 1 - gamma\n",
    "\t\tw_noise_sum = np.sum(w_noise) + 1e-10\n",
    "\t\tmu_n = np.sum(w_noise * data) / w_noise_sum\n",
    "\t\tvar_n = np.sum(w_noise * (data - mu_n) ** 2) / w_noise_sum\n",
    "\n",
    "\t\tif var_n < mu_n * (1 - mu_n):\n",
    "\t\t\tcommon = (mu_n * (1 - mu_n) / (var_n + 1e-10)) - 1\n",
    "\t\t\ta = max(mu_n * common, 1.0)\n",
    "\t\t\tb_p = max((1 - mu_n) * common, 1.0)\n",
    "\n",
    "\t\tlog_likelihood_new = np.sum(np.log(total_evidence))\n",
    "\t\tif abs(log_likelihood_new - log_likelihood_old) < tol:\n",
    "\t\t\tbreak\n",
    "\t\tlog_likelihood_old = log_likelihood_new\n",
    "\n",
    "\tfinal_params = {\"pi\": pi, \"lambda\": lamb, \"alpha\": a, \"beta\": b_p}\n",
    "\n",
    "\t# Compute Full Posterior\n",
    "\tpdf_S_full = expon.pdf(fused_statistic_matrix, scale=1.0 / lamb)\n",
    "\tpdf_N_full = beta.pdf(np.clip(fused_statistic_matrix, 1e-6, 1 - 1e-6), a, b_p)\n",
    "\tnumerator = pi * pdf_S_full\n",
    "\tposterior = numerator / (numerator + (1 - pi) * pdf_N_full + 1e-10)\n",
    "\n",
    "\treturn posterior, final_params\n",
    "\n",
    "\n",
    "# def fitWeibullBetaMixture(\n",
    "# \tfused_statistic_matrix, initial_params=None, max_iter=20, tol=1e-4\n",
    "# ):\n",
    "# \tdata = _get_off_diagonal_samples(fused_statistic_matrix)\n",
    "# \tdata = np.clip(data, 1e-6, 1.0 - 1e-6)\n",
    "\n",
    "# \tif initial_params is None:\n",
    "# \t\tparams = _initializeParametersViaNaiveQuantile(fused_statistic_matrix)\n",
    "# \telse:\n",
    "# \t\tparams = initial_params\n",
    "\n",
    "# \tpi, lamb, a, b_p = params[\"pi\"], params[\"lambda\"], params[\"alpha\"], params[\"beta\"]\n",
    "# \tlog_likelihood_old = -np.inf\n",
    "\n",
    "# \t# --- EM LOOP (Learning Shapes with True Priors) ---\n",
    "# \tfor _ in range(max_iter):\n",
    "# \t\t# E-Step\n",
    "# \t\tpdf_signal = expon.pdf(data, scale=1.0 / lamb)\n",
    "# \t\tpdf_noise = beta.pdf(data, a, b_p)\n",
    "\n",
    "# \t\tweighted_signal = pi * pdf_signal\n",
    "# \t\tweighted_noise = (1 - pi) * pdf_noise\n",
    "# \t\ttotal_evidence = weighted_signal + weighted_noise + 1e-10\n",
    "# \t\tgamma = weighted_signal / total_evidence\n",
    "\n",
    "# \t\t# M-Step\n",
    "# \t\tN_s = np.sum(gamma)\n",
    "# \t\tpi = N_s / len(data)\n",
    "\n",
    "# \t\tweighted_sum_x = np.sum(gamma * data)\n",
    "# \t\tlamb = 1.0 / (weighted_sum_x / (N_s + 1e-10))\n",
    "\n",
    "# \t\tw_noise = 1 - gamma\n",
    "# \t\tw_noise_sum = np.sum(w_noise) + 1e-10\n",
    "# \t\tmu_n = np.sum(w_noise * data) / w_noise_sum\n",
    "# \t\tvar_n = np.sum(w_noise * (data - mu_n) ** 2) / w_noise_sum\n",
    "\n",
    "# \t\tif var_n < mu_n * (1 - mu_n):\n",
    "# \t\t\tcommon = (mu_n * (1 - mu_n) / (var_n + 1e-10)) - 1\n",
    "# \t\t\ta = max(mu_n * common, 1.0)\n",
    "# \t\t\tb_p = max((1 - mu_n) * common, 1.0)\n",
    "\n",
    "# \t\tlog_likelihood_new = np.sum(np.log(total_evidence))\n",
    "# \t\tif abs(log_likelihood_new - log_likelihood_old) < tol:\n",
    "# \t\t\tbreak\n",
    "# \t\tlog_likelihood_old = log_likelihood_new\n",
    "\n",
    "# \tfinal_params = {\"pi\": pi, \"lambda\": lamb, \"alpha\": a, \"beta\": b_p}\n",
    "\n",
    "# \t# --- FINAL INFERENCE (Balanced Prior) ---\n",
    "# \t# We calculate the probability assuming P(Signal) = 0.5 vs P(Noise) = 0.5\n",
    "# \t# This turns the output into a Likelihood Ratio test normalized to [0,1].\n",
    "\n",
    "# \tpdf_S_full = expon.pdf(fused_statistic_matrix, scale=1.0 / lamb)\n",
    "# \tpdf_N_full = beta.pdf(np.clip(fused_statistic_matrix, 1e-6, 1 - 1e-6), a, b_p)\n",
    "\n",
    "# \t# Use 0.5 instead of 'pi' here to ignore class imbalance for detection\n",
    "# \tnumerator = 0.5 * pdf_S_full\n",
    "# \tposterior = numerator / (numerator + 0.5 * pdf_N_full + 1e-10)\n",
    "\n",
    "# \treturn posterior, final_params\n",
    "\n",
    "\n",
    "def fitWeibullBetaMixture(\n",
    "\tfused_statistic_matrix, initial_params=None, max_iter=100, tol=1e-4\n",
    "):\n",
    "\t\"\"\"\n",
    "\tFits EVT Mixture Model using Balanced-EM.\n",
    "\tCrucial Fix: Forces 50/50 prior during training to prevent\n",
    "\tclass imbalance from collapsing the signal distribution.\n",
    "\t\"\"\"\n",
    "\tdata = _get_off_diagonal_samples(fused_statistic_matrix)\n",
    "\tdata = np.clip(data, 1e-6, 1.0 - 1e-6)\n",
    "\n",
    "\tif initial_params is None:\n",
    "\t\tparams = _initializeParametersViaNaiveQuantile(fused_statistic_matrix)\n",
    "\telse:\n",
    "\t\tparams = initial_params\n",
    "\n",
    "\t# We ignore the 'pi' from init because it is too small (e.g. 0.00002).\n",
    "\t# We will trust the Lambda/Alpha/Beta shapes, but ignore the frequency.\n",
    "\tlamb = params[\"lambda\"]\n",
    "\ta = params[\"alpha\"]\n",
    "\tb_p = params[\"beta\"]\n",
    "\n",
    "\tlog_likelihood_old = -np.inf\n",
    "\n",
    "\t# --- BALANCED EM LOOP ---\n",
    "\tfor _ in range(max_iter):\n",
    "\t\t# E-Step: Calculate Responsibilities assuming Balanced Priors (0.5 / 0.5)\n",
    "\t\t# This asks: \"Based on SHAPE alone, which distribution fits best?\"\n",
    "\t\tpdf_signal = expon.pdf(data, scale=1.0 / lamb)\n",
    "\t\tpdf_noise = beta.pdf(data, a, b_p)\n",
    "\n",
    "\t\t# Force 50/50 weights\n",
    "\t\tweighted_signal = 0.5 * pdf_signal\n",
    "\t\tweighted_noise = 0.5 * pdf_noise\n",
    "\n",
    "\t\ttotal_evidence = weighted_signal + weighted_noise + 1e-10\n",
    "\t\tgamma = weighted_signal / total_evidence\t# P(Signal | Data, Balanced)\n",
    "\n",
    "\t\t# M-Step: Update Shape Parameters based on these responsibilities\n",
    "\t\tN_s = np.sum(gamma)\n",
    "\n",
    "\t\t# Update Lambda (Weighted MLE)\n",
    "\t\tweighted_sum_x = np.sum(gamma * data)\n",
    "\t\t# Protect against N_s vanishing\n",
    "\t\tlamb = 1.0 / (weighted_sum_x / (N_s + 1e-10))\n",
    "\n",
    "\t\t# Update Noise Params\n",
    "\t\tw_noise = 1 - gamma\n",
    "\t\tw_noise_sum = np.sum(w_noise) + 1e-10\n",
    "\t\tmu_n = np.sum(w_noise * data) / w_noise_sum\n",
    "\t\tvar_n = np.sum(w_noise * (data - mu_n) ** 2) / w_noise_sum\n",
    "\n",
    "\t\tif var_n < mu_n * (1 - mu_n):\n",
    "\t\t\tcommon = (mu_n * (1 - mu_n) / (var_n + 1e-10)) - 1\n",
    "\t\t\ta = max(mu_n * common, 1.0)\n",
    "\t\t\tb_p = max((1 - mu_n) * common, 1.0)\n",
    "\n",
    "\t\t# Check convergence on the balanced likelihood\n",
    "\t\tlog_likelihood_new = np.sum(np.log(total_evidence))\n",
    "\t\tif abs(log_likelihood_new - log_likelihood_old) < tol:\n",
    "\t\t\tbreak\n",
    "\t\tlog_likelihood_old = log_likelihood_new\n",
    "\n",
    "\t# We do NOT return a learned 'pi'. We return the shapes.\n",
    "\tfinal_params = {\"lambda\": lamb, \"alpha\": a, \"beta\": b_p}\n",
    "\n",
    "\t# --- FINAL INFERENCE ---\n",
    "\t# Again, use Balanced Inference. P > 0.5 means \"More likely Signal than Noise\".\n",
    "\tpdf_S_full = expon.pdf(fused_statistic_matrix, scale=1.0 / lamb)\n",
    "\tpdf_N_full = beta.pdf(np.clip(fused_statistic_matrix, 1e-6, 1 - 1e-6), a, b_p)\n",
    "\n",
    "\tnumerator = 0.5 * pdf_S_full\n",
    "\tposterior = numerator / (numerator + 0.5 * pdf_N_full + 1e-10)\n",
    "\n",
    "\treturn posterior, final_params\n",
    "\n",
    "\n",
    "# def fitWeibullBetaMixture( # errs bcos clustering and inf AgglomerativeClustering does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
    "\n",
    "# \tfused_statistic_matrix, initial_params=None, max_iter=20, tol=1e-4\n",
    "# ):\n",
    "# \t\"\"\"\n",
    "# \tFits EVT Mixture Model using Balanced-EM.\n",
    "# \tCrucial Fix: Forces 50/50 prior during training steps.\n",
    "# \t\"\"\"\n",
    "# \tdata = _get_off_diagonal_samples(fused_statistic_matrix)\n",
    "# \tdata = np.clip(data, 1e-6, 1.0 - 1e-6)\n",
    "\n",
    "# \tif initial_params is None:\n",
    "# \t\tparams = _initializeParametersViaNaiveQuantile(fused_statistic_matrix)\n",
    "# \telse:\n",
    "# \t\tparams = initial_params\n",
    "\n",
    "# \tlamb = params[\"lambda\"]\n",
    "# \ta = params[\"alpha\"]\n",
    "# \tb_p = params[\"beta\"]\n",
    "\n",
    "# \tlog_likelihood_old = -np.inf\n",
    "\n",
    "# \t# --- BALANCED EM LOOP ---\n",
    "# \tfor _ in range(max_iter):\n",
    "# \t\t# E-Step: Force 50/50 weights\n",
    "# \t\tpdf_signal = expon.pdf(data, scale=1.0 / lamb)\n",
    "# \t\tpdf_noise = beta.pdf(data, a, b_p)\n",
    "\n",
    "# \t\t# Use 0.5 prior to learn SHAPE, ignoring FREQUENCY\n",
    "# \t\tweighted_signal = 0.5 * pdf_signal\n",
    "# \t\tweighted_noise = 0.5 * pdf_noise\n",
    "\n",
    "# \t\ttotal_evidence = weighted_signal + weighted_noise + 1e-10\n",
    "# \t\tgamma = weighted_signal / total_evidence\n",
    "\n",
    "# \t\t# M-Step\n",
    "# \t\tN_s = np.sum(gamma)\n",
    "# \t\tweighted_sum_x = np.sum(gamma * data)\n",
    "# \t\tlamb = 1.0 / (weighted_sum_x / (N_s + 1e-10))\n",
    "\n",
    "# \t\tw_noise = 1 - gamma\n",
    "# \t\tw_noise_sum = np.sum(w_noise) + 1e-10\n",
    "# \t\tmu_n = np.sum(w_noise * data) / w_noise_sum\n",
    "# \t\tvar_n = np.sum(w_noise * (data - mu_n) ** 2) / w_noise_sum\n",
    "\n",
    "# \t\tif var_n < mu_n * (1 - mu_n):\n",
    "# \t\t\tcommon = (mu_n * (1 - mu_n) / (var_n + 1e-10)) - 1\n",
    "# \t\t\ta = max(mu_n * common, 1.0)\n",
    "# \t\t\tb_p = max((1 - mu_n) * common, 1.0)\n",
    "\n",
    "# \t\tif abs(np.sum(np.log(total_evidence)) - log_likelihood_old) < tol:\n",
    "# \t\t\tbreak\n",
    "# \t\tlog_likelihood_old = np.sum(np.log(total_evidence))\n",
    "\n",
    "# \tfinal_params = {\"lambda\": lamb, \"alpha\": a, \"beta\": b_p}\n",
    "\n",
    "# \t# --- FINAL INFERENCE (Balanced) ---\n",
    "# \tpdf_S_full = expon.pdf(fused_statistic_matrix, scale=1.0 / lamb)\n",
    "# \tpdf_N_full = beta.pdf(np.clip(fused_statistic_matrix, 1e-6, 1 - 1e-6), a, b_p)\n",
    "\n",
    "# \tnumerator = 0.5 * pdf_S_full\n",
    "# \tposterior = numerator / (numerator + 0.5 * pdf_N_full + 1e-10)\n",
    "\n",
    "# \treturn posterior, final_params\n",
    "\n",
    "\n",
    "def fitGaussianMixture(fused_statistic_matrix, n_components=2, initial_params=None):\n",
    "\tdata = _get_off_diagonal_samples(fused_statistic_matrix).reshape(-1, 1)\n",
    "\tgmm = GaussianMixture(\n",
    "\t\tn_components=n_components, covariance_type=\"full\", random_state=42\n",
    "\t)\n",
    "\tgmm.fit(data)\n",
    "\n",
    "\tsignal_idx = np.argmin(gmm.means_.flatten())\n",
    "\tprobs = gmm.predict_proba(fused_statistic_matrix.reshape(-1, 1))\n",
    "\tposterior = probs[:, signal_idx].reshape(fused_statistic_matrix.shape)\n",
    "\n",
    "\tparams = {\n",
    "\t\t\"means\": gmm.means_.tolist(),\n",
    "\t\t\"signal_idx\": int(signal_idx),\n",
    "\t\t\"weights\": gmm.weights_.tolist(),\n",
    "\t}\n",
    "\treturn posterior, params\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. TOPOLOGY & CLUSTERING\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def computeTransitivityPrior(likelihood_matrix):\n",
    "\traw_support = likelihood_matrix @ likelihood_matrix\n",
    "\tmax_val = np.max(raw_support)\n",
    "\treturn (raw_support / max_val) if max_val > 0 else raw_support\n",
    "\n",
    "\n",
    "def computeFinalPosterior(likelihood_matrix, prior_matrix, weight_factor=0.5):\n",
    "\tsafe_prior = np.clip(prior_matrix, 0.1, 1.0)\n",
    "\treturn likelihood_matrix * (safe_prior**weight_factor)\n",
    "\n",
    "\n",
    "def extractAndPrintClusters(posterior_matrix, semantic_data, threshold=0.5):\n",
    "\tdist_matrix = np.clip(1.0 - posterior_matrix, 0.0, 1.0)\n",
    "\tmodel = AgglomerativeClustering(\n",
    "\t\tn_clusters=None,\n",
    "\t\tdistance_threshold=(1.0 - threshold),\n",
    "\t\tmetric=\"precomputed\",\n",
    "\t\tlinkage=\"complete\",\n",
    "\t)\n",
    "\tlabels = model.fit_predict(dist_matrix)\n",
    "\n",
    "\tgroups = {}\n",
    "\tfor idx, label in enumerate(labels):\n",
    "\t\tgroups.setdefault(label, []).append(idx)\n",
    "\n",
    "\tsignificant_groups = [g for g in groups.values() if len(g) > 1]\n",
    "\tsignificant_groups.sort(key=len, reverse=True)\n",
    "\n",
    "\tprint(f\"\\n{'='*80}\")\n",
    "\tprint(f\"PROBABILISTIC CLUSTERING RESULTS (Threshold P > {threshold})\")\n",
    "\tprint(f\"{'='*80}\")\n",
    "\tprint(f\"Found {len(significant_groups)} significant groups.\\n\")\n",
    "\n",
    "\tfor i, indices in enumerate(significant_groups):\n",
    "\t\tsub_probs = posterior_matrix[np.ix_(indices, indices)]\n",
    "\t\tmask = ~np.eye(len(indices), dtype=bool)\n",
    "\t\tavg_prob = np.mean(sub_probs[mask]) if len(indices) > 1 else 1.0\n",
    "\n",
    "\t\tprint(f\"GROUP {i+1} (Size: {len(indices)}) [Cohesion: {avg_prob:.4f}]\")\n",
    "\t\tfor idx in indices:\n",
    "\t\t\tprint(f\" - {semantic_data[idx]}\")\n",
    "\t\tprint(\"-\" * 80)\n",
    "\n",
    "\n",
    "def findConsensusViaJaccard(clustering_cache, model_keys):\n",
    "\t\"\"\"\n",
    "\tFinds the 'Platinum' set: Pairs that appear in ALL models at the same\n",
    "\tthreshold configuration that maximizes Intersection-over-Union.\n",
    "\t\"\"\"\n",
    "\tthreshold_axes = [list(clustering_cache[m].keys()) for m in model_keys]\n",
    "\tbest_score = -1\n",
    "\tbest_p_true = set()\n",
    "\n",
    "\t# Grid Search\n",
    "\tfor thresholds in product(*threshold_axes):\n",
    "\t\tcurrent_config = dict(zip(model_keys, thresholds))\n",
    "\t\tpair_sets = [clustering_cache[m][t][2] for m, t in current_config.items()]\n",
    "\n",
    "\t\t# INTERSECTION (The Core)\n",
    "\t\tp_true = set.intersection(*pair_sets)\n",
    "\t\tif not p_true:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# UNION (The Broad Net)\n",
    "\t\tp_union = set.union(*pair_sets)\n",
    "\t\tif not p_union:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# Jaccard Score\n",
    "\t\tcurrent_score = len(p_true) / len(p_union)\n",
    "\n",
    "\t\t# We prefer high Jaccard, but also want non-trivial size\n",
    "\t\t# (Tie-breaker: larger set size)\n",
    "\t\tif current_score > best_score:\n",
    "\t\t\tbest_score = current_score\n",
    "\t\t\tbest_p_true = p_true\n",
    "\t\telif current_score == best_score:\n",
    "\t\t\tif len(p_true) > len(best_p_true):\n",
    "\t\t\t\tbest_p_true = p_true\n",
    "\n",
    "\treturn best_p_true\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. MAIN EXECUTION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\">>> Preparing Model Artifacts...\")\n",
    "model_keys = [\"embedding_vector\", \"retrieval_embedding_vector\"]\n",
    "artifacts = prepareModelArtifacts(qdata, model_keys, truncation_dim=256, debug=False)\n",
    "\n",
    "print(\"\\n>>> Phase 1: Deriving Consensus Priors (ACGC)...\")\n",
    "max_dist = max(np.max(a[\"dist_matrix\"]) for a in artifacts.values())\n",
    "tau_range = np.arange(0.1, max_dist, 0.05)\n",
    "cache = createClusteringCache(artifacts, tau_range)\n",
    "print(\"\\n>>> Phase 1: Deriving Consensus Priors (ACGC)...\")\n",
    "max_dist = max(np.max(a[\"dist_matrix\"]) for a in artifacts.values())\n",
    "tau_range = np.arange(0.1, max_dist, 0.05)\n",
    "cache = createClusteringCache(artifacts, tau_range)\n",
    "\n",
    "consensus_pairs = findConsensusViaACGC(cache, model_keys)\n",
    "# consensus_pairs = findConsensusViaJaccard(cache, model_keys) # doesnt work\n",
    "# gives AgglomerativeClustering does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
    "# importantly ACGC isnt the precise labels, just most consensus sets\n",
    "print(f\"Consensus Anchors Found: {len(consensus_pairs)}\")\n",
    "\n",
    "\n",
    "def findConsensusViaJaccard(clustering_cache, model_keys):\n",
    "\t\"\"\"\n",
    "\tFinds the 'Platinum' set: Pairs that appear in ALL models at the same\n",
    "\tthreshold configuration that maximizes Intersection-over-Union.\n",
    "\t\"\"\"\n",
    "\tthreshold_axes = [list(clustering_cache[m].keys()) for m in model_keys]\n",
    "\tbest_score = -1\n",
    "\tbest_p_true = set()\n",
    "\n",
    "\t# Grid Search\n",
    "\tfor thresholds in product(*threshold_axes):\n",
    "\t\tcurrent_config = dict(zip(model_keys, thresholds))\n",
    "\t\tpair_sets = [clustering_cache[m][t][2] for m, t in current_config.items()]\n",
    "\n",
    "\t\t# INTERSECTION (The Core)\n",
    "\t\tp_true = set.intersection(*pair_sets)\n",
    "\t\tif not p_true:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# UNION (The Broad Net)\n",
    "\t\tp_union = set.union(*pair_sets)\n",
    "\t\tif not p_union:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# Jaccard Score\n",
    "\t\tcurrent_score = len(p_true) / len(p_union)\n",
    "\n",
    "\t\t# We prefer high Jaccard, but also want non-trivial size\n",
    "\t\t# (Tie-breaker: larger set size)\n",
    "\t\tif current_score > best_score:\n",
    "\t\t\tbest_score = current_score\n",
    "\t\t\tbest_p_true = p_true\n",
    "\t\telif current_score == best_score:\n",
    "\t\t\tif len(p_true) > len(best_p_true):\n",
    "\t\t\t\tbest_p_true = p_true\n",
    "\n",
    "\treturn best_p_true\n",
    "\n",
    "\n",
    "# --- STEP 1.5: Derive Strict Safety Horizon ---\n",
    "consensus_dists = []\n",
    "safety_horizon = 0\n",
    "for key in model_keys:\n",
    "\tmat = artifacts[key][\"dist_matrix\"]\n",
    "\tfor i, j in consensus_pairs:\n",
    "\t\tconsensus_dists.append(mat[i, j])\n",
    "\n",
    "if consensus_dists:\n",
    "\tmax_consensus_dist = np.max(consensus_dists)\n",
    "\t# Jaccard pairs are very tight (approx Dist 8.8).\n",
    "\t# 1.5x buffer -> ~13.2. This allows for valid fuzzy matches\n",
    "\t# but filters out the \"Child Privacy\" hallucination (Dist ~15+).\n",
    "\tsafety_horizon = max_consensus_dist * 1.5\n",
    "\tprint(\n",
    "\t\tf\"Safety Horizon Derived: {safety_horizon:.4f} (Max Seed: {max_consensus_dist:.4f})\"\n",
    "\t)\n",
    "else:\n",
    "\t# Fallback if Jaccard finds nothing (rare)\n",
    "\tsafety_horizon = None\n",
    "\tprint(\"Warning: No Jaccard seeds. Safety Horizon disabled.\")\n",
    "\n",
    "# --- STEP 2: Probabilistic Modeling ---\n",
    "print(\"\\n>>> Phase 2: Running EVT Pipeline...\")\n",
    "k_neighbors = 40\n",
    "\n",
    "rank_matrices = {}\n",
    "for key, artifact in artifacts.items():\n",
    "\t# PASS THE SAFETY HORIZON HERE\n",
    "\trank_matrices[key] = computeLocalDensityRanks(\n",
    "\t\tartifact[\"dist_matrix\"], k_neighbors=k_neighbors, max_valid_dist=safety_horizon\n",
    "\t)\n",
    "\n",
    "fused_matrix = computeFusedEVTStatistic(rank_matrices)\n",
    "\n",
    "# Re-init params since the rank inputs have changed (gated items are now 1.0)\n",
    "init_params = _initializeParametersViaConsensus(fused_matrix, consensus_pairs)\n",
    "print(f\"Bootstrapped Params: {init_params}\")\n",
    "\n",
    "likelihood_matrix, final_params = fitWeibullBetaMixture(\n",
    "\tfused_matrix, initial_params=None\n",
    ")\n",
    "print(\"EM Model Converged.\")\n",
    "\n",
    "\n",
    "# --- STEP 3: Topology & Results ---\n",
    "print(\"\\n>>> Phase 3: Final Inference\")\n",
    "prior_matrix = computeTransitivityPrior(likelihood_matrix)\n",
    "posterior_matrix = computeFinalPosterior(likelihood_matrix, prior_matrix)\n",
    "\n",
    "\n",
    "final_matrix = posterior_matrix * prior_matrix\n",
    "print(\"Applying Conservative Symmetrization...\")\n",
    "final_symmetric_matrix = np.minimum(final_matrix, final_matrix.T)\n",
    "extractAndPrintClusters(\n",
    "\tfinal_matrix, artifacts[model_keys[0]][\"semantic_data\"], threshold=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d903b66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "202f8e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Preparing Model Artifacts...\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 6. MAIN EXECUTION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\">>> Preparing Model Artifacts...\")\n",
    "model_keys = [\"embedding_vector\", \"retrieval_embedding_vector\"]\n",
    "artifacts = prepareModelArtifacts(qdata, model_keys, truncation_dim=256, debug=False)\n",
    "\n",
    "print(\"\\n>>> Phase 1: Deriving Consensus Priors (ACGC)...\")\n",
    "max_dist = max(np.max(a[\"dist_matrix\"]) for a in artifacts.values())\n",
    "tau_range = np.arange(0.1, max_dist, 0.05)\n",
    "cache = createClusteringCache(artifacts, tau_range)\n",
    "print(\"\\n>>> Phase 1: Deriving Consensus Priors (ACGC)...\")\n",
    "max_dist = max(np.max(a[\"dist_matrix\"]) for a in artifacts.values())\n",
    "tau_range = np.arange(0.1, max_dist, 0.05)\n",
    "cache = createClusteringCache(artifacts, tau_range)\n",
    "\n",
    "consensus_pairs = findConsensusViaACGC(cache, model_keys)\n",
    "# consensus_pairs = findConsensusViaJaccard(cache, model_keys) # doesnt work\n",
    "# gives AgglomerativeClustering does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
    "# importantly ACGC isnt the precise labels, just most consensus sets\n",
    "print(f\"Consensus Anchors Found: {len(consensus_pairs)}\")\n",
    "\n",
    "\n",
    "def findConsensusViaJaccard(clustering_cache, model_keys):\n",
    "\t\"\"\"\n",
    "\tFinds the 'Platinum' set: Pairs that appear in ALL models at the same\n",
    "\tthreshold configuration that maximizes Intersection-over-Union.\n",
    "\t\"\"\"\n",
    "\tthreshold_axes = [list(clustering_cache[m].keys()) for m in model_keys]\n",
    "\tbest_score = -1\n",
    "\tbest_p_true = set()\n",
    "\n",
    "\t# Grid Search\n",
    "\tfor thresholds in product(*threshold_axes):\n",
    "\t\tcurrent_config = dict(zip(model_keys, thresholds))\n",
    "\t\tpair_sets = [clustering_cache[m][t][2] for m, t in current_config.items()]\n",
    "\n",
    "\t\t# INTERSECTION (The Core)\n",
    "\t\tp_true = set.intersection(*pair_sets)\n",
    "\t\tif not p_true:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# UNION (The Broad Net)\n",
    "\t\tp_union = set.union(*pair_sets)\n",
    "\t\tif not p_union:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# Jaccard Score\n",
    "\t\tcurrent_score = len(p_true) / len(p_union)\n",
    "\n",
    "\t\t# We prefer high Jaccard, but also want non-trivial size\n",
    "\t\t# (Tie-breaker: larger set size)\n",
    "\t\tif current_score > best_score:\n",
    "\t\t\tbest_score = current_score\n",
    "\t\t\tbest_p_true = p_true\n",
    "\t\telif current_score == best_score:\n",
    "\t\t\tif len(p_true) > len(best_p_true):\n",
    "\t\t\t\tbest_p_true = p_true\n",
    "\n",
    "\treturn best_p_true\n",
    "\n",
    "\n",
    "# --- STEP 1.5: Derive Strict Safety Horizon ---\n",
    "consensus_dists = []\n",
    "safety_horizon = 0\n",
    "for key in model_keys:\n",
    "\tmat = artifacts[key][\"dist_matrix\"]\n",
    "\tfor i, j in consensus_pairs:\n",
    "\t\tconsensus_dists.append(mat[i, j])\n",
    "\n",
    "if consensus_dists:\n",
    "\tmax_consensus_dist = np.max(consensus_dists)\n",
    "\t# Jaccard pairs are very tight (approx Dist 8.8).\n",
    "\t# 1.5x buffer -> ~13.2. This allows for valid fuzzy matches\n",
    "\t# but filters out the \"Child Privacy\" hallucination (Dist ~15+).\n",
    "\tsafety_horizon = max_consensus_dist * 1.5\n",
    "\tprint(\n",
    "\t\tf\"Safety Horizon Derived: {safety_horizon:.4f} (Max Seed: {max_consensus_dist:.4f})\"\n",
    "\t)\n",
    "else:\n",
    "\t# Fallback if Jaccard finds nothing (rare)\n",
    "\tsafety_horizon = None\n",
    "\tprint(\"Warning: No Jaccard seeds. Safety Horizon disabled.\")\n",
    "\n",
    "# --- STEP 2: Probabilistic Modeling ---\n",
    "print(\"\\n>>> Phase 2: Running EVT Pipeline...\")\n",
    "k_neighbors = 40\n",
    "\n",
    "rank_matrices = {}\n",
    "for key, artifact in artifacts.items():\n",
    "\t# PASS THE SAFETY HORIZON HERE\n",
    "\trank_matrices[key] = computeLocalDensityRanks(\n",
    "\t\tartifact[\"dist_matrix\"], k_neighbors=k_neighbors, max_valid_dist=safety_horizon\n",
    "\t)\n",
    "\n",
    "fused_matrix = computeFusedEVTStatistic(rank_matrices)\n",
    "\n",
    "# Re-init params since the rank inputs have changed (gated items are now 1.0)\n",
    "init_params = _initializeParametersViaConsensus(fused_matrix, consensus_pairs)\n",
    "print(f\"Bootstrapped Params: {init_params}\")\n",
    "\n",
    "likelihood_matrix, final_params = fitWeibullBetaMixture(\n",
    "\tfused_matrix, initial_params=None\n",
    ")\n",
    "print(\"EM Model Converged.\")\n",
    "\n",
    "\n",
    "# --- STEP 3: Topology & Results ---\n",
    "print(\"\\n>>> Phase 3: Final Inference\")\n",
    "prior_matrix = computeTransitivityPrior(likelihood_matrix)\n",
    "posterior_matrix = computeFinalPosterior(likelihood_matrix, prior_matrix)\n",
    "\n",
    "\n",
    "final_matrix = posterior_matrix * prior_matrix\n",
    "print(\"Applying Conservative Symmetrization...\")\n",
    "final_symmetric_matrix = np.minimum(final_matrix, final_matrix.T)\n",
    "extractAndPrintClusters(\n",
    "\tfinal_matrix, artifacts[model_keys[0]][\"semantic_data\"], threshold=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9da0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Phase 1: Deriving Consensus Priors (ACGC)...\n",
      "Consensus Anchors Found: 5\n",
      "Safety Horizon Derived: 13.8013 (Max Seed: 9.2009)\n",
      "\n",
      ">>> Phase 2: Running EVT Pipeline...\n",
      "Bootstrapped Params: {'pi': 1.8526752630798872e-05, 'lambda': np.float64(39.99840006399744), 'alpha': 1.0, 'beta': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilbowry/AICompatibleWEB/.venv/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/emilbowry/AICompatibleWEB/.venv/lib/python3.13/site-packages/numpy/_core/_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/emilbowry/AICompatibleWEB/.venv/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:4268: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/emilbowry/AICompatibleWEB/.venv/lib/python3.13/site-packages/numpy/_core/_methods.py:180: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/Users/emilbowry/AICompatibleWEB/.venv/lib/python3.13/site-packages/numpy/_core/_methods.py:214: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM Model Converged.\n",
      "\n",
      ">>> Phase 3: Final Inference\n",
      "Applying Conservative Symmetrization...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nAgglomerativeClustering does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[156]\u001b[39m\u001b[32m, line 105\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mApplying Conservative Symmetrization...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    104\u001b[39m final_symmetric_matrix = np.minimum(final_matrix, final_matrix.T)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[43mextractAndPrintClusters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mfinal_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifacts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_keys\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msemantic_data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\n\u001b[32m    107\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[33;03mAgglomerativeClustering does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[146]\u001b[39m\u001b[32m, line 612\u001b[39m, in \u001b[36mextractAndPrintClusters\u001b[39m\u001b[34m(posterior_matrix, semantic_data, threshold)\u001b[39m\n\u001b[32m    605\u001b[39m dist_matrix = np.clip(\u001b[32m1.0\u001b[39m - posterior_matrix, \u001b[32m0.0\u001b[39m, \u001b[32m1.0\u001b[39m)\n\u001b[32m    606\u001b[39m model = AgglomerativeClustering(\n\u001b[32m    607\u001b[39m \tn_clusters=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    608\u001b[39m \tdistance_threshold=(\u001b[32m1.0\u001b[39m - threshold),\n\u001b[32m    609\u001b[39m \tmetric=\u001b[33m\"\u001b[39m\u001b[33mprecomputed\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    610\u001b[39m \tlinkage=\u001b[33m\"\u001b[39m\u001b[33mcomplete\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    611\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m labels = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    614\u001b[39m groups = {}\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(labels):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/cluster/_agglomerative.py:1116\u001b[39m, in \u001b[36mAgglomerativeClustering.fit_predict\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m   1095\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1096\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit and return the result of each sample's clustering assignment.\u001b[39;00m\n\u001b[32m   1097\u001b[39m \n\u001b[32m   1098\u001b[39m \u001b[33;03m    In addition to fitting, this method also return the result of the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m \u001b[33;03m        Cluster labels.\u001b[39;00m\n\u001b[32m   1115\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/base.py:695\u001b[39m, in \u001b[36mClusterMixin.fit_predict\u001b[39m\u001b[34m(self, X, y, **kwargs)\u001b[39m\n\u001b[32m    672\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    673\u001b[39m \u001b[33;03mPerform clustering on `X` and returns cluster labels.\u001b[39;00m\n\u001b[32m    674\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    691\u001b[39m \u001b[33;03m    Cluster labels.\u001b[39;00m\n\u001b[32m    692\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    693\u001b[39m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[32m    694\u001b[39m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.labels_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/cluster/_agglomerative.py:990\u001b[39m, in \u001b[36mAgglomerativeClustering.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    971\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    972\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    973\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the hierarchical clustering from features, or distance matrix.\u001b[39;00m\n\u001b[32m    974\u001b[39m \n\u001b[32m    975\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    988\u001b[39m \u001b[33;03m        Returns the fitted instance.\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m990\u001b[39m     X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    991\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nAgglomerativeClustering does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "print(\"\\n>>> Phase 1: Deriving Consensus Priors (ACGC)...\")\n",
    "max_dist = max(np.max(a[\"dist_matrix\"]) for a in artifacts.values())\n",
    "tau_range = np.arange(0.1, max_dist, 0.05)\n",
    "cache = createClusteringCache(artifacts, tau_range)\n",
    "\n",
    "consensus_pairs = findConsensusViaACGC(cache, model_keys)\n",
    "# consensus_pairs = findConsensusViaJaccard(cache, model_keys) # doesnt work\n",
    "# gives AgglomerativeClustering does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
    "# importantly ACGC isnt the precise labels, just most consensus sets\n",
    "print(f\"Consensus Anchors Found: {len(consensus_pairs)}\")\n",
    "\n",
    "\n",
    "def findConsensusViaJaccard(clustering_cache, model_keys):\n",
    "\t\"\"\"\n",
    "\tFinds the 'Platinum' set: Pairs that appear in ALL models at the same\n",
    "\tthreshold configuration that maximizes Intersection-over-Union.\n",
    "\t\"\"\"\n",
    "\tthreshold_axes = [list(clustering_cache[m].keys()) for m in model_keys]\n",
    "\tbest_score = -1\n",
    "\tbest_p_true = set()\n",
    "\n",
    "\t# Grid Search\n",
    "\tfor thresholds in product(*threshold_axes):\n",
    "\t\tcurrent_config = dict(zip(model_keys, thresholds))\n",
    "\t\tpair_sets = [clustering_cache[m][t][2] for m, t in current_config.items()]\n",
    "\n",
    "\t\t# INTERSECTION (The Core)\n",
    "\t\tp_true = set.intersection(*pair_sets)\n",
    "\t\tif not p_true:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# UNION (The Broad Net)\n",
    "\t\tp_union = set.union(*pair_sets)\n",
    "\t\tif not p_union:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# Jaccard Score\n",
    "\t\tcurrent_score = len(p_true) / len(p_union)\n",
    "\n",
    "\t\t# We prefer high Jaccard, but also want non-trivial size\n",
    "\t\t# (Tie-breaker: larger set size)\n",
    "\t\tif current_score > best_score:\n",
    "\t\t\tbest_score = current_score\n",
    "\t\t\tbest_p_true = p_true\n",
    "\t\telif current_score == best_score:\n",
    "\t\t\tif len(p_true) > len(best_p_true):\n",
    "\t\t\t\tbest_p_true = p_true\n",
    "\n",
    "\treturn best_p_true\n",
    "\n",
    "\n",
    "# --- STEP 1.5: Derive Strict Safety Horizon ---\n",
    "consensus_dists = []\n",
    "safety_horizon = 0\n",
    "for key in model_keys:\n",
    "\tmat = artifacts[key][\"dist_matrix\"]\n",
    "\tfor i, j in consensus_pairs:\n",
    "\t\tconsensus_dists.append(mat[i, j])\n",
    "\n",
    "if consensus_dists:\n",
    "\tmax_consensus_dist = np.max(consensus_dists)\n",
    "\t# Jaccard pairs are very tight (approx Dist 8.8).\n",
    "\t# 1.5x buffer -> ~13.2. This allows for valid fuzzy matches\n",
    "\t# but filters out the \"Child Privacy\" hallucination (Dist ~15+).\n",
    "\tsafety_horizon = max_consensus_dist * 1.5\n",
    "\tprint(\n",
    "\t\tf\"Safety Horizon Derived: {safety_horizon:.4f} (Max Seed: {max_consensus_dist:.4f})\"\n",
    "\t)\n",
    "else:\n",
    "\t# Fallback if Jaccard finds nothing (rare)\n",
    "\tsafety_horizon = None\n",
    "\tprint(\"Warning: No Jaccard seeds. Safety Horizon disabled.\")\n",
    "\n",
    "# --- STEP 2: Probabilistic Modeling ---\n",
    "print(\"\\n>>> Phase 2: Running EVT Pipeline...\")\n",
    "k_neighbors = 40\n",
    "\n",
    "rank_matrices = {}\n",
    "for key, artifact in artifacts.items():\n",
    "\t# PASS THE SAFETY HORIZON HERE\n",
    "\trank_matrices[key] = computeLocalDensityRanks(\n",
    "\t\tartifact[\"dist_matrix\"], k_neighbors=k_neighbors, max_valid_dist=safety_horizon\n",
    "\t)\n",
    "\n",
    "fused_matrix = computeFusedEVTStatistic(rank_matrices)\n",
    "\n",
    "# Re-init params since the rank inputs have changed (gated items are now 1.0)\n",
    "init_params = _initializeParametersViaConsensus(fused_matrix, consensus_pairs)\n",
    "print(f\"Bootstrapped Params: {init_params}\")\n",
    "\n",
    "likelihood_matrix, final_params = fitWeibullBetaMixture(\n",
    "\tfused_matrix, initial_params=None\n",
    ")\n",
    "print(\"EM Model Converged.\")\n",
    "\n",
    "\n",
    "# --- STEP 3: Topology & Results ---\n",
    "print(\"\\n>>> Phase 3: Final Inference\")\n",
    "prior_matrix = computeTransitivityPrior(likelihood_matrix)\n",
    "posterior_matrix = computeFinalPosterior(likelihood_matrix, prior_matrix)\n",
    "\n",
    "\n",
    "final_matrix = posterior_matrix * prior_matrix\n",
    "print(\"Applying Conservative Symmetrization...\")\n",
    "final_symmetric_matrix = np.minimum(final_matrix, final_matrix.T)\n",
    "extractAndPrintClusters(\n",
    "\tfinal_matrix, artifacts[model_keys[0]][\"semantic_data\"], threshold=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c541e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a7f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safety Horizon Derived: 13.8013 (Max Seed: 9.2009)\n",
      "\n",
      ">>> Phase 2: Running EVT Pipeline...\n",
      "Bootstrapped Params: {'pi': 1.8526752630798872e-05, 'lambda': np.float64(39.99840006399744), 'alpha': 1.0, 'beta': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilbowry/AICompatibleWEB/.venv/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/emilbowry/AICompatibleWEB/.venv/lib/python3.13/site-packages/numpy/_core/_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/emilbowry/AICompatibleWEB/.venv/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:4268: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/emilbowry/AICompatibleWEB/.venv/lib/python3.13/site-packages/numpy/_core/_methods.py:180: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/Users/emilbowry/AICompatibleWEB/.venv/lib/python3.13/site-packages/numpy/_core/_methods.py:214: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM Model Converged.\n",
      "\n",
      ">>> Phase 3: Final Inference\n",
      "Applying Conservative Symmetrization...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nAgglomerativeClustering does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[153]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mApplying Conservative Symmetrization...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m final_symmetric_matrix = np.minimum(final_matrix, final_matrix.T)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43mextractAndPrintClusters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mfinal_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifacts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_keys\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msemantic_data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\n\u001b[32m     57\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[146]\u001b[39m\u001b[32m, line 612\u001b[39m, in \u001b[36mextractAndPrintClusters\u001b[39m\u001b[34m(posterior_matrix, semantic_data, threshold)\u001b[39m\n\u001b[32m    605\u001b[39m dist_matrix = np.clip(\u001b[32m1.0\u001b[39m - posterior_matrix, \u001b[32m0.0\u001b[39m, \u001b[32m1.0\u001b[39m)\n\u001b[32m    606\u001b[39m model = AgglomerativeClustering(\n\u001b[32m    607\u001b[39m \tn_clusters=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    608\u001b[39m \tdistance_threshold=(\u001b[32m1.0\u001b[39m - threshold),\n\u001b[32m    609\u001b[39m \tmetric=\u001b[33m\"\u001b[39m\u001b[33mprecomputed\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    610\u001b[39m \tlinkage=\u001b[33m\"\u001b[39m\u001b[33mcomplete\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    611\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m labels = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    614\u001b[39m groups = {}\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(labels):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/cluster/_agglomerative.py:1116\u001b[39m, in \u001b[36mAgglomerativeClustering.fit_predict\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m   1095\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1096\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit and return the result of each sample's clustering assignment.\u001b[39;00m\n\u001b[32m   1097\u001b[39m \n\u001b[32m   1098\u001b[39m \u001b[33;03m    In addition to fitting, this method also return the result of the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m \u001b[33;03m        Cluster labels.\u001b[39;00m\n\u001b[32m   1115\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/base.py:695\u001b[39m, in \u001b[36mClusterMixin.fit_predict\u001b[39m\u001b[34m(self, X, y, **kwargs)\u001b[39m\n\u001b[32m    672\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    673\u001b[39m \u001b[33;03mPerform clustering on `X` and returns cluster labels.\u001b[39;00m\n\u001b[32m    674\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    691\u001b[39m \u001b[33;03m    Cluster labels.\u001b[39;00m\n\u001b[32m    692\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    693\u001b[39m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[32m    694\u001b[39m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.labels_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/cluster/_agglomerative.py:990\u001b[39m, in \u001b[36mAgglomerativeClustering.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    971\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    972\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    973\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the hierarchical clustering from features, or distance matrix.\u001b[39;00m\n\u001b[32m    974\u001b[39m \n\u001b[32m    975\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    988\u001b[39m \u001b[33;03m        Returns the fitted instance.\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m990\u001b[39m     X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    991\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nAgglomerativeClustering does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc61871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(13.80133726823928)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5436ab36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Phase 2: Running EVT Pipeline...\n",
      "Bootstrapped Params: {'pi': 1.8526752630798872e-05, 'lambda': np.float64(39.99840006399744), 'alpha': 1.0, 'beta': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilbowry/AICompatibleWEB/.venv/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/emilbowry/AICompatibleWEB/.venv/lib/python3.13/site-packages/numpy/_core/_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/emilbowry/AICompatibleWEB/.venv/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:4268: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  return _methods._var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/Users/emilbowry/AICompatibleWEB/.venv/lib/python3.13/site-packages/numpy/_core/_methods.py:180: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "/Users/emilbowry/AICompatibleWEB/.venv/lib/python3.13/site-packages/numpy/_core/_methods.py:214: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM Model Converged.\n",
      "\n",
      ">>> Phase 3: Final Inference\n",
      "Applying Conservative Symmetrization...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nAgglomerativeClustering does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[151]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mApplying Conservative Symmetrization...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# final_symmetric_matrix = np.minimum(final_matrix, final_matrix.T)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mextractAndPrintClusters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mfinal_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifacts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_keys\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msemantic_data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\n\u001b[32m     35\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[146]\u001b[39m\u001b[32m, line 612\u001b[39m, in \u001b[36mextractAndPrintClusters\u001b[39m\u001b[34m(posterior_matrix, semantic_data, threshold)\u001b[39m\n\u001b[32m    605\u001b[39m dist_matrix = np.clip(\u001b[32m1.0\u001b[39m - posterior_matrix, \u001b[32m0.0\u001b[39m, \u001b[32m1.0\u001b[39m)\n\u001b[32m    606\u001b[39m model = AgglomerativeClustering(\n\u001b[32m    607\u001b[39m \tn_clusters=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    608\u001b[39m \tdistance_threshold=(\u001b[32m1.0\u001b[39m - threshold),\n\u001b[32m    609\u001b[39m \tmetric=\u001b[33m\"\u001b[39m\u001b[33mprecomputed\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    610\u001b[39m \tlinkage=\u001b[33m\"\u001b[39m\u001b[33mcomplete\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    611\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m612\u001b[39m labels = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    614\u001b[39m groups = {}\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(labels):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/cluster/_agglomerative.py:1116\u001b[39m, in \u001b[36mAgglomerativeClustering.fit_predict\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m   1095\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1096\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit and return the result of each sample's clustering assignment.\u001b[39;00m\n\u001b[32m   1097\u001b[39m \n\u001b[32m   1098\u001b[39m \u001b[33;03m    In addition to fitting, this method also return the result of the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m \u001b[33;03m        Cluster labels.\u001b[39;00m\n\u001b[32m   1115\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/base.py:695\u001b[39m, in \u001b[36mClusterMixin.fit_predict\u001b[39m\u001b[34m(self, X, y, **kwargs)\u001b[39m\n\u001b[32m    672\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    673\u001b[39m \u001b[33;03mPerform clustering on `X` and returns cluster labels.\u001b[39;00m\n\u001b[32m    674\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    691\u001b[39m \u001b[33;03m    Cluster labels.\u001b[39;00m\n\u001b[32m    692\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    693\u001b[39m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[32m    694\u001b[39m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.labels_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/cluster/_agglomerative.py:990\u001b[39m, in \u001b[36mAgglomerativeClustering.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    971\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    972\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    973\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the hierarchical clustering from features, or distance matrix.\u001b[39;00m\n\u001b[32m    974\u001b[39m \n\u001b[32m    975\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    988\u001b[39m \u001b[33;03m        Returns the fitted instance.\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m990\u001b[39m     X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    991\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/sklearn/utils/validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nAgglomerativeClustering does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# --- STEP 2: Probabilistic Modeling ---\n",
    "print(\"\\n>>> Phase 2: Running EVT Pipeline...\")\n",
    "k_neighbors = 40\n",
    "\n",
    "rank_matrices = {}\n",
    "for key, artifact in artifacts.items():\n",
    "\t# PASS THE SAFETY HORIZON HERE\n",
    "\trank_matrices[key] = computeLocalDensityRanks(\n",
    "\t\tartifact[\"dist_matrix\"], k_neighbors=k_neighbors, max_valid_dist=safety_horizon\n",
    "\t)\n",
    "\n",
    "fused_matrix = computeFusedEVTStatistic(rank_matrices)\n",
    "\n",
    "# Re-init params since the rank inputs have changed (gated items are now 1.0)\n",
    "init_params = _initializeParametersViaConsensus(fused_matrix, consensus_pairs)\n",
    "print(f\"Bootstrapped Params: {init_params}\")\n",
    "\n",
    "likelihood_matrix, final_params = fitWeibullBetaMixture(\n",
    "\tfused_matrix, initial_params=None\n",
    ")\n",
    "print(\"EM Model Converged.\")\n",
    "\n",
    "\n",
    "# --- STEP 3: Topology & Results ---\n",
    "print(\"\\n>>> Phase 3: Final Inference\")\n",
    "prior_matrix = computeTransitivityPrior(likelihood_matrix)\n",
    "posterior_matrix = computeFinalPosterior(likelihood_matrix, prior_matrix)\n",
    "\n",
    "\n",
    "# final_matrix = posterior_matrix * prior_matrix\n",
    "print(\"Applying Conservative Symmetrization...\")\n",
    "final_symmetric_matrix = np.minimum(final_matrix, final_matrix.T)\n",
    "extractAndPrintClusters(\n",
    "\tfinal_matrix, artifacts[model_keys[0]][\"semantic_data\"], threshold=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf15f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Phase 2: Running EVT Pipeline...\n",
      "Bootstrapped Params: {'pi': 0.00047428486734845115, 'lambda': np.float64(62.555015132292255), 'alpha': np.float64(1.3946512320706785), 'beta': np.float64(3.024188090720149)}\n",
      "EM Model Converged.\n",
      "Applying Lexical Verification Prior...\n",
      "\n",
      ">>> Phase 3: Final Inference\n",
      "Applying Conservative Symmetrization...\n",
      "\n",
      "================================================================================\n",
      "PROBABILISTIC CLUSTERING RESULTS (Threshold P > 0.5)\n",
      "================================================================================\n",
      "Found 13 significant groups.\n",
      "\n",
      "GROUP 1 (Size: 2) [Cohesion: 0.5316]\n",
      " - Does the privacy policy affirm that security measures are designed to protect personal data from unauthorized access?\n",
      " - Does the privacy policy affirm that security measures are designed to protect personal data from unauthorized disclosure?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 2 (Size: 2) [Cohesion: 0.5048]\n",
      " - Does the privacy policy affirm that the Services are not directed to children under 13?\n",
      " - Does the privacy policy affirm that the Services are not intended for children under 13?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 3 (Size: 2) [Cohesion: 0.5014]\n",
      " - Does the privacy policy affirm that the company does not knowingly collect information from children under the age of 18?\n",
      " - Does the privacy policy affirm that the company does not knowingly share information from children under the age of 18?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 4 (Size: 2) [Cohesion: 0.5402]\n",
      " - Does the privacy policy affirm that the company implements commercially reasonable technical measures to protect Personal Data?\n",
      " - Does the privacy policy affirm that the company implements commercially reasonable organizational measures to protect Personal Data?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 5 (Size: 2) [Cohesion: 0.6611]\n",
      " - Does the privacy policy affirm that user email addresses are entrusted to the domestic representative?\n",
      " - Does the privacy policy affirm that user addresses are entrusted to the domestic representative?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 6 (Size: 2) [Cohesion: 0.5201]\n",
      " - Does the privacy policy affirm that Inputs and Outputs disassociated via Feedback are used for training models?\n",
      " - Does the privacy policy affirm that Inputs and Outputs disassociated via Feedback are used for improving models?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 7 (Size: 2) [Cohesion: 0.5773]\n",
      " - Does the privacy policy affirm that Personal Data is retained to comply with legal obligations?\n",
      " - Does the privacy policy affirm that Personal Data is used to comply with legal obligations?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 8 (Size: 2) [Cohesion: 0.5327]\n",
      " - Does the privacy policy affirm that providing personal data constitutes agreement to the transfer of data outside of Canada?\n",
      " - Does the privacy policy affirm that providing personal data constitutes agreement to the disclosure of data outside of Canada?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 9 (Size: 2) [Cohesion: 0.5969]\n",
      " - Does the privacy policy affirm that Personal Data is used to improve the company's services?\n",
      " - Does the privacy policy affirm that Personal Data is used to develop the company's services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 10 (Size: 2) [Cohesion: 0.5197]\n",
      " - Does the privacy policy affirm that Personal Data is used to send information about services and events?\n",
      " - Does the privacy policy affirm that personal data is used to send information about events?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 11 (Size: 2) [Cohesion: 0.5768]\n",
      " - Does the privacy policy affirm that Personal Data is used to protect the rights, privacy, safety, or property of users?\n",
      " - Does the privacy policy affirm that Personal Data is used to protect the rights, privacy, safety, or property of the company?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 12 (Size: 2) [Cohesion: 0.6804]\n",
      " - Does the privacy policy affirm that the company collects the dates and times of access?\n",
      " - Does the privacy policy affirm that the company collects the dates and times of access to the services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 13 (Size: 2) [Cohesion: 0.7020]\n",
      " - Does the privacy policy affirm that users have the right to request the correction of inaccurate personal data?\n",
      " - Does the privacy policy affirm that users have the right to request the correction of inaccurate data?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def computeLexicalVerificationPrior(semantic_data, sensitivity=0.8):\n",
    "\t\"\"\"\n",
    "\tCalculates a penalty matrix based on discrete token mismatches.\n",
    "\t\"\"\"\n",
    "\tN = len(semantic_data)\n",
    "\tprior_matrix = np.ones((N, N), dtype=np.float64)\n",
    "\n",
    "\t# User requested to omit 'privacy' and 'policy' from stopwords\n",
    "\tSTOPWORDS = {\n",
    "\t\t\"the\",\n",
    "\t\t\"a\",\n",
    "\t\t\"an\",\n",
    "\t\t\"of\",\n",
    "\t\t\"to\",\n",
    "\t\t\"in\",\n",
    "\t\t\"for\",\n",
    "\t\t\"on\",\n",
    "\t\t\"by\",\n",
    "\t\t\"with\",\n",
    "\t\t\"is\",\n",
    "\t\t\"are\",\n",
    "\t\t\"was\",\n",
    "\t\t\"were\",\n",
    "\t\t\"be\",\n",
    "\t\t\"been\",\n",
    "\t\t\"that\",\n",
    "\t\t\"this\",\n",
    "\t\t\"it\",\n",
    "\t\t\"not\",\n",
    "\t\t\"or\",\n",
    "\t\t\"and\",\n",
    "\t\t\"does\",\n",
    "\t\t\"affirm\",\n",
    "\t}\n",
    "\n",
    "\tdef get_tokens(text):\n",
    "\t\twords = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "\t\treturn set(w for w in words if w not in STOPWORDS)\n",
    "\n",
    "\ttoken_sets = [get_tokens(s) for s in semantic_data]\n",
    "\n",
    "\tfor i in range(N):\n",
    "\t\tfor j in range(i + 1, N):\n",
    "\t\t\tset_a = token_sets[i]\n",
    "\t\t\tset_b = token_sets[j]\n",
    "\n",
    "\t\t\tdiff = (set_a - set_b) | (set_b - set_a)\n",
    "\t\t\tmismatch_count = len(diff)\n",
    "\n",
    "\t\t\tif mismatch_count > 0:\n",
    "\t\t\t\tpenalty = np.exp(-sensitivity * mismatch_count)\n",
    "\t\t\t\tprior_matrix[i, j] = penalty\n",
    "\t\t\t\tprior_matrix[j, i] = penalty\n",
    "\n",
    "\treturn prior_matrix\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4fe6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.06319536285627\n",
      "25.353436297260522\n"
     ]
    }
   ],
   "source": [
    "print(np.max(consensus_dists))\n",
    "print(np.max(dist_data))\n",
    "\n",
    "# 17.06319536285627\n",
    "# 25.353436297260522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fbfbad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25b9879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Phase 2: Running EVT Pipeline...\n",
      "Bootstrapped Params: {'pi': 0.00047428486734845115, 'lambda': np.float64(62.555015132292255), 'alpha': np.float64(1.3946512320706785), 'beta': np.float64(3.024188090720149)}\n",
      "EM Model Converged.\n",
      "Applying Lexical Verification Prior...\n",
      "\n",
      ">>> Phase 3: Final Inference\n",
      "Applying Conservative Symmetrization...\n",
      "\n",
      "================================================================================\n",
      "PROBABILISTIC CLUSTERING RESULTS (Threshold P > 0.5)\n",
      "================================================================================\n",
      "Found 64 significant groups.\n",
      "\n",
      "GROUP 1 (Size: 20) [Cohesion: 0.8015]\n",
      " - Does the privacy policy affirm that the company implements commercially reasonable technical measures to protect Personal Data?\n",
      " - Does the privacy policy affirm that the company implements commercially reasonable organizational measures to protect Personal Data?\n",
      " - Does the privacy policy affirm that the company relies on Standard Contractual Clauses (SCCs) for transfers to countries without an adequacy decision?\n",
      " - Does the privacy policy affirm that the company relies on Standard Contractual Clauses (SCCs) for transfers to jurisdictions without adequacy decisions?\n",
      " - Does the privacy policy affirm that processing contact information to send technical announcements is based on the necessity to perform a contract?\n",
      " - Does the privacy policy affirm that the company relies on user consent to process contact information for specific marketing communications?\n",
      " - Does the privacy policy affirm that providing personal data constitutes agreement to the transfer of data outside of Canada?\n",
      " - Does the privacy policy affirm that providing personal data constitutes agreement to the disclosure of data outside of Canada?\n",
      " - Does the privacy policy affirm that administrators of enterprise or business accounts may access and control a user's account?\n",
      " - Does the privacy policy affirm that administrators of enterprise or business accounts may access a user's Content?\n",
      " - Does the privacy policy affirm that the company collects the user's time zone?\n",
      " - Does the privacy policy affirm that the company collects the dates and times of access?\n",
      " - Does the privacy policy affirm that users have the right to request the correction of inaccurate personal data?\n",
      " - Does the privacy policy affirm that users have the right to request the correction of inaccurate data?\n",
      " - Does the privacy policy affirm that the company may disclose personal data to governmental regulatory authorities as required by law?\n",
      " - Does the privacy policy affirm that the company may disclose personal data in response to requests from governmental regulatory authorities?\n",
      " - Does the privacy policy affirm that the company collects time zone settings?\n",
      " - Does the privacy policy affirm that the company collects the dates and times of access to the services?\n",
      " - Does the privacy policy affirm that the company processes contact information to send technical announcements based on the necessity to perform a contract?\n",
      " - Does the privacy policy affirm that the company processes contact information for marketing communications based on user consent?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 2 (Size: 8) [Cohesion: 0.6770]\n",
      " - Does the privacy policy affirm that the Services are not directed to children under 13?\n",
      " - Does the privacy policy affirm that the Services are not intended for children under 13?\n",
      " - Does the privacy policy affirm that Inputs and Outputs disassociated via Feedback are used for training models?\n",
      " - Does the privacy policy affirm that Inputs and Outputs disassociated via Feedback are used for improving models?\n",
      " - Does the privacy policy affirm that the company collects the user's name if they communicate via email or social media pages?\n",
      " - Does the privacy policy affirm that the company collects contact information if the user communicates via email or social media pages?\n",
      " - Does the privacy policy affirm that the company collects the operating system of the device used to access the Services?\n",
      " - Does the privacy policy affirm that the company collects operating system information?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 3 (Size: 5) [Cohesion: 0.6941]\n",
      " - Does the privacy policy affirm that the company trains its models using data provided by users?\n",
      " - Does the privacy policy affirm that user-provided Content may be used to train the company's models?\n",
      " - Does the privacy policy affirm that the company honors global privacy controls?\n",
      " - Does the privacy policy affirm that personal data is used to train the company's models?\n",
      " - Does the privacy policy affirm that user Inputs and Outputs may be used to train the company's models?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 4 (Size: 5) [Cohesion: 0.8082]\n",
      " - Does the privacy policy affirm that Personal Data is retained to comply with legal obligations?\n",
      " - Does the privacy policy affirm that Personal Data is used to comply with legal obligations?\n",
      " - Does the privacy policy affirm that users have the statutory right to access their Personal Data?\n",
      " - Does the privacy policy affirm that users have the statutory right to access information relating to how their Personal Data is processed?\n",
      " - Does the privacy policy affirm that the company may disclose personal data to assist in investigations by governmental regulatory authorities?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 5 (Size: 4) [Cohesion: 0.6725]\n",
      " - Does the privacy policy affirm that the Data Protection Officer can be contacted via email regarding matters related to Personal Data processing?\n",
      " - Does the privacy policy affirm that users can contact the company's Data Protection Officer via email?\n",
      " - Does the privacy policy affirm that if a user creates an account using an email address belonging to an employer, the company may share the fact that the user has an account with that employer?\n",
      " - Does the privacy policy affirm that if a user creates an account using an email address belonging to an employer, the company may share account information with the employer to enable the user to be added to a business account?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 6 (Size: 3) [Cohesion: 0.6711]\n",
      " - Does the privacy policy affirm that security measures are designed to protect personal data from unauthorized access?\n",
      " - Does the privacy policy affirm that security measures are designed to protect personal data from unauthorized disclosure?\n",
      " - Does the privacy policy affirm that the company may store information via cookies for users who do not create an account?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 7 (Size: 3) [Cohesion: 0.6938]\n",
      " - Does the privacy policy affirm that the company does not knowingly collect information from children under the age of 18?\n",
      " - Does the privacy policy affirm that the company does not knowingly share information from children under the age of 18?\n",
      " - Does the privacy policy affirm that the company receives information from marketing vendors about potential customers?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 8 (Size: 3) [Cohesion: 0.7241]\n",
      " - Does the privacy policy affirm that no Internet or email transmission is ever fully secure or error free?\n",
      " - Does the privacy policy affirm that Personal Data is used to protect the rights, privacy, safety, or property of users?\n",
      " - Does the privacy policy affirm that Personal Data is used to protect the rights, privacy, safety, or property of the company?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 9 (Size: 3) [Cohesion: 0.6719]\n",
      " - Does the privacy policy affirm that the company utilizes trustees to assist with domestic representative duties?\n",
      " - Does the privacy policy affirm that user email addresses are entrusted to the domestic representative?\n",
      " - Does the privacy policy affirm that user addresses are entrusted to the domestic representative?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 10 (Size: 3) [Cohesion: 0.5868]\n",
      " - Does the privacy policy affirm that the recipient location for entrusted data is South Korea?\n",
      " - Does the privacy policy affirm that personal data is used to investigate security issues?\n",
      " - Does the privacy policy affirm that personal data is used to resolve security issues?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 11 (Size: 3) [Cohesion: 0.7075]\n",
      " - Does the privacy policy affirm that users have the right to lodge a complaint with the supervisory authority in the place where they work?\n",
      " - Does the privacy policy affirm that the company collects publicly available information from the internet to develop the models powering its Services?\n",
      " - Does the privacy policy affirm that personal data is used to send information about events?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 12 (Size: 3) [Cohesion: 0.7740]\n",
      " - Does the privacy policy affirm that Personal Data is used to improve the company's services?\n",
      " - Does the privacy policy affirm that Personal Data is used to develop the company's services?\n",
      " - Does the privacy policy affirm that the company collects mobile network information?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 13 (Size: 3) [Cohesion: 0.5626]\n",
      " - Does the privacy policy affirm that users can interact with the services via chat sessions?\n",
      " - Does the privacy policy affirm that users can interact with the services via coding sessions?\n",
      " - Does the privacy policy affirm that users can interact with the services via agentic sessions?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 14 (Size: 3) [Cohesion: 0.5960]\n",
      " - Does the privacy policy affirm that the company may disclose Personal Data to content delivery services?\n",
      " - Does the privacy policy affirm that personal data is used to investigate disputes?\n",
      " - Does the privacy policy affirm that personal data is used to resolve disputes?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 15 (Size: 3) [Cohesion: 0.7713]\n",
      " - Does the privacy policy affirm that users can send information to third-party applications via custom actions or web searches?\n",
      " - Does the privacy policy affirm that personal data may be shared with service providers for data processing purposes?\n",
      " - Does the privacy policy affirm that personal data may be shared with service providers for the purpose of providing services to the user?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 16 (Size: 3) [Cohesion: 0.5853]\n",
      " - Does the privacy policy affirm that users have the right to object to the processing of their Personal Data when the processing is based on legitimate interests?\n",
      " - Does the privacy policy affirm that users have the right to object to the processing of their personal data?\n",
      " - Does the privacy policy affirm that the company collects web page referrers?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 17 (Size: 2) [Cohesion: 0.5514]\n",
      " - Does the privacy policy affirm that users should contact the company via email if they believe a child under 13 has provided Personal Data to the company?\n",
      " - Does the privacy policy affirm that users should contact the company via email if they become aware that a child under 18 has provided personal data?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 18 (Size: 2) [Cohesion: 0.5671]\n",
      " - Does the privacy policy affirm that the company does not knowingly disclose information from children under the age of 18?\n",
      " - Does the privacy policy affirm that user consent may be implied depending on the circumstances and sensitivity of information?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 19 (Size: 2) [Cohesion: 0.5083]\n",
      " - Does the privacy policy affirm that the company trains its models using publicly available information from the Internet?\n",
      " - Does the privacy policy affirm that the company collects probabilistic identifiers?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 20 (Size: 2) [Cohesion: 0.6088]\n",
      " - Does the privacy policy affirm that the company uses Standard Contractual Clauses to transfer information to certain affiliates?\n",
      " - Does the privacy policy affirm that the company uses Standard Contractual Clauses to transfer information to third parties?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 21 (Size: 2) [Cohesion: 0.5315]\n",
      " - Does the privacy policy affirm that it does not apply to content processed on behalf of customers of the company's business offerings?\n",
      " - Does the privacy policy affirm that the use of data processed on behalf of business customers is governed by separate customer agreements?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 22 (Size: 2) [Cohesion: 0.5660]\n",
      " - Does the privacy policy affirm that it does not apply where the company acts as a data processor?\n",
      " - Does the privacy policy affirm that the company acts as a data processor with respect to personal data received when users utilize the company's commercial services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 23 (Size: 2) [Cohesion: 0.5040]\n",
      " - Does the privacy policy affirm that information regarding how Large Language Models are trained is located in the Non-User Privacy Policy?\n",
      " - Does the privacy policy affirm that information regarding how personal data obtained from third-party sources is used can be found in the Non-User Privacy Policy?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 24 (Size: 2) [Cohesion: 0.6566]\n",
      " - Does the privacy policy affirm that the company performs necessary procedures for handling data when it is no longer required?\n",
      " - Does the privacy policy affirm that service providers perform necessary procedures for handling data when it is no longer required?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 25 (Size: 2) [Cohesion: 0.5032]\n",
      " - Does the privacy policy affirm that personal data may be converted into an anonymous form when it is no longer required?\n",
      " - Does the privacy policy affirm that the disposal or anonymization of data is performed as permitted or required under applicable laws?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 26 (Size: 2) [Cohesion: 0.5322]\n",
      " - Does the privacy policy affirm that countries where data is processed may not offer the same level of data protection as the user's home country?\n",
      " - Does the privacy policy affirm that the company applies the protections described in the policy to Personal Data regardless of where it is processed?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 27 (Size: 2) [Cohesion: 0.5454]\n",
      " - Does the privacy policy affirm that users can contact the company to obtain a copy of the appropriate safeguards in place for data transfers?\n",
      " - Does the privacy policy affirm that users can contact the company via email regarding data transfer safeguards?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 28 (Size: 2) [Cohesion: 0.5160]\n",
      " - Does the privacy policy affirm that the domestic representative acts in compliance with the Personal Information Protection Act?\n",
      " - Does the privacy policy affirm that the domestic representative acts in compliance with the Act on Promotion of Information and Communications Network Utilization and Data Protection?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 29 (Size: 2) [Cohesion: 0.7090]\n",
      " - Does the privacy policy affirm that users can contact the domestic representative via telephone?\n",
      " - Does the privacy policy affirm that users can contact the domestic representative via email?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 30 (Size: 2) [Cohesion: 0.6245]\n",
      " - Does the privacy policy affirm that user names are entrusted to the domestic representative?\n",
      " - Does the privacy policy affirm that user IDs are entrusted to the domestic representative?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 31 (Size: 2) [Cohesion: 0.5055]\n",
      " - Does the privacy policy affirm that data transfer methods to the representative include telephone?\n",
      " - Does the privacy policy affirm that data transfer methods to the representative include email?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 32 (Size: 2) [Cohesion: 0.5346]\n",
      " - Does the privacy policy affirm that the company processes aggregated or de-identified data to conduct research?\n",
      " - Does the privacy policy affirm that Personal Data is used to conduct research?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 33 (Size: 2) [Cohesion: 0.5145]\n",
      " - Does the privacy policy affirm that Personal Data is retained for legitimate business purposes such as resolving disputes?\n",
      " - Does the privacy policy affirm that general location information is used for security reasons, such as detecting unusual login activity?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 34 (Size: 2) [Cohesion: 0.5226]\n",
      " - Does the privacy policy affirm that the supplemental disclosures should be read in conjunction with the rest of the privacy policy?\n",
      " - Does the privacy policy affirm that the supplemental disclosures prevail over the main privacy policy in case of conflict regarding residents of Brazil?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 35 (Size: 2) [Cohesion: 0.6236]\n",
      " - Does the privacy policy affirm that expressly consenting to the policy confirms the user has read and understood the policy?\n",
      " - Does the privacy policy affirm that user consent may be given expressly?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 36 (Size: 2) [Cohesion: 0.5196]\n",
      " - Does the privacy policy affirm that data may be transferred to jurisdictions where data protection laws are less stringent than the user's jurisdiction?\n",
      " - Does the privacy policy affirm that user information may be transferred to countries that may not have data protection laws equivalent to those in the user's country?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 37 (Size: 2) [Cohesion: 0.5228]\n",
      " - Does the privacy policy affirm that users may contact the company via email for questions regarding personal data processing?\n",
      " - Does the privacy policy affirm that users may contact the company via email to exercise their rights?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 38 (Size: 2) [Cohesion: 0.6508]\n",
      " - Does the privacy policy affirm that for users outside the European Region, the data controller is a specific entity of the company?\n",
      " - Does the privacy policy affirm that Personal Data is used to protect the rights, privacy, safety, or property of third parties?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 39 (Size: 2) [Cohesion: 0.5800]\n",
      " - Does the privacy policy affirm that Personal Data is used to provide the company's services?\n",
      " - Does the privacy policy affirm that Personal Data is used to maintain the company's services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 40 (Size: 2) [Cohesion: 0.5394]\n",
      " - Does the privacy policy affirm that Personal Data is used to send information about services and events?\n",
      " - Does the privacy policy affirm that personal data is used to send information about the company's services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 41 (Size: 2) [Cohesion: 0.5633]\n",
      " - Does the privacy policy affirm that Personal Data is used to prevent illegal activity?\n",
      " - Does the privacy policy affirm that the company may share Personal Data to detect or prevent fraud or other illegal activity?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 42 (Size: 2) [Cohesion: 0.5376]\n",
      " - Does the privacy policy affirm that interactions with the service include third-party applications users choose to integrate?\n",
      " - Does the privacy policy affirm that the company's services may involve integrations with websites managed by third parties?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 43 (Size: 2) [Cohesion: 0.5120]\n",
      " - Does the privacy policy affirm that the company collects personal data if it is included in user Inputs?\n",
      " - Does the privacy policy affirm that the company collects Personal Data included in the user's prompts or input to the Services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 44 (Size: 2) [Cohesion: 0.5126]\n",
      " - Does the privacy policy affirm that the company may disclose Personal Data to vendors and service providers to assist with business operations needs?\n",
      " - Does the privacy policy affirm that vendors and service providers process Personal Data only to perform their duties pursuant to the company's instructions?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 45 (Size: 2) [Cohesion: 0.5143]\n",
      " - Does the privacy policy affirm that information users share with third parties is governed by those third parties' terms and privacy policies?\n",
      " - Does the privacy policy affirm that information provided to third parties is subject to the third party's privacy policy?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 46 (Size: 2) [Cohesion: 0.5155]\n",
      " - Does the privacy policy affirm that users have the statutory right to withdraw consent at any time where consent is the legal basis for processing?\n",
      " - Does the privacy policy affirm that users have the right to withdraw consent where processing is based on consent?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 47 (Size: 2) [Cohesion: 0.5193]\n",
      " - Does the privacy policy affirm that the company's services generate responses by predicting the words most likely to appear next?\n",
      " - Does the privacy policy affirm that user Inputs and Outputs may be used to improve the company's services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 48 (Size: 2) [Cohesion: 0.5004]\n",
      " - Does the privacy policy affirm that the company collects the user's name when an account is created?\n",
      " - Does the privacy policy affirm that the company collects contact information when an account is created?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 49 (Size: 2) [Cohesion: 0.5632]\n",
      " - Does the privacy policy affirm that the company collects information when users participate in events?\n",
      " - Does the privacy policy affirm that the company collects information when users participate in surveys?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 50 (Size: 2) [Cohesion: 0.5101]\n",
      " - Does the privacy policy affirm that the company collects browser type and settings as part of Log Data?\n",
      " - Does the privacy policy affirm that the company collects browser information?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 51 (Size: 2) [Cohesion: 0.5095]\n",
      " - Does the privacy policy affirm that the company collects information about the features the user uses?\n",
      " - Does the privacy policy affirm that the company collects information about the actions the user takes?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 52 (Size: 2) [Cohesion: 0.5095]\n",
      " - Does the privacy policy affirm that the company collects the user's country?\n",
      " - Does the privacy policy affirm that user rights regarding personal data depend on their country of residence?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 53 (Size: 2) [Cohesion: 0.5674]\n",
      " - Does the privacy policy affirm that users have the right to know the categories of personal data processed about them?\n",
      " - Does the privacy policy affirm that users have the right to know the categories of third parties to whom their data is disclosed?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 54 (Size: 2) [Cohesion: 0.5938]\n",
      " - Does the privacy policy affirm that the company does not engage in decision-making based solely on automated processing that produces a legal effect?\n",
      " - Does the privacy policy affirm that the company does not engage in decision-making based solely on automated processing that significantly affects the user?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 55 (Size: 2) [Cohesion: 0.5574]\n",
      " - Does the privacy policy affirm that the company may continue to process data despite a request for deletion to comply with legal obligations?\n",
      " - Does the privacy policy affirm that the company may continue to process data despite a request for deletion to safeguard and exercise rights?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 56 (Size: 2) [Cohesion: 0.5563]\n",
      " - Does the privacy policy affirm that the company may disclose personal data for tax purposes?\n",
      " - Does the privacy policy affirm that the company may disclose personal data for accounting purposes?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 57 (Size: 2) [Cohesion: 0.5009]\n",
      " - Does the privacy policy affirm that personal data may be disclosed to enforce the company's legal rights?\n",
      " - Does the privacy policy affirm that personal data may be disclosed to enforce the legal rights of others?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 58 (Size: 2) [Cohesion: 0.5071]\n",
      " - Does the privacy policy affirm that the company will disclose personal data when an individual gives permission?\n",
      " - Does the privacy policy affirm that the company will disclose personal data when an individual directs the company to disclose such information?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 59 (Size: 2) [Cohesion: 0.5455]\n",
      " - Does the privacy policy affirm that device location information is derived from IP addresses?\n",
      " - Does the privacy policy affirm that the company collects device location data?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 60 (Size: 2) [Cohesion: 0.5430]\n",
      " - Does the privacy policy affirm that cookies are used to manage the services?\n",
      " - Does the privacy policy affirm that cookies are used to recognize the user?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 61 (Size: 2) [Cohesion: 0.5042]\n",
      " - Does the privacy policy affirm that cookies are used to customize or personalize the user's experience?\n",
      " - Does the privacy policy affirm that cookies are used to market additional products or services to the user?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 62 (Size: 2) [Cohesion: 0.5198]\n",
      " - Does the privacy policy affirm that inputs and outputs are processed to improve services and conduct research excluding model training?\n",
      " - Does the privacy policy affirm that feedback is processed to improve services and conduct research including model training?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 63 (Size: 2) [Cohesion: 0.6041]\n",
      " - Does the privacy policy affirm that personal data is used to provide products and services related to the user's account?\n",
      " - Does the privacy policy affirm that personal data is used to maintain products and services related to the user's account?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 64 (Size: 2) [Cohesion: 0.5154]\n",
      " - Does the privacy policy affirm that personal data is used to facilitate optional services that enhance platform functionality?\n",
      " - Does the privacy policy affirm that personal data is used to facilitate optional features that enhance user experience?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "safety_horizon = (\n",
    "\t1\t# Changing this doesnt seem to matter, even set at 1 it prints the same\n",
    ")\n",
    "\n",
    "# --- STEP 2: Probabilistic Modeling ---\n",
    "print(\"\\n>>> Phase 2: Running EVT Pipeline...\")\n",
    "k_neighbors = 1\t# Changing this doesnt seem to matter, even 1 prints the same\n",
    "\n",
    "rank_matrices = {}\n",
    "for key, artifact in artifacts.items():\n",
    "\t# PASS THE SAFETY HORIZON HERE\n",
    "\trank_matrices[key] = computeLocalDensityRanks(\n",
    "\t\tartifact[\"dist_matrix\"], k_neighbors=k_neighbors, max_valid_dist=safety_horizon\n",
    "\t)\n",
    "\n",
    "fused_matrix = computeFusedEVTStatistic(rank_matrices)\n",
    "\n",
    "# Re-init params since the rank inputs have changed (gated items are now 1.0)\n",
    "init_params = _initializeParametersViaConsensus(fused_matrix, consensus_pairs)\n",
    "print(f\"Bootstrapped Params: {init_params}\")\n",
    "\n",
    "likelihood_matrix, final_params = fitWeibullBetaMixture(\n",
    "\tfused_matrix, initial_params=None\n",
    ")\n",
    "print(\"EM Model Converged.\")\n",
    "# --- STEP 4: Lexical Verification (Inserted) ---\n",
    "print(\"Applying Lexical Verification Prior...\")\n",
    "lexical_prior = computeLexicalVerificationPrior(\n",
    "\tartifacts[model_keys[0]][\"semantic_data\"],\n",
    "\tsensitivity=0.4,\t# Tunable: 0.8 penalizes single-word swaps heavily\n",
    ")\n",
    "\n",
    "# --- STEP 3: Topology & Results ---\n",
    "print(\"\\n>>> Phase 3: Final Inference\")\n",
    "prior_matrix = computeTransitivityPrior(likelihood_matrix)\n",
    "posterior_matrix = computeFinalPosterior(likelihood_matrix, prior_matrix)\n",
    "\n",
    "\n",
    "final_matrix = posterior_matrix\t# * lexical_prior\n",
    "print(\"Applying Conservative Symmetrization...\")\n",
    "final_symmetric_matrix = np.minimum(final_matrix, final_matrix.T)\n",
    "extractAndPrintClusters(\n",
    "\tfinal_symmetric_matrix, artifacts[model_keys[0]][\"semantic_data\"], threshold=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c3af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- STEP 2: Probabilistic Modeling ---\n",
    "# print(\"\\n>>> Phase 2: Running EVT Pipeline...\")\n",
    "\n",
    "# rank_matrices = {}\n",
    "# for key, artifact in artifacts.items():\n",
    "# \trank_matrices[key] = computeLocalDensityRanks(artifact[\"dist_matrix\"], k_neighbors=30)\n",
    "\n",
    "# fused_matrix = computeFusedEVTStatistic(rank_matrices)\n",
    "# init_params = _initializeParametersViaConsensus(fused_matrix, consensus_pairs)\n",
    "# print(f\"Bootstrapped Params: {init_params}\")\n",
    "\n",
    "# likelihood_matrix, final_params = fitWeibullBetaMixture(\n",
    "# \tfused_matrix, initial_params=init_params\n",
    "# )\n",
    "# print(\"EM Model Converged.\")\n",
    "\n",
    "\n",
    "# # --- STEP 3: Topology & Results ---\n",
    "# print(\"\\n>>> Phase 3: Final Inference...\")\n",
    "# prior_matrix = computeTransitivityPrior(likelihood_matrix)\n",
    "# posterior_matrix = computeFinalPosterior(likelihood_matrix, prior_matrix)\n",
    "\n",
    "# extractAndPrintClusters(\n",
    "# \tposterior_matrix, artifacts[model_keys[0]][\"semantic_data\"], threshold=0.5\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
