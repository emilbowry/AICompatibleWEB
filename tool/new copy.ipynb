{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20cb54ad",
   "metadata": {},
   "source": [
    "# Background\n",
    "I am writing some model to predict likely \"semantic\" duplicate groups amongs some very grammatically similar strings, about the same topic.\n",
    "We have a collection of different embedding vectors in some data structure `qdata` which is organised in the following form:\n",
    "```\n",
    "{\n",
    "[semantic_data:string]:{[embedding_name:list[float]]}\n",
    "}\n",
    "```\n",
    "\n",
    "We want to quantify where we think we have semantic duplicates, however trivially we do not know any distributions about the embedding space(s). i.e we do not know an expected distance for a duplicate a priori. (also i will want to generalise it to test different distance metrics)\n",
    "\n",
    "We want to quantify the some probability of seeing a duplicate, given a distance. We can refine this model by inferring some distributions about our data.\n",
    "\n",
    "We want to end up with some statistic p(duplicate|distance_metric|neighbours|embedding_model|all_embedding_models).\n",
    "The reason we can make an inference of `embedding_model|all_embedding_models` is because we are using the same fundamental model to produce our embeddings, \"gemini-embedding-001\", however we produce the embeddings from some master string $S$, by doing different preprocessing steps $f_i(S)$ along with specifying different particular task types $g_i(S)$. Which means our different embeddings are really all different perspectives on the same underlying data ($\\text{embedding}_i = \\text{EmbeddingModel}(g_i(f_i(S)))$).\n",
    "\n",
    "# Ground Truth Estimation\n",
    "We first need to quantify some ground truth about our data. We will produce some \"extremely\" likely pairings that are agreed amongst all embeddings. We treat this as some estimate of our number of pairings/duplicate groups [1].\n",
    "\n",
    "# Ground Probability Estimation\n",
    "We then want to produce some estimate about the number of these groups, given our nearest neighbours. Tentitively we will use the average over the number of groupings vs the largest distance that keeps these grous coherent [2]. We will treat this as the expected probability of a duplicate group given distances given all_embedding_models.\n",
    "\n",
    "# Empirical Distribution\n",
    "Next we want to compute some distribution about the expected distances of nearest neighbours for a particular model. Then use some measure of agreement to infer what it says about all_embedding_models given a particular distance.\n",
    "\n",
    "Now we can compare our estimate of where we expect duplicates using our \"Ground Probability Estimation\", vs the \"Empirical Distribution\", and we will just for now say that we will treat all distances that give some probability less than our \"Ground Probability Estimation\" as our duplicate groups.\n",
    "\n",
    "\n",
    "- 1. This may need refining since this is almost certainly correlated to the number of semantic groups but isnt exact. I also don't know if it is an over or under estimate. The group sizing may be smaller, but perhaps we would produce more distinct groups. I have a primitive normalisation of Jaccard Metric = Intersection(pairings)/Union(pairings) to try to rectify this.\n",
    "\n",
    "- 2. No clue how to justify this, ideally we could use the law of large numbers, however we only have a small number of distinct embedding types, however a large number of individual embeddings, at a high dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624ad9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "Q_FILE = \"./data/questions_filter_after.json\"\n",
    "\n",
    "\n",
    "def _loadJson(filepath):\n",
    "\tif not os.path.exists(filepath):\n",
    "\t\tprint(f\"Warning: File not found: {filepath}\")\n",
    "\t\treturn {}\n",
    "\ttry:\n",
    "\t\twith open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "\t\t\treturn json.load(f)\n",
    "\texcept json.JSONDecodeError:\n",
    "\t\tprint(f\"Error decoding JSON: {filepath}\")\n",
    "\t\treturn {}\n",
    "\n",
    "\n",
    "qdata = _loadJson(QUESTIONS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c21e448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Model A (Statement)...\n",
      "Processing Model B (Question)...\n",
      "\n",
      "--- Phase 1: Grid Search for Consensus (Tau) ---\n",
      "Search Range Defined: [5.77 to 25.35] (Step: 0.1)\n",
      "Pre-computing clusters for 196 thresholds...\n",
      "tau a:9.273086354550212, tau b:8.873086354550214\n",
      "\n",
      "Consensus Structure Found:\n",
      "Platinum Pairs Identified: 5\n",
      "N_target: 5.0 (Avg of A=5, B=5)\n",
      "[Model A] Optimal t: 9.3 | Group Error: 0.0 | F1: 1.0000\n",
      "[Model B] Optimal t: 8.9 | Group Error: 0.0 | F1: 1.0000\n",
      "\n",
      "Final Calibrated Thresholds:\n",
      "Model A (Statement): 9.27\n",
      "Model B (Question):  8.87\n",
      "\n",
      "================================================================================\n",
      "FINAL OUTPUT: Model A (Statement Embeddings) (Threshold: 9.27)\n",
      "================================================================================\n",
      "Found 5 total significant groups.\n",
      "P_true pairs captured: 5 (False Positives: 0)\n",
      "\n",
      "GROUP 1 (Size: 2) [Radius: 4.6004]\n",
      " [CENTROID]  Does the privacy policy affirm that the company relies on user consent to process contact information for specific marketing communications?\n",
      "             Does the privacy policy affirm that the company processes contact information for marketing communications based on user consent?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 2 (Size: 2) [Radius: 4.2270]\n",
      " [CENTROID]  Does the privacy policy affirm that users have the right to request the correction of inaccurate personal data?\n",
      "             Does the privacy policy affirm that users have the right to request the correction of inaccurate data?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 3 (Size: 2) [Radius: 4.3943]\n",
      " [CENTROID]  Does the privacy policy affirm that the company relies on Standard Contractual Clauses (SCCs) for transfers to countries without an adequacy decision?\n",
      "             Does the privacy policy affirm that the company relies on Standard Contractual Clauses (SCCs) for transfers to jurisdictions without adequacy decisions?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 4 (Size: 2) [Radius: 4.1676]\n",
      " [CENTROID]  Does the privacy policy affirm that the company collects the dates and times of access?\n",
      "             Does the privacy policy affirm that the company collects the dates and times of access to the services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 5 (Size: 2) [Radius: 4.1913]\n",
      " [CENTROID]  Does the privacy policy affirm that providing personal data constitutes agreement to the transfer of data outside of Canada?\n",
      "             Does the privacy policy affirm that providing personal data constitutes agreement to the disclosure of data outside of Canada?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "FINAL OUTPUT: Model B (Question Embeddings) (Threshold: 8.87)\n",
      "================================================================================\n",
      "Found 5 total significant groups.\n",
      "P_true pairs captured: 5 (False Positives: 0)\n",
      "\n",
      "GROUP 1 (Size: 2) [Radius: 4.3913]\n",
      " [CENTROID]  Does the privacy policy affirm that providing personal data constitutes agreement to the transfer of data outside of Canada?\n",
      "             Does the privacy policy affirm that providing personal data constitutes agreement to the disclosure of data outside of Canada?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 2 (Size: 2) [Radius: 4.3318]\n",
      " [CENTROID]  Does the privacy policy affirm that the company relies on Standard Contractual Clauses (SCCs) for transfers to countries without an adequacy decision?\n",
      "             Does the privacy policy affirm that the company relies on Standard Contractual Clauses (SCCs) for transfers to jurisdictions without adequacy decisions?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 3 (Size: 2) [Radius: 2.8865]\n",
      " [CENTROID]  Does the privacy policy affirm that users have the right to request the correction of inaccurate personal data?\n",
      "             Does the privacy policy affirm that users have the right to request the correction of inaccurate data?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 4 (Size: 2) [Radius: 4.0055]\n",
      " [CENTROID]  Does the privacy policy affirm that the company collects the dates and times of access?\n",
      "             Does the privacy policy affirm that the company collects the dates and times of access to the services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 5 (Size: 2) [Radius: 4.2495]\n",
      " [CENTROID]  Does the privacy policy affirm that the company relies on user consent to process contact information for specific marketing communications?\n",
      "             Does the privacy policy affirm that the company processes contact information for marketing communications based on user consent?\n",
      "--------------------------------------------------------------------------------\n",
      "--- Calculating Empirical Probabilities (EVT Logic) ---\n",
      "Probabilities Calculated.\n",
      "Example (Model A): Dist=8.8 -> P=0.01354\n",
      "Example (Model B): Dist=8.8 -> P=0.01744\n",
      "Max Stable A:105\n",
      "Max Stable A:105\n",
      "Average Stable Stable A:111.0\n",
      "Inferred Probability Threshold: 0.045045\n",
      "\n",
      "================================================================================\n",
      "STAGE 2 OUTPUT: Probabilistic Outlier Model (P < 0.045045)\n",
      "================================================================================\n",
      "Found 14 significant groups.\n",
      "\n",
      "GROUP 1 (Size: 2) [Radius: 4.8566]\n",
      " [CENTROID]  Does the privacy policy affirm that Inputs and Outputs disassociated via Feedback are used for training models?\n",
      "             Does the privacy policy affirm that Inputs and Outputs disassociated via Feedback are used for improving models?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 2 (Size: 2) [Radius: 4.7142]\n",
      "             Does the privacy policy affirm that processing contact information to send technical announcements is based on the necessity to perform a contract?\n",
      " [CENTROID]  Does the privacy policy affirm that the company processes contact information to send technical announcements based on the necessity to perform a contract?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 3 (Size: 2) [Radius: 4.7381]\n",
      " [CENTROID]  Does the privacy policy affirm that the company collects the user's time zone?\n",
      "             Does the privacy policy affirm that the company collects time zone settings?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 4 (Size: 2) [Radius: 4.6968]\n",
      " [CENTROID]  Does the privacy policy affirm that Personal Data is retained to comply with legal obligations?\n",
      "             Does the privacy policy affirm that Personal Data is used to comply with legal obligations?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 5 (Size: 2) [Radius: 4.5656]\n",
      " [CENTROID]  Does the privacy policy affirm that Personal Data is used to protect the rights, privacy, safety, or property of users?\n",
      "             Does the privacy policy affirm that Personal Data is used to protect the rights, privacy, safety, or property of the company?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 6 (Size: 2) [Radius: 5.2123]\n",
      " [CENTROID]  Does the privacy policy affirm that Personal Data is used to improve the company's services?\n",
      "             Does the privacy policy affirm that Personal Data is used to develop the company's services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 7 (Size: 2) [Radius: 4.8486]\n",
      " [CENTROID]  Does the privacy policy affirm that users have the statutory right to access their Personal Data?\n",
      "             Does the privacy policy affirm that users have the statutory right to access information relating to how their Personal Data is processed?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 8 (Size: 2) [Radius: 4.3913]\n",
      " [CENTROID]  Does the privacy policy affirm that providing personal data constitutes agreement to the transfer of data outside of Canada?\n",
      "             Does the privacy policy affirm that providing personal data constitutes agreement to the disclosure of data outside of Canada?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 9 (Size: 2) [Radius: 4.6063]\n",
      " [CENTROID]  Does the privacy policy affirm that the company may disclose personal data to governmental regulatory authorities as required by law?\n",
      "             Does the privacy policy affirm that the company may disclose personal data in response to requests from governmental regulatory authorities?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 10 (Size: 2) [Radius: 4.3318]\n",
      " [CENTROID]  Does the privacy policy affirm that the company relies on Standard Contractual Clauses (SCCs) for transfers to countries without an adequacy decision?\n",
      "             Does the privacy policy affirm that the company relies on Standard Contractual Clauses (SCCs) for transfers to jurisdictions without adequacy decisions?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 11 (Size: 2) [Radius: 5.1673]\n",
      " [CENTROID]  Does the privacy policy affirm that personal data may be shared with service providers for data processing purposes?\n",
      "             Does the privacy policy affirm that personal data may be shared with service providers for the purpose of providing services to the user?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 12 (Size: 2) [Radius: 4.2495]\n",
      " [CENTROID]  Does the privacy policy affirm that the company relies on user consent to process contact information for specific marketing communications?\n",
      "             Does the privacy policy affirm that the company processes contact information for marketing communications based on user consent?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 13 (Size: 2) [Radius: 2.8865]\n",
      " [CENTROID]  Does the privacy policy affirm that users have the right to request the correction of inaccurate personal data?\n",
      "             Does the privacy policy affirm that users have the right to request the correction of inaccurate data?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 14 (Size: 2) [Radius: 4.0055]\n",
      " [CENTROID]  Does the privacy policy affirm that the company collects the dates and times of access?\n",
      "             Does the privacy policy affirm that the company collects the dates and times of access to the services?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from scipy.spatial.distance import cdist, mahalanobis\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def prepare_model_artifacts(raw_vector_list, name=\"Model\"):\n",
    "\t\"\"\"\n",
    "\tReturns dictionary containing:\n",
    "\t- 'dist_matrix': N x N pairwise distances\n",
    "\t- 'vectors': N x 256 normalized vectors\n",
    "\t- 'precision': 256 x 256 inverse covariance matrix\n",
    "\t\"\"\"\n",
    "\tprint(f\"Processing {name}...\")\n",
    "\n",
    "\tdata = np.array(raw_vector_list)\n",
    "\tdata_trunc = data[:, :256]\n",
    "\tnorms = np.linalg.norm(data_trunc, axis=1, keepdims=True)\n",
    "\tcleaned_vectors = data_trunc / (norms + 1e-10)\n",
    "\n",
    "\tlw = LedoitWolf()\n",
    "\tlw.fit(cleaned_vectors)\n",
    "\tprecision_matrix = lw.precision_\n",
    "\n",
    "\tdist_matrix = cdist(\n",
    "\t\tcleaned_vectors, cleaned_vectors, metric=\"mahalanobis\", VI=precision_matrix\n",
    "\t)\n",
    "\n",
    "\treturn {\n",
    "\t\t\"dist_matrix\": dist_matrix,\n",
    "\t\t\"vectors\": cleaned_vectors,\n",
    "\t\t\"precision\": precision_matrix,\n",
    "\t}\n",
    "\n",
    "\n",
    "data_set = qdata\n",
    "strings = list(data_set.keys())\n",
    "\n",
    "raw_vectors_A = [data_set[s][\"embedding_vector\"] for s in strings]\n",
    "raw_vectors_B = [data_set[s][\"retrieval_embedding_vector\"] for s in strings]\n",
    "\n",
    "\n",
    "Model_A = prepare_model_artifacts(raw_vectors_A, \"Model A (Statement)\")\n",
    "Model_B = prepare_model_artifacts(raw_vectors_B, \"Model B (Question)\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from scipy.spatial.distance import cdist, mahalanobis\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# def get_pairs_from_labels(labels):\n",
    "# \t\"\"\"Converts cluster labels into a Set of unique pairs (indices).\"\"\"\n",
    "# \tdf = pd.DataFrame({\"label\": labels, \"id\": range(len(labels))})\n",
    "# \tpairs = set()\n",
    "# \tfor label, group in df.groupby(\"label\"):\n",
    "# \t\tindices = group[\"id\"].tolist()\n",
    "# \t\tif len(indices) > 1:\n",
    "# \t\t\tfor p in combinations(sorted(indices), 2):\n",
    "# \t\t\t\tpairs.add(p)\n",
    "# \treturn pairs\n",
    "\n",
    "\n",
    "# def get_clustering_pairs_and_labels(dist_matrix, threshold):\n",
    "# \t\"\"\"Runs clustering and returns both the pair set and the labels array.\"\"\"\n",
    "# \tmodel = AgglomerativeClustering(\n",
    "# \t\tn_clusters=None,\n",
    "# \t\tdistance_threshold=threshold,\n",
    "# \t\tmetric=\"precomputed\",\n",
    "# \t\tlinkage=\"complete\",\n",
    "# \t)\n",
    "# \tlabels = model.fit_predict(dist_matrix)\n",
    "# \treturn get_pairs_from_labels(labels), labels\n",
    "\n",
    "\n",
    "def calculate_n_true(labels_array, target_pairs):\n",
    "\t\"\"\"Calculates the number of unique clusters containing the elements of target_pairs.\"\"\"\n",
    "\tif not target_pairs or labels_array is None:\n",
    "\t\treturn 0\n",
    "\n",
    "\tinvolved_indices = set(idx for pair in target_pairs for idx in pair)\n",
    "\n",
    "\tlabels_of_interest = set(\n",
    "\t\tlabels_array[idx] for idx in involved_indices if idx < len(labels_array)\n",
    "\t)\n",
    "\n",
    "\tif not involved_indices:\n",
    "\t\treturn 0\n",
    "\n",
    "\treturn len(labels_of_interest)\n",
    "\n",
    "\n",
    "print(\"\\n--- Phase 1: Grid Search for Consensus (Tau) ---\")\n",
    "\n",
    "\n",
    "dist_A_off = Model_A[\"dist_matrix\"][np.triu_indices_from(Model_A[\"dist_matrix\"], k=1)]\n",
    "dist_B_off = Model_B[\"dist_matrix\"][np.triu_indices_from(Model_B[\"dist_matrix\"], k=1)]\n",
    "all_dists = np.concatenate([dist_A_off, dist_B_off])\n",
    "\n",
    "\n",
    "D_min_min = np.min(all_dists)\n",
    "D_max_max = np.max(all_dists)\n",
    "\n",
    "\n",
    "SEARCH_STEP = 0.1\t# converges to same groups here 0.1 was too high\n",
    "\n",
    "\n",
    "TAU_SEARCH_START = max(D_min_min, 0.0)\n",
    "TAU_SEARCH_END = D_max_max\n",
    "\n",
    "tau_range = np.arange(TAU_SEARCH_START, TAU_SEARCH_END, SEARCH_STEP)\n",
    "t_range = np.arange(TAU_SEARCH_START, TAU_SEARCH_END, SEARCH_STEP)\n",
    "\n",
    "print(\n",
    "\tf\"Search Range Defined: [{TAU_SEARCH_START:.2f} to {TAU_SEARCH_END:.2f}] (Step: {SEARCH_STEP})\"\n",
    ")\n",
    "best_jaccard = -1\n",
    "P_true = set()\n",
    "labels_A_star = None\n",
    "labels_B_star = None\n",
    "\n",
    "\n",
    "cache_A = {}\n",
    "cache_B = {}\n",
    "\n",
    "print(f\"Pre-computing clusters for {len(tau_range)} thresholds...\")\n",
    "for t in tau_range:\n",
    "\tcache_A[t] = get_clustering_pairs_and_labels(Model_A[\"dist_matrix\"], t)\n",
    "\tcache_B[t] = get_clustering_pairs_and_labels(Model_B[\"dist_matrix\"], t)\n",
    "TAU_A = 0\n",
    "TAU_B = 0\n",
    "\n",
    "\n",
    "def calculate_n_true(labels_array, target_pairs):\n",
    "\t\"\"\"Calculates the number of unique clusters containing the elements of target_pairs.\"\"\"\n",
    "\tif not target_pairs or labels_array is None:\n",
    "\t\treturn 0\n",
    "\n",
    "\tinvolved_indices = set(idx for pair in target_pairs for idx in pair)\n",
    "\n",
    "\tlabels_of_interest = set(\n",
    "\t\tlabels_array[idx] for idx in involved_indices if idx < len(labels_array)\n",
    "\t)\n",
    "\n",
    "\tif not involved_indices:\n",
    "\t\treturn 0\n",
    "\n",
    "\treturn len(labels_of_interest)\n",
    "\n",
    "\n",
    "best_metric_score = -1\n",
    "\n",
    "for t_A in tau_range:\n",
    "\tpairs_A, labels_A = cache_A[t_A]\n",
    "\tfor t_B in tau_range:\n",
    "\t\tpairs_B, labels_B = cache_B[t_B]\n",
    "\n",
    "\t\tintersection = pairs_A.intersection(pairs_B)\n",
    "\n",
    "\t\tif len(intersection) > 0:\n",
    "\n",
    "\t\t\tN_A = calculate_n_true(labels_A, intersection)\n",
    "\n",
    "\t\t\tN_B = calculate_n_true(labels_B, intersection)\n",
    "\n",
    "\t\t\tavg_consensus_groups = (N_A + N_B) / 2\n",
    "\n",
    "\t\t\tif avg_consensus_groups > best_metric_score:\n",
    "\t\t\t\tbest_metric_score = avg_consensus_groups\n",
    "\n",
    "\t\t\t\t# Update Best State\n",
    "\t\t\t\tP_true = intersection\n",
    "\t\t\t\tlabels_A_star = labels_A\n",
    "\t\t\t\tlabels_B_star = labels_B\n",
    "\t\t\t\tTAU_A = t_A\n",
    "\t\t\t\tTAU_B = t_B\n",
    "for t_A in tau_range:\n",
    "\tpairs_A, labels_A = cache_A[t_A]\n",
    "\tfor t_B in tau_range:\n",
    "\t\tpairs_B, labels_B = cache_B[t_B]\n",
    "\n",
    "\t\tintersection = pairs_A.intersection(pairs_B)\n",
    "\t\tunion = pairs_A.union(pairs_B)\n",
    "\t\tif len(union) > 0:\n",
    "\t\t\tjaccard = len(intersection) / len(union)\n",
    "\n",
    "\t\t\tif jaccard > best_jaccard:\n",
    "\n",
    "\t\t\t\tbest_jaccard = jaccard\n",
    "\t\t\t\tP_true = intersection\n",
    "\t\t\t\tlabels_A_star = labels_A\n",
    "\t\t\t\tlabels_B_star = labels_B\n",
    "\t\t\t\tTAU_A = t_A\n",
    "\t\t\t\tTAU_B = t_B\n",
    "\n",
    "print(f\"tau a:{TAU_A}, tau b:{TAU_B}\")\n",
    "\n",
    "\n",
    "N_A_true = calculate_n_true(labels_A_star, P_true)\n",
    "N_B_true = calculate_n_true(labels_B_star, P_true)\n",
    "N_target = (N_A_true + N_B_true) / 2\n",
    "\n",
    "\n",
    "def getMaxStable(start_tau, model_data):\n",
    "\t\"\"\"\n",
    "\tIncreases threshold until baseline clusters are no longer a subset of current clusters.\n",
    "\tReturns the number of significant clusters at the maximum stable threshold.\n",
    "\t\"\"\"\n",
    "\tdist_matrix = model_data[\"dist_matrix\"]\n",
    "\n",
    "\tbase_model = AgglomerativeClustering(\n",
    "\t\tn_clusters=None,\n",
    "\t\tdistance_threshold=start_tau,\n",
    "\t\tmetric=\"precomputed\",\n",
    "\t\tlinkage=\"complete\",\n",
    "\t)\n",
    "\tbase_labels = base_model.fit_predict(dist_matrix)\n",
    "\n",
    "\tdef get_cluster_set_and_count(labels, n_items):\n",
    "\t\tdf = pd.DataFrame({\"label\": labels, \"idx\": range(n_items)})\n",
    "\t\tgroups = [\n",
    "\t\t\tg[\"idx\"].sort_values().tolist() for _, g in df.groupby(\"label\") if len(g) > 1\n",
    "\t\t]\n",
    "\n",
    "\t\treturn set(tuple(g) for g in groups), len(groups)\n",
    "\n",
    "\tbaseline_indices, baseline_count = get_cluster_set_and_count(\n",
    "\t\tbase_labels, len(dist_matrix)\n",
    "\t)\n",
    "\n",
    "\tif not baseline_indices:\n",
    "\t\treturn 0\n",
    "\n",
    "\tSTEP_SIZE = SEARCH_STEP\n",
    "\tMAX_ITERATIONS = 500\n",
    "\tt = start_tau\n",
    "\tlast_stable_count = baseline_count\n",
    "\n",
    "\tfor _ in range(MAX_ITERATIONS):\n",
    "\t\tt += STEP_SIZE\n",
    "\n",
    "\t\tmodel_t = AgglomerativeClustering(\n",
    "\t\t\tn_clusters=None,\n",
    "\t\t\tdistance_threshold=t,\n",
    "\t\t\tmetric=\"precomputed\",\n",
    "\t\t\tlinkage=\"complete\",\n",
    "\t\t)\n",
    "\t\tlabels_t = model_t.fit_predict(dist_matrix)\n",
    "\n",
    "\t\tcurrent_indices, current_count = get_cluster_set_and_count(labels_t, len(dist_matrix))\n",
    "\n",
    "\t\tif not baseline_indices.issubset(current_indices):\n",
    "\t\t\treturn last_stable_count\n",
    "\n",
    "\t\tlast_stable_count = current_count\n",
    "\n",
    "\treturn last_stable_count\n",
    "\n",
    "\n",
    "print(f\"\\nConsensus Structure Found:\")\n",
    "print(f\"Platinum Pairs Identified: {len(P_true)}\")\n",
    "print(f\"N_target: {N_target:.1f} (Avg of A={N_A_true}, B={N_B_true})\")\n",
    "\n",
    "\n",
    "# This bit is an artifact and the found t's are actually necesarily the corresponding tau's im very sure\n",
    "def tune_threshold_by_group_count(model_data, N_target, model_name):\n",
    "\tbest_t = 0\n",
    "\tmin_error = float(\"inf\")\n",
    "\tbest_f1 = -1\n",
    "\n",
    "\tfor t in t_range:\n",
    "\t\tcurrent_pairs, current_labels = get_clustering_pairs_and_labels(\n",
    "\t\t\tmodel_data[\"dist_matrix\"], t\n",
    "\t\t)\n",
    "\n",
    "\t\tN_predicted = calculate_n_true(current_labels, P_true)\n",
    "\t\tgroup_error = abs(N_predicted - N_target)\n",
    "\n",
    "\t\ttp = len(current_pairs.intersection(P_true))\n",
    "\t\tfp = len(current_pairs - P_true)\n",
    "\t\tfn = len(P_true - current_pairs)\n",
    "\n",
    "\t\tif tp > 0:\n",
    "\t\t\tprecision = tp / (tp + fp)\n",
    "\t\t\trecall = tp / (tp + fn)\n",
    "\t\t\tf1 = 2 * (precision * recall) / (precision + recall)\n",
    "\t\telse:\n",
    "\t\t\tf1 = 0\n",
    "\n",
    "\t\tif group_error < min_error:\n",
    "\t\t\tmin_error = group_error\n",
    "\t\t\tbest_f1 = f1\n",
    "\t\t\tbest_t = t\n",
    "\t\telif group_error == min_error and f1 > best_f1:\n",
    "\t\t\tbest_f1 = f1\n",
    "\t\t\tbest_t = t\n",
    "\n",
    "\tprint(\n",
    "\t\tf\"[{model_name}] Optimal t: {best_t:.1f} | Group Error: {min_error:.1f} | F1: {best_f1:.4f}\"\n",
    "\t)\n",
    "\treturn best_t\n",
    "\n",
    "\n",
    "optimal_t_A = tune_threshold_by_group_count(Model_A, N_target, \"Model A\")\n",
    "optimal_t_B = tune_threshold_by_group_count(Model_B, N_target, \"Model B\")\n",
    "\n",
    "print(f\"\\nFinal Calibrated Thresholds:\")\n",
    "print(f\"Model A (Statement): {optimal_t_A:.2f}\")\n",
    "print(f\"Model B (Question):  {optimal_t_B:.2f}\")\n",
    "\n",
    "\n",
    "def print_final_clusters(model_data, threshold, string_list, title):\n",
    "\tprint(f\"\\n{'='*80}\")\n",
    "\tprint(f\"FINAL OUTPUT: {title} (Threshold: {threshold:.2f})\")\n",
    "\tprint(f\"{'='*80}\")\n",
    "\n",
    "\tcluster_model = AgglomerativeClustering(\n",
    "\t\tn_clusters=None,\n",
    "\t\tdistance_threshold=threshold,\n",
    "\t\tmetric=\"precomputed\",\n",
    "\t\tlinkage=\"complete\",\n",
    "\t)\n",
    "\tlabels = cluster_model.fit_predict(model_data[\"dist_matrix\"])\n",
    "\n",
    "\tdf = pd.DataFrame(\n",
    "\t\t{\"string\": string_list, \"label\": labels, \"idx\": range(len(string_list))}\n",
    "\t)\n",
    "\tgroups = [g for _, g in df.groupby(\"label\") if len(g) > 1]\n",
    "\tgroups.sort(key=lambda x: len(x), reverse=True)\n",
    "\n",
    "\tfinal_pairs, _ = get_clustering_pairs_and_labels(model_data[\"dist_matrix\"], threshold)\n",
    "\ttp = len(final_pairs.intersection(P_true))\n",
    "\tfp = len(final_pairs - P_true)\n",
    "\n",
    "\tprint(f\"Found {len(groups)} total significant groups.\")\n",
    "\tprint(f\"P_true pairs captured: {tp} (False Positives: {fp})\\n\")\n",
    "\n",
    "\tfor i, group in enumerate(groups):\n",
    "\t\tindices = group[\"idx\"].tolist()\n",
    "\t\tcluster_strs = group[\"string\"].tolist()\n",
    "\n",
    "\t\tvecs = model_data[\"vectors\"][indices]\n",
    "\t\tprec = model_data[\"precision\"]\n",
    "\t\tlocal_mean = np.mean(vecs, axis=0)\n",
    "\n",
    "\t\tdistances = []\n",
    "\t\tmin_dist = float(\"inf\")\n",
    "\t\trep_idx = -1\n",
    "\n",
    "\t\tfor local_i, global_i in enumerate(indices):\n",
    "\t\t\td = mahalanobis(model_data[\"vectors\"][global_i], local_mean, prec)\n",
    "\t\t\tdistances.append(d)\n",
    "\t\t\tif d < min_dist:\n",
    "\t\t\t\tmin_dist = d\n",
    "\t\t\t\trep_idx = local_i\n",
    "\n",
    "\t\tgroup_radius = max(distances)\n",
    "\n",
    "\t\tprint(f\"GROUP {i+1} (Size: {len(indices)}) [Radius: {group_radius:.4f}]\")\n",
    "\n",
    "\t\tfor idx, s in enumerate(cluster_strs):\n",
    "\t\t\tprefix = \" [CENTROID] \" if idx == rep_idx else \"            \"\n",
    "\t\t\tprint(f\"{prefix} {s}\")\n",
    "\t\tprint(\"-\" * 80)\n",
    "\n",
    "\n",
    "print_final_clusters(Model_A, optimal_t_A, strings, \"Model A (Statement Embeddings)\")\n",
    "print_final_clusters(Model_B, optimal_t_B, strings, \"Model B (Question Embeddings)\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "def get_nearest_neighbor_distances(dist_matrix):\n",
    "\t\"\"\"\n",
    "\tExtracts the distance to the nearest neighbor for every point.\n",
    "\tIgnores the diagonal (0).\n",
    "\t\"\"\"\n",
    "\tnp.fill_diagonal(dist_matrix, float(\"inf\"))\n",
    "\tmin_dists = np.min(dist_matrix, axis=1)\n",
    "\tnp.fill_diagonal(dist_matrix, 0.0)\n",
    "\treturn min_dists\n",
    "\n",
    "\n",
    "def convert_dist_to_prob(dist_matrix, reference_dist_array):\n",
    "\t\"\"\"\n",
    "\tConverts raw distances into Probabilities (P-values) based on the\n",
    "\tEmpirical CDF of the provided reference distribution (Nearest Neighbors).\n",
    "\n",
    "\tP(d) = (Rank of d) / (Total Count + 1)\n",
    "\t\"\"\"\n",
    "\n",
    "\tsorted_refs = np.sort(reference_dist_array)\n",
    "\tn = len(sorted_refs)\n",
    "\n",
    "\tranks = np.searchsorted(sorted_refs, dist_matrix)\n",
    "\n",
    "\tprobs = (ranks + 1) / (n + 1)\n",
    "\n",
    "\treturn probs\n",
    "\n",
    "\n",
    "print(\"--- Calculating Empirical Probabilities (EVT Logic) ---\")\n",
    "\n",
    "\n",
    "nn_dists_A = get_nearest_neighbor_distances(Model_A[\"dist_matrix\"])\n",
    "nn_dists_B = get_nearest_neighbor_distances(Model_B[\"dist_matrix\"])\n",
    "\n",
    "\n",
    "Prob_A = convert_dist_to_prob(Model_A[\"dist_matrix\"], nn_dists_A)\n",
    "Prob_B = convert_dist_to_prob(Model_B[\"dist_matrix\"], nn_dists_B)\n",
    "\n",
    "print(f\"Probabilities Calculated.\")\n",
    "print(\n",
    "\tf\"Example (Model A): Dist=8.8 -> P={np.interp(8.8, np.sort(nn_dists_A), np.linspace(0,1,len(nn_dists_A))):.5f}\"\n",
    ")\n",
    "print(\n",
    "\tf\"Example (Model B): Dist=8.8 -> P={np.interp(8.8, np.sort(nn_dists_B), np.linspace(0,1,len(nn_dists_B))):.5f}\"\n",
    ")\n",
    "\n",
    "\n",
    "Prob_Fused = np.minimum(Prob_A, Prob_B)\n",
    "max_a = getMaxStable(TAU_A, Model_A)\n",
    "max_b = getMaxStable(TAU_B, Model_B)\n",
    "\n",
    "\n",
    "AV_STABLE = (max_a + max_b) / 2\n",
    "print(f\"Max Stable A:{max_a}\")\n",
    "print(f\"Max Stable B:{max_b}\")\n",
    "print(f\"Average Stable Stable A:{AV_STABLE}\")\n",
    "\n",
    "\n",
    "PROB_THRESHOLD = len(P_true) / AV_STABLE\n",
    "# PROB_THRESHOLD = 0.04\n",
    "print(f\"Inferred Probability Threshold: {PROB_THRESHOLD:.6f}\")\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STAGE 2 OUTPUT: Probabilistic Outlier Model (P < {PROB_THRESHOLD:.6f})\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "cluster_model = AgglomerativeClustering(\n",
    "\tn_clusters=None,\n",
    "\tdistance_threshold=PROB_THRESHOLD,\n",
    "\tmetric=\"precomputed\",\n",
    "\tlinkage=\"complete\",\n",
    ")\n",
    "\n",
    "labels = cluster_model.fit_predict(Prob_Fused)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"string\": strings, \"label\": labels, \"idx\": range(len(strings))})\n",
    "groups = [g for _, g in df.groupby(\"label\") if len(g) > 1]\n",
    "groups.sort(key=lambda x: len(x), reverse=True)\n",
    "\n",
    "print(f\"Found {len(groups)} significant groups.\\n\")\n",
    "\n",
    "for i, group in enumerate(groups):\n",
    "\tindices = group[\"idx\"].tolist()\n",
    "\tcluster_strs = group[\"string\"].tolist()\n",
    "\n",
    "\tref_vectors = Model_B[\"vectors\"][indices]\n",
    "\tref_prec = Model_B[\"precision\"]\n",
    "\tlocal_mean = np.mean(ref_vectors, axis=0)\n",
    "\n",
    "\tdists = []\n",
    "\tmin_d = float(\"inf\")\n",
    "\trep_i = -1\n",
    "\n",
    "\tfor loc_i, glob_i in enumerate(indices):\n",
    "\t\td = mahalanobis(Model_B[\"vectors\"][glob_i], local_mean, ref_prec)\n",
    "\t\tdists.append(d)\n",
    "\t\tif d < min_d:\n",
    "\t\t\tmin_d = d\n",
    "\t\t\trep_i = loc_i\n",
    "\n",
    "\tprint(f\"GROUP {i+1} (Size: {len(indices)}) [Radius: {max(dists):.4f}]\")\n",
    "\n",
    "\tfor idx, s in enumerate(cluster_strs):\n",
    "\t\tprefix = \" [CENTROID] \" if idx == rep_i else \"            \"\n",
    "\t\tprint(f\"{prefix} {s}\")\n",
    "\tprint(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a080fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Stable B:117\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max Stable B:{max_b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c780013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from scipy.spatial.distance import cdist, mahalanobis\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from itertools import combinations\n",
    "\n",
    "# ==========================================\n",
    "# PRE-PROCESSING & MODEL ARTIFACTS\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "def prepare_model_artifacts(raw_vector_list, name=\"Model\"):\n",
    "\tprint(f\"Processing {name}...\")\n",
    "\tdata = np.array(raw_vector_list)\n",
    "\tdata_trunc = data[:, :256]\n",
    "\tnorms = np.linalg.norm(data_trunc, axis=1, keepdims=True)\n",
    "\tcleaned_vectors = data_trunc / (norms + 1e-10)\n",
    "\n",
    "\tlw = LedoitWolf()\n",
    "\tlw.fit(cleaned_vectors)\n",
    "\tprecision_matrix = lw.precision_\n",
    "\n",
    "\tdist_matrix = cdist(\n",
    "\t\tcleaned_vectors, cleaned_vectors, metric=\"mahalanobis\", VI=precision_matrix\n",
    "\t)\n",
    "\n",
    "\treturn {\n",
    "\t\t\"dist_matrix\": dist_matrix,\n",
    "\t\t\"vectors\": cleaned_vectors,\n",
    "\t\t\"precision\": precision_matrix,\n",
    "\t}\n",
    "\n",
    "\n",
    "# --- DATA LOADING (Assumes 'qdata' exists in your scope) ---\n",
    "data_set = qdata\n",
    "strings = list(data_set.keys())\n",
    "\n",
    "raw_vectors_A = [data_set[s][\"embedding_vector\"] for s in strings]\n",
    "raw_vectors_B = [data_set[s][\"retrieval_embedding_vector\"] for s in strings]\n",
    "\n",
    "Model_A = prepare_model_artifacts(raw_vectors_A, \"Model A (Statement)\")\n",
    "Model_B = prepare_model_artifacts(raw_vectors_B, \"Model B (Question)\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "def get_pairs_from_labels(labels):\n",
    "\t\"\"\"Converts cluster labels into a Set of unique pairs (indices).\"\"\"\n",
    "\tdf = pd.DataFrame({\"label\": labels, \"id\": range(len(labels))})\n",
    "\tpairs = set()\n",
    "\tfor label, group in df.groupby(\"label\"):\n",
    "\t\tindices = group[\"id\"].tolist()\n",
    "\t\tif len(indices) > 1:\n",
    "\t\t\tfor p in combinations(sorted(indices), 2):\n",
    "\t\t\t\tpairs.add(p)\n",
    "\treturn pairs\n",
    "\n",
    "\n",
    "def get_clustering_pairs_and_labels(dist_matrix, threshold):\n",
    "\t\"\"\"Runs clustering and returns both the pair set and the labels array.\"\"\"\n",
    "\tmodel = AgglomerativeClustering(\n",
    "\t\tn_clusters=None,\n",
    "\t\tdistance_threshold=threshold,\n",
    "\t\tmetric=\"precomputed\",\n",
    "\t\tlinkage=\"complete\",\n",
    "\t)\n",
    "\tlabels = model.fit_predict(dist_matrix)\n",
    "\treturn get_pairs_from_labels(labels), labels\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# PHASE 1: CONSENSUS GENERATION (EXCESS AGREEMENT)\n",
    "# ==========================================\n",
    "print(\"\\n--- Phase 1: Grid Search for Consensus (Excess Agreement) ---\")\n",
    "\n",
    "dist_A_off = Model_A[\"dist_matrix\"][np.triu_indices_from(Model_A[\"dist_matrix\"], k=1)]\n",
    "dist_B_off = Model_B[\"dist_matrix\"][np.triu_indices_from(Model_B[\"dist_matrix\"], k=1)]\n",
    "all_dists = np.concatenate([dist_A_off, dist_B_off])\n",
    "\n",
    "D_min = np.min(all_dists)\n",
    "D_max = np.max(all_dists)\n",
    "SEARCH_STEP = 0.05\n",
    "\n",
    "TAU_SEARCH_START = max(D_min, 0.0)\n",
    "TAU_SEARCH_END = D_max\n",
    "\n",
    "tau_range = np.arange(TAU_SEARCH_START, TAU_SEARCH_END, SEARCH_STEP)\n",
    "print(f\"Search Range: [{TAU_SEARCH_START:.2f} to {TAU_SEARCH_END:.2f}]\")\n",
    "\n",
    "# Pre-compute Clusters\n",
    "cache_A = {}\n",
    "cache_B = {}\n",
    "print(f\"Pre-computing clusters for {len(tau_range)} thresholds...\")\n",
    "for t in tau_range:\n",
    "\tcache_A[t] = get_clustering_pairs_and_labels(Model_A[\"dist_matrix\"], t)\n",
    "\tcache_B[t] = get_clustering_pairs_and_labels(Model_B[\"dist_matrix\"], t)\n",
    "\n",
    "# Calculate Total Possible Pairs for Expectation\n",
    "N_samples = Model_A[\"dist_matrix\"].shape[0]\n",
    "TOTAL_POSSIBLE_PAIRS = (N_samples * (N_samples - 1)) / 2\n",
    "\n",
    "best_score = -float(\"inf\")\n",
    "P_true = set()\n",
    "labels_A_star = None\n",
    "labels_B_star = None\n",
    "TAU_A = 0\n",
    "TAU_B = 0\n",
    "\n",
    "for t_A in tau_range:\n",
    "\tpairs_A, labels_A = cache_A[t_A]\n",
    "\tn_A = len(pairs_A)\n",
    "\n",
    "\tfor t_B in tau_range:\n",
    "\t\tpairs_B, labels_B = cache_B[t_B]\n",
    "\t\tn_B = len(pairs_B)\n",
    "\n",
    "\t\tintersection = pairs_A.intersection(pairs_B)\n",
    "\t\tn_obs = len(intersection)\n",
    "\n",
    "\t\t# Expected Agreement (Random Chance Baseline)\n",
    "\t\tif TOTAL_POSSIBLE_PAIRS > 0:\n",
    "\t\t\tn_exp = (n_A * n_B) / TOTAL_POSSIBLE_PAIRS\n",
    "\t\telse:\n",
    "\t\t\tn_exp = 0\n",
    "\n",
    "\t\t# METRIC: Excess Agreement\n",
    "\t\tscore = n_obs - n_exp\n",
    "\n",
    "\t\tif score > best_score:\n",
    "\t\t\tbest_score = score\n",
    "\t\t\tP_true = intersection\n",
    "\t\t\tlabels_A_star = labels_A\n",
    "\t\t\tlabels_B_star = labels_B\n",
    "\t\t\tTAU_A = t_A\n",
    "\t\t\tTAU_B = t_B\n",
    "\n",
    "print(f\"\\nOptimization Complete.\")\n",
    "print(f\"Best Excess Agreement Score: {best_score:.4f}\")\n",
    "print(f\"Optimal Taus -> Model A: {TAU_A:.2f}, Model B: {TAU_B:.2f}\")\n",
    "\n",
    "# Count TOTAL clusters (including singletons) for Phase 3\n",
    "# len(set(labels)) gives the total number of unique groups found (including noise/singletons)\n",
    "count_clusters_A = len(set(labels_A_star)) if labels_A_star is not None else 0\n",
    "count_clusters_B = len(set(labels_B_star)) if labels_B_star is not None else 0\n",
    "N_total_clusters_C_prime = (count_clusters_A + count_clusters_B) / 2\n",
    "\n",
    "print(f\"Platinum Pairs Identified: {len(P_true)}\")\n",
    "print(f\"Total Cluster Universe (C_prime): {N_total_clusters_C_prime:.1f}\")\n",
    "\n",
    "print(\"A:\")\n",
    "print(labels_A_star)\n",
    "print(labels_B_star)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# PHASE 2: CALIBRATION OUTPUT\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "def print_final_clusters(model_data, threshold, string_list, title):\n",
    "\tprint(f\"\\n{'='*80}\")\n",
    "\tprint(f\"FINAL OUTPUT: {title} (Threshold: {threshold:.2f})\")\n",
    "\tprint(f\"{'='*80}\")\n",
    "\n",
    "\tcluster_model = AgglomerativeClustering(\n",
    "\t\tn_clusters=None,\n",
    "\t\tdistance_threshold=threshold,\n",
    "\t\tmetric=\"precomputed\",\n",
    "\t\tlinkage=\"complete\",\n",
    "\t)\n",
    "\tlabels = cluster_model.fit_predict(model_data[\"dist_matrix\"])\n",
    "\n",
    "\tdf = pd.DataFrame(\n",
    "\t\t{\"string\": string_list, \"label\": labels, \"idx\": range(len(string_list))}\n",
    "\t)\n",
    "\tgroups = [g for _, g in df.groupby(\"label\") if len(g) > 1]\n",
    "\tgroups.sort(key=lambda x: len(x), reverse=True)\n",
    "\n",
    "\tfinal_pairs, _ = get_clustering_pairs_and_labels(model_data[\"dist_matrix\"], threshold)\n",
    "\ttp = len(final_pairs.intersection(P_true))\n",
    "\tfp = len(final_pairs - P_true)\n",
    "\n",
    "\tprint(f\"Found {len(groups)} total significant groups.\")\n",
    "\tprint(f\"P_true pairs captured: {tp} (False Positives: {fp})\\n\")\n",
    "\n",
    "\tfor i, group in enumerate(groups):\n",
    "\t\tindices = group[\"idx\"].tolist()\n",
    "\t\tcluster_strs = group[\"string\"].tolist()\n",
    "\n",
    "\t\tvecs = model_data[\"vectors\"][indices]\n",
    "\t\tprec = model_data[\"precision\"]\n",
    "\t\tlocal_mean = np.mean(vecs, axis=0)\n",
    "\n",
    "\t\tdistances = []\n",
    "\t\tmin_dist = float(\"inf\")\n",
    "\t\trep_idx = -1\n",
    "\n",
    "\t\tfor local_i, global_i in enumerate(indices):\n",
    "\t\t\td = mahalanobis(model_data[\"vectors\"][global_i], local_mean, prec)\n",
    "\t\t\tdistances.append(d)\n",
    "\t\t\tif d < min_dist:\n",
    "\t\t\t\tmin_dist = d\n",
    "\t\t\t\trep_idx = local_i\n",
    "\n",
    "\t\tgroup_radius = max(distances)\n",
    "\n",
    "\t\tprint(f\"GROUP {i+1} (Size: {len(indices)}) [Radius: {group_radius:.4f}]\")\n",
    "\n",
    "\t\tfor idx, s in enumerate(cluster_strs):\n",
    "\t\t\tprefix = \" [CENTROID] \" if idx == rep_idx else \"            \"\n",
    "\t\t\tprint(f\"{prefix} {s}\")\n",
    "\t\tprint(\"-\" * 80)\n",
    "\n",
    "\n",
    "print_final_clusters(Model_A, TAU_A, strings, \"Model A (Statement Embeddings)\")\n",
    "print_final_clusters(Model_B, TAU_B, strings, \"Model B (Question Embeddings)\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# PHASE 3: PROBABILISTIC MODEL (INFERRING MISSES)\n",
    "# ==========================================\n",
    "print(\"\\n--- Calculating Empirical Probabilities (EVT Logic) ---\")\n",
    "\n",
    "\n",
    "def get_nearest_neighbor_distances(dist_matrix):\n",
    "\td_mat = dist_matrix.copy()\n",
    "\tnp.fill_diagonal(d_mat, float(\"inf\"))\n",
    "\tmin_dists = np.min(d_mat, axis=1)\n",
    "\treturn min_dists\n",
    "\n",
    "\n",
    "def convert_dist_to_prob(dist_matrix, reference_dist_array):\n",
    "\tsorted_refs = np.sort(reference_dist_array)\n",
    "\tn = len(sorted_refs)\n",
    "\tranks = np.searchsorted(sorted_refs, dist_matrix)\n",
    "\tprobs = (ranks + 1) / (n + 1)\n",
    "\treturn probs\n",
    "\n",
    "\n",
    "nn_dists_A = get_nearest_neighbor_distances(Model_A[\"dist_matrix\"])\n",
    "nn_dists_B = get_nearest_neighbor_distances(Model_B[\"dist_matrix\"])\n",
    "\n",
    "Prob_A = convert_dist_to_prob(Model_A[\"dist_matrix\"], nn_dists_A)\n",
    "Prob_B = convert_dist_to_prob(Model_B[\"dist_matrix\"], nn_dists_B)\n",
    "\n",
    "Prob_Fused = np.minimum(Prob_A, Prob_B)\n",
    "\n",
    "# --- THRESHOLD CALCULATION ---\n",
    "# Correct Logic: Pairs (P_true) / Total Universe Clusters (C_prime)\n",
    "if N_total_clusters_C_prime > 0:\n",
    "\tPROB_THRESHOLD = len(P_true) / N_total_clusters_C_prime\n",
    "else:\n",
    "\tPROB_THRESHOLD = 0.0\n",
    "\n",
    "print(f\"Inferred Probability Threshold: {PROB_THRESHOLD:.6f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STAGE 3 OUTPUT: Probabilistic Outlier Model (P < {PROB_THRESHOLD:.6f})\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "cluster_model = AgglomerativeClustering(\n",
    "\tn_clusters=None,\n",
    "\tdistance_threshold=PROB_THRESHOLD,\n",
    "\tmetric=\"precomputed\",\n",
    "\tlinkage=\"complete\",\n",
    ")\n",
    "\n",
    "labels = cluster_model.fit_predict(Prob_Fused)\n",
    "\n",
    "df = pd.DataFrame({\"string\": strings, \"label\": labels, \"idx\": range(len(strings))})\n",
    "groups = [g for _, g in df.groupby(\"label\") if len(g) > 1]\n",
    "groups.sort(key=lambda x: len(x), reverse=True)\n",
    "\n",
    "print(f\"Found {len(groups)} significant groups.\\n\")\n",
    "\n",
    "for i, group in enumerate(groups):\n",
    "\tindices = group[\"idx\"].tolist()\n",
    "\tcluster_strs = group[\"string\"].tolist()\n",
    "\n",
    "\tref_vectors = Model_B[\"vectors\"][indices]\n",
    "\tref_prec = Model_B[\"precision\"]\n",
    "\tlocal_mean = np.mean(ref_vectors, axis=0)\n",
    "\n",
    "\tdists = []\n",
    "\tmin_d = float(\"inf\")\n",
    "\trep_i = -1\n",
    "\n",
    "\tfor loc_i, glob_i in enumerate(indices):\n",
    "\t\td = mahalanobis(Model_B[\"vectors\"][glob_i], local_mean, ref_prec)\n",
    "\t\tdists.append(d)\n",
    "\t\tif d < min_d:\n",
    "\t\t\tmin_d = d\n",
    "\t\t\trep_i = loc_i\n",
    "\n",
    "\tprint(f\"GROUP {i+1} (Size: {len(indices)}) [Radius: {max(dists):.4f}]\")\n",
    "\n",
    "\tfor idx, s in enumerate(cluster_strs):\n",
    "\t\tprefix = \" [CENTROID] \" if idx == rep_i else \"            \"\n",
    "\t\tprint(f\"{prefix} {s}\")\n",
    "\tprint(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
