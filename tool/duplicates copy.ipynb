{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7734ff5",
   "metadata": {},
   "source": [
    "# Background and Problem Space\n",
    "## Background\n",
    "I am looking to find some probabalistic model to determine likely duplicate strings. The strings have a common grammer and structure, additionally assertained from similar texts. \n",
    "\n",
    "For an example that we will use for the remainder of the task our strings are of the form \"Does the privacy policy affirm {X}?\", where `X` is some variable statement. And they're all assertained from different privacy policies about similar products.\n",
    "\n",
    "I have then for each string in our corpus a selection of embedding vectors, each embedding vector is assertained from the same fundamental model, however there are slight differences. Some have been assertained by manipulating the string, e.g via these two example functions (written in python as a demonstration):\n",
    "```\n",
    "def example_question_manipulation_1(question):\n",
    "\tprefix = \"Does the privacy policy affirm\"\n",
    "\treplacement = \"The privacy policy affirms\"\n",
    "\tprocessed_text = (\"The privacy policy affirms\" + question[len(prefix) :])[:-1] + \".\"\n",
    "\n",
    "\treturn processed_text\n",
    "\n",
    "def example_question_manipulation_2(question):\n",
    "\tprefix = \"Does the privacy policy affirm that \"\n",
    "\tprocessed_text = (question[len(prefix) :].capitalize())[:-1] + \".\"\n",
    "\treturn processed_text\n",
    "```\n",
    "And the specific task type e.g \"SEMANTIC_SIMILARITY\", \"FACT_VERIFICATION\" etc, we will use `gemini-embedding-001` as our cannonical embedding model.\n",
    "## Problem Space\n",
    "We want to derive the factors required to construct a probabilistic model $P(\\text{Duplicate} \\mid X)$, where $X$ is a multivariate state including:\n",
    "-   The distance vector $\\vec{d}$ (Mahalanobis, Cosine, etc.).\n",
    "-   The embedding source type (e.g., specific manipulation functions).\n",
    "-   Consensus signals (whether the pair is a nearest neighbor across multiple embedding views, whether we have highly overlapping clustors).\n",
    "-   Local topology of clusters and vector spaces\n",
    "-   Similarities in topology of vector spaces, since we expect different embedding types to be correlated since they all represent \"views\" on the same fundamental information.\n",
    "\n",
    "**Constraints & Assumptions:**\n",
    "1. **Zero-Shot / Unsupervised:** We have no labeled 'true duplicates'.\n",
    "2. **No Arbitrary Heuristics:** We reject metrics like \"top 1% nearest neighbors.\" All probabilities must be inferred from the structural properties of the vector space and the consensus between embedding views.\n",
    "3. **Multivariate Dependencies:** The probability is not solely a function of scalar distance. It may be conditioned on the specific embedding model used, the stability of the nearest-neighbor relationship across models, etc.\n",
    "\n",
    "## Tentative Considerations for the Solution Space\n",
    "I have identified the preliminarr empirical signals (metrics). Please analyze how these function as variables in a probabilistic framework (e.g., as priors, likelihood ratios, or density estimation parameters):\n",
    "\n",
    "1.  **Metric: Pairwise Jaccard Index**\n",
    "    The intersection over union of pair sets identified by different embedding models.\n",
    "2.  **Metric: Average Consensus-Supporting Group Count (ACGC):**\n",
    "\tThe average count of clusters per model that are validated by the intersection of all pair sets.\n",
    "3.  **Factor: Empirical distributions of Nearest Neighbours**\n",
    "4.  **Factor:Vector & Distance Deltas**\n",
    "    The raw distance metrics ($D$) and the variance of $D$ across different embedding manipulations.\n",
    "5.  **Idea: Pairwise filtering:**\n",
    "\tFilter pairs for our metrics based on some conditions, i.e using all pairs in some cluster, only using nearest neighbour pairs. This may help us determine the relationship between duplicates, being a nearest neighbour vs not being a nearest neighbour\n",
    "\n",
    "**Important:**\n",
    "In the following task is about defining the problem space, metrics, distributions and assumptions **NOT** creating the duplicate detection model.\n",
    "\n",
    "\n",
    "# Task\n",
    "I would like you to help me fill out relevant factors and metrics we can use in our model.\n",
    "\n",
    "Please determine any other potentially relevant things to consider that may be valuable to our model,\n",
    "\n",
    "The initial start that I have determined in our solution space is as following.\n",
    "- Quantify the distance delta between two embedding vectors, using some distance metric D (e.g Mahalanobis, Cosine, L1, L2)\n",
    "- We can determine some useful empirical distributions and utilise (Extreme Value Theory) and other mathematical frameworks, e.g distributions of nearest neighbours.\n",
    "- We can then cluster distances via AgglomerativeClustering\n",
    "- We can threshold distances from Agglomerative Clustering, this can give us some other useful metrics like:\n",
    "\t- **Threshold Minimisations**:\n",
    "\t\t- Maximise the Jaccard Index of pairs between embedding_vector types: `len(set.intersection(*pair_sets))/set.union(*pair_sets)`\n",
    "\t\t- Maximise our Average Consensus-Supporting Group Count (ACGC) i.e the average count of clusters per model that are validated by the intersection of all pair sets.\n",
    "\t- **Threshold Maximisation**:\n",
    "\t\t- Given a metric score find the maximum distance threshold that retains this index, or more specific constraints e.g:\n",
    "\t\tFor `Jaccard Index` we can maximize distance threshold T, subject to the constraint that the set of identified duplicate pairs remains invariant\n",
    "\n",
    "We can also for these thresholding restrict our pairsets, e.g from pairsets in a cluster, to pairsets in a cluster which are nearest neighbours. For some metrics like Jaccard our score may be similar (I may even suggest that it will with high likelihood be identical), or minimialy change (ACGC). This may be interesting since our empirical distributions assume that all semantic duplicates will be a nearest neighbour in at least one model. However our metrics if we chose all pairs in clusters include non-nearest neighbour pairs. We may be able to infer some likelihood of (duplicates | not nearest neighbour) from manipulation of our pairing mechanism.\n",
    "\n",
    "With well-behaved vector spaces and with some assumptions we may be able to compute some other metrics and distributions, i.e:\n",
    "- Deduce correlation of vectors associated with the same string given different embedding vector types.\n",
    "- Deduce correlation of distances associated with the same pairing of given different embedding vector types.\n",
    "\n",
    "# Constraints:\n",
    "We are not solving the actual problem yet, we are only expanding the factors in our solution space, therefore I expect:\n",
    "- Mainly mathematical derivations and qualitiative discussions. I expect you will only return codeblocks if absolutely necessary to demonstrate some example or test.\n",
    "- If I ask specific questions, your answer should be targetted and precise. Treat each question as self contained, and the answer does not modify the context of our discussion and solution space until I explicitly say we will add it to. No premature implementations/incorperations before we have fully discussed the answer.\n",
    "- Treat this as a Zero-Shot Unsupervised problem. We have no labeled duplicates. The definition of a 'duplicate' must be inferred from the structural properties of the vector space and the consensus between embedding views.\"\n",
    "- We strictly reject the use of arbitrary, exogenous thresholds (e.g., \"fixed distance < 0.5\" or \"top 1% nearest neighbors\") as they fail to capture the underlying uncertainty of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd5660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "QUESTIONS_FILE = \"./data/questions_filter_after.json\"\n",
    "\n",
    "\n",
    "def _loadJson(filepath):\n",
    "\tif not os.path.exists(filepath):\n",
    "\t\tprint(f\"Warning: File not found: {filepath}\")\n",
    "\t\treturn {}\n",
    "\ttry:\n",
    "\t\twith open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "\t\t\treturn json.load(f)\n",
    "\texcept json.JSONDecodeError:\n",
    "\t\tprint(f\"Error decoding JSON: {filepath}\")\n",
    "\t\treturn {}\n",
    "\n",
    "\n",
    "qdata = _loadJson(QUESTIONS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad13d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd8f27e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# executor = concurrent.futures.ThreadPoolExecutor(max_workers=num_policy_chunks)\n",
    "# e_dict = dict()\n",
    "# for index, chunk in enumerate(policy_chunks):\n",
    "# \te_dict[index] = executor.submit(\n",
    "# \t\tself.processPolicyChunks, chunk, policy_hash, policy_name, index\n",
    "# \t)\n",
    "# executor.shutdown(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bc74ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4738fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.covariance import LedoitWolf\n",
    "\n",
    "\n",
    "# abstracted\n",
    "def getMahalanobisDistances(vectors_a, vectors_b):\n",
    "\tnorms = np.linalg.norm(vectors_a, axis=1, keepdims=True)\n",
    "\tcleaned_vectors = vectors_a / (norms + 1e-10)\n",
    "\n",
    "\tlw = LedoitWolf()\n",
    "\tlw.fit(cleaned_vectors)\t# assumption vectors_a=vectors_b\n",
    "\tprecision_matrix = lw.precision_\n",
    "\n",
    "\tdist_matrix = cdist(\n",
    "\t\tcleaned_vectors, cleaned_vectors, metric=\"mahalanobis\", VI=precision_matrix\n",
    "\t)\n",
    "\treturn dist_matrix, precision_matrix\n",
    "\n",
    "\n",
    "Distance_Processors = {\n",
    "\t\"cosine\": lambda emb_a, emb_b: 1.0\n",
    "\t- (emb_a @ emb_b.T)\n",
    "\t/ (\n",
    "\t\tnp.linalg.norm(emb_a, axis=1, keepdims=True)\n",
    "\t\t@ np.linalg.norm(emb_b, axis=1, keepdims=True).T\n",
    "\t\t+ 1e-10\n",
    "\t),\n",
    "\t\"l1\": lambda emb_a, emb_b: np.sum(np.abs(emb_a[..., np.newaxis] - emb_b.T), axis=1),\n",
    "\t\"l2\": lambda emb_a, emb_b: np.linalg.norm(emb_a[..., np.newaxis] - emb_b.T, axis=1),\n",
    "\t\"dot\": lambda emb_a, emb_b: emb_a @ emb_b.T,\n",
    "\t\"mahalanobis\": lambda emb_a, emb_b: getMahalanobisDistances(emb_a, emb_b),\n",
    "}\n",
    "\n",
    "\n",
    "def _prepareModelArtifact(\n",
    "\traw_vectors,\n",
    "\tsemantic_data,\n",
    "\ttruncation_dim=256,\n",
    "\tdistance_metric=\"mahalanobis\",\n",
    "\tdebug=True,\n",
    "):\n",
    "\t# 1. Truncation\n",
    "\tdata_matrix = np.array(raw_vectors)\n",
    "\tinput_dim = data_matrix.shape[1]\n",
    "\n",
    "\tif input_dim < truncation_dim and debug:\n",
    "\t\tprint(\n",
    "\t\t\tf\"Warning: Vector dimension ({input_dim}) is smaller than truncation limit ({truncation_dim}). Proceeding without truncation.\"\n",
    "\t\t)\n",
    "\n",
    "\tdata_truncated = data_matrix[:, :truncation_dim]\n",
    "\n",
    "\t# 2. Distance Calculation\n",
    "\tdist_output = Distance_Processors[distance_metric](data_truncated, data_truncated)\n",
    "\n",
    "\tprecision_matrix = None\n",
    "\tif distance_metric == \"mahalanobis\":\n",
    "\t\tdist_matrix, precision_matrix = dist_output\n",
    "\telse:\n",
    "\t\tdist_matrix = dist_output\n",
    "\n",
    "\t# 3. NN Indices (In-place modification to avoid copy overhead)\n",
    "\tnp.fill_diagonal(dist_matrix, float(\"inf\"))\n",
    "\tnn_indices = np.argmin(dist_matrix, axis=1)\n",
    "\tnp.fill_diagonal(dist_matrix, 0.0)\n",
    "\n",
    "\treturn {\n",
    "\t\t\"dist_matrix\": dist_matrix,\n",
    "\t\t\"vectors\": data_truncated,\n",
    "\t\t\"precision\": precision_matrix,\n",
    "\t\t\"semantic_data\": semantic_data,\n",
    "\t\t\"metric\": distance_metric,\n",
    "\t\t\"nn_indices\": nn_indices,\n",
    "\t}\n",
    "\n",
    "\n",
    "def prepareModelArtifacts(\n",
    "\tdata_set, vector_keys, truncation_dim=256, distance_metric=\"mahalanobis\", debug=True\n",
    "):\n",
    "\tsemantic_data = list(data_set.keys())\n",
    "\tmodel_artifacts = {}\n",
    "\traw_vectors = {}\n",
    "\tfor key in vector_keys:\n",
    "\t\traw_vectors[key] = [data_set[s][key] for s in semantic_data]\n",
    "\n",
    "\tfor key in vector_keys:\n",
    "\t\tif debug:\n",
    "\t\t\tprint(f\"Processing {key}...\")\n",
    "\n",
    "\t\tmodel_artifacts[key] = _prepareModelArtifact(\n",
    "\t\t\traw_vectors[key],\n",
    "\t\t\tsemantic_data,\n",
    "\t\t\ttruncation_dim,\n",
    "\t\t\tdistance_metric,\n",
    "\t\t\tdebug,\n",
    "\t\t)\n",
    "\n",
    "\treturn model_artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af71511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "\n",
    "# def computeLocalDensityRanks(dist_matrix, k_neighbors=50):\n",
    "# \t\"\"\"\n",
    "# \tConverts raw scalar distances into Local Density Scores (Empirical CDF).\n",
    "\n",
    "# \tThis effectively normalizes the manifold: a distance of 0.2 is treated\n",
    "# \tdifferently in a dense cluster vs. a sparse region.\n",
    "\n",
    "# \tArgs:\n",
    "# \t    dist_matrix: (N x N) Raw distance matrix (e.g., Mahalanobis).\n",
    "# \t    k_neighbors: The effective 'local bandwidth'. We normalize ranks\n",
    "# \t                 relative to this count.\n",
    "\n",
    "# \tReturns:\n",
    "# \t    local_rank_matrix: (N x N) Values in [0, 1].\n",
    "# \t                       Values near 0 indicate the pair is within the\n",
    "# \t                       tightest local neighbors. Values clipped at 1.0\n",
    "# \t                       indicate 'Background/Noise'.\n",
    "# \t\"\"\"\n",
    "# \t# 1. Compute Ranks Row-wise\n",
    "# \t# method='average' assigns the average rank to ties, preserving statistical properties\n",
    "# \t# We rank strictly along axis 1 (neighbors of row i)\n",
    "# \tranks = rankdata(dist_matrix, axis=1, method=\"average\")\n",
    "\n",
    "# \t# 2. Normalize by k_neighbors (The Local Density assumption)\n",
    "# \t# If k=50, the 1st neighbor gets score 0.02, the 50th gets 1.0.\n",
    "# \t# The 51st neighbor gets 1.02.\n",
    "# \tnormalized_ranks = ranks / k_neighbors\n",
    "\n",
    "# \t# 3. Clip to [0, 1]\n",
    "# \t# We treat anything outside the local neighborhood (rank > k) as\n",
    "# \t# effectively \"Probability 1.0\" (Maximum Entropy / Background).\n",
    "# \t# In EVT terms, we only care about the tail approaching 0.\n",
    "# \tlocal_rank_matrix = np.clip(normalized_ranks, 0.0, 1.0)\n",
    "\n",
    "# \treturn local_rank_matrix\n",
    "\n",
    "\n",
    "def computeLocalDensityRanks(dist_matrix, k_neighbors=50):\n",
    "\t\"\"\"\n",
    "\tRevised: Subtracts 1 from rank so Self-Match is 0.0 and\n",
    "\tFirst Neighbor is 1.0/k.\n",
    "\t\"\"\"\n",
    "\t# method='min' ensures ties get the lower rank (good for strict duplicates)\n",
    "\tranks = rankdata(dist_matrix, axis=1, method=\"min\")\n",
    "\n",
    "\t# Subtract 1 so the diagonal (self) is 0.0\n",
    "\t# The first NN is now 1.0\n",
    "\tranks = ranks - 1.0\n",
    "\n",
    "\t# Normalize\n",
    "\tnormalized_ranks = ranks / k_neighbors\n",
    "\n",
    "\t# Clip [0, 1]\n",
    "\tlocal_rank_matrix = np.clip(normalized_ranks, 0.0, 1.0)\n",
    "\n",
    "\treturn local_rank_matrix\n",
    "\n",
    "\n",
    "def computeFusedEVTStatistic(rank_matrices_dict):\n",
    "\t\"\"\"\n",
    "\tCombines views using the Extreme Value Theory 'Minimum' assumption.\n",
    "\n",
    "\tHypothesis:\n",
    "\t    If two strings are equivalent, they will collide (rank -> 0) in\n",
    "\t    AT LEAST ONE valid semantic manipulation.\n",
    "\n",
    "\tArgs:\n",
    "\t    rank_matrices_dict: Dictionary {model_key: (N x N) local_rank_matrix}.\n",
    "\n",
    "\tReturns:\n",
    "\t    fused_statistic_matrix: (N x N) The observation variable X.\n",
    "\t                            Represents the \"best case\" topological\n",
    "\t                            closeness across all views.\n",
    "\t\"\"\"\n",
    "\tif not rank_matrices_dict:\n",
    "\t\traise ValueError(\"rank_matrices_dict is empty.\")\n",
    "\n",
    "\t# Convert dict values to a list of matrices\n",
    "\tmatrices = list(rank_matrices_dict.values())\n",
    "\n",
    "\t# Stack along a new axis (M, N, N)\n",
    "\tstacked_matrices = np.stack(matrices, axis=0)\n",
    "\n",
    "\t# Compute element-wise minimum along the model axis\n",
    "\tfused_statistic_matrix = np.min(stacked_matrices, axis=0)\n",
    "\n",
    "\treturn fused_statistic_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fce9da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Test Data Generation ---\n",
    "# # Model A: Standard sorted distances\n",
    "# dist_A = np.array(\n",
    "# \t[\n",
    "# \t\t[0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "# \t\t[0.1, 0.0, 0.5, 0.5, 0.8],\t# Note the tie at 0.5\n",
    "# \t\t[0.2, 0.5, 0.0, 0.9, 0.1],\n",
    "# \t\t[0.3, 0.5, 0.9, 0.0, 0.2],\n",
    "# \t\t[0.4, 0.8, 0.1, 0.2, 0.0],\n",
    "# \t]\n",
    "# )\n",
    "\n",
    "# # Model B: Inverted distances (to test Fusion)\n",
    "# # e.g., what was far in A is close in B\n",
    "# dist_B = np.array(\n",
    "# \t[\n",
    "# \t\t[0.0, 0.4, 0.3, 0.2, 0.1],\n",
    "# \t\t[0.4, 0.0, 0.2, 0.1, 0.3],\n",
    "# \t\t[0.3, 0.2, 0.0, 0.1, 0.5],\n",
    "# \t\t[0.2, 0.1, 0.1, 0.0, 0.4],\n",
    "# \t\t[0.1, 0.3, 0.5, 0.4, 0.0],\n",
    "# \t]\n",
    "# )\n",
    "\n",
    "# print(\"--- Test: Local Density Ranks (k=5) ---\")\n",
    "# # Using k=5 means ranks 1-5 become probabilities 0.2, 0.4, 0.6, 0.8, 1.0\n",
    "# rank_A = computeLocalDensityRanks(dist_A, k_neighbors=5)\n",
    "# rank_B = computeLocalDensityRanks(dist_B, k_neighbors=5)\n",
    "\n",
    "# print(\"\\nMatrix A Ranks (Row 0 should be ordered 0.2 -> 1.0):\")\n",
    "# print(rank_A[0])\n",
    "\n",
    "# print(\"\\nMatrix A Row 1 (Handling Ties at 0.5):\")\n",
    "# # Distances: [0.1, 0.0, 0.5, 0.5, 0.8]\n",
    "# # Ranks:     [2,   1,   3.5, 3.5, 5  ] (Average rank for ties)\n",
    "# # Norm:      [0.4, 0.2, 0.7, 0.7, 1.0]\n",
    "# print(f\"Distances: {dist_A[1]}\")\n",
    "# print(f\"Ranks:     {rank_A[1]}\")\n",
    "\n",
    "# print(\"\\n--- Test: EVT Fusion (Minimum) ---\")\n",
    "# rank_dict = {\"Model_A\": rank_A, \"Model_B\": rank_B}\n",
    "# fused = computeFusedEVTStatistic(rank_dict)\n",
    "\n",
    "# print(\"Fused Matrix Row 0:\")\n",
    "# print(f\"Rank A: {rank_A[0]}\")\n",
    "# print(f\"Rank B: {rank_B[0]}\")\n",
    "# print(f\"Fused : {fused[0]}\")\n",
    "\n",
    "# # Verification for Row 0, Index 1:\n",
    "# # A=0.4 (Dist 0.1), B=1.0 (Dist 0.4). Min should be 0.4.\n",
    "# assert fused[0, 1] == 0.4\n",
    "# print(\"\\nTest passed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5ef44ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Preparing Model Artifacts...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# # --- REFINED HELPER FUNCTIONS ---\u001b[39;00m\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m \n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# --- RE-RUN PIPELINE ---\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m>>> Preparing Model Artifacts...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m artifacts = \u001b[43mprepareModelArtifacts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m>>> Running Probabilistic Pipeline (Debug Mode)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# 1. Local Density\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36mprepareModelArtifacts\u001b[39m\u001b[34m(data_set, vector_keys, truncation_dim, distance_metric, debug)\u001b[39m\n\u001b[32m     88\u001b[39m \t\u001b[38;5;28;01mif\u001b[39;00m debug:\n\u001b[32m     89\u001b[39m \t\t\u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m \tmodel_artifacts[key] = \u001b[43m_prepareModelArtifact\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43mraw_vectors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43msemantic_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43mtruncation_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43mdistance_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m\t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model_artifacts\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36m_prepareModelArtifact\u001b[39m\u001b[34m(raw_vectors, semantic_data, truncation_dim, distance_metric, debug)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_prepareModelArtifact\u001b[39m(\n\u001b[32m     37\u001b[39m \traw_vectors,\n\u001b[32m     38\u001b[39m \tsemantic_data,\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m ):\n\u001b[32m     43\u001b[39m \t\u001b[38;5;66;03m# 1. Truncation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \tdata_matrix = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_vectors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \tinput_dim = data_matrix.shape[\u001b[32m1\u001b[39m]\n\u001b[32m     47\u001b[39m \t\u001b[38;5;28;01mif\u001b[39;00m input_dim < truncation_dim \u001b[38;5;129;01mand\u001b[39;00m debug:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "# # --- REFINED HELPER FUNCTIONS ---\n",
    "\n",
    "\n",
    "# def computeLocalDensityRanks(dist_matrix, k_neighbors=50):\n",
    "# \t\"\"\"\n",
    "# \tRevised: Subtracts 1 from rank so Self-Match is 0.0 and\n",
    "# \tFirst Neighbor is 1.0/k.\n",
    "# \t\"\"\"\n",
    "# \t# method='min' ensures ties get the lower rank (good for strict duplicates)\n",
    "# \tranks = rankdata(dist_matrix, axis=1, method=\"min\")\n",
    "\n",
    "# \t# Subtract 1 so the diagonal (self) is 0.0\n",
    "# \t# The first NN is now 1.0\n",
    "# \tranks = ranks - 1.0\n",
    "\n",
    "# \t# Normalize\n",
    "# \tnormalized_ranks = ranks / k_neighbors\n",
    "\n",
    "# \t# Clip [0, 1]\n",
    "# \tlocal_rank_matrix = np.clip(normalized_ranks, 0.0, 1.0)\n",
    "\n",
    "# \treturn local_rank_matrix\n",
    "\n",
    "\n",
    "# def _initializeParametersViaConsensus(\n",
    "# \tfused_statistic_matrix, percentile_threshold=0.02\n",
    "# ):\n",
    "# \t\"\"\"\n",
    "# \tRelaxed threshold to 0.02 (2%) to ensure we catch the signal tail\n",
    "# \tbefore the EM algorithm refines it.\n",
    "# \t\"\"\"\n",
    "# \treturn _initializeParametersViaNaiveQuantile(\n",
    "# \t\tfused_statistic_matrix, signal_quantile=percentile_threshold\n",
    "# \t)\n",
    "\n",
    "\n",
    "# --- RE-RUN PIPELINE ---\n",
    "\n",
    "print(\">>> Preparing Model Artifacts...\")\n",
    "artifacts = prepareModelArtifacts(qdata, model_keys, truncation_dim=256, debug=False)\n",
    "\n",
    "print(\"\\n>>> Running Probabilistic Pipeline (Debug Mode)...\")\n",
    "\n",
    "# 1. Local Density\n",
    "rank_matrices = {}\n",
    "for key, artifact in artifacts.items():\n",
    "\trank_matrices[key] = computeLocalDensityRanks(artifact[\"dist_matrix\"], k_neighbors=50)\n",
    "\n",
    "# 2. Fusion\n",
    "fused_matrix = computeFusedEVTStatistic(rank_matrices)\n",
    "\n",
    "# DEBUG: Check the data range\n",
    "off_diag = fused_matrix[~np.eye(fused_matrix.shape[0], dtype=bool)]\n",
    "print(\n",
    "\tf\"Fused Data Stats: Min={off_diag.min():.4f}, Max={off_diag.max():.4f}, Mean={off_diag.mean():.4f}\"\n",
    ")\n",
    "print(f\"Values < 0.05: {np.sum(off_diag < 0.05)} pairs\")\n",
    "\n",
    "# 3. Initialization\n",
    "# Using 2% quantile\n",
    "init_params = _initializeParametersViaConsensus(fused_matrix, percentile_threshold=0.02)\n",
    "print(f\"New Init Params: {init_params}\")\n",
    "\n",
    "# 4. Modeling\n",
    "likelihood_matrix, final_params = fitWeibullBetaMixture(\n",
    "\tfused_matrix, initial_params=init_params\n",
    ")\n",
    "print(f\"Final Model Params: {final_params}\")\n",
    "\n",
    "# 5. Topology\n",
    "prior_matrix = computeTransitivityPrior(likelihood_matrix)\n",
    "posterior_matrix = computeFinalPosterior(likelihood_matrix, prior_matrix)\n",
    "\n",
    "# 6. Extract\n",
    "extractAndPrintClusters(\n",
    "\tposterior_matrix, artifacts[model_keys[0]][\"semantic_data\"], threshold=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2592907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import expon, beta\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "\n",
    "def _get_off_diagonal_samples(matrix):\n",
    "\t\"\"\"Helper: Flattens matrix and removes self-matching diagonal.\"\"\"\n",
    "\tN = matrix.shape[0]\n",
    "\t# Create mask for off-diagonal\n",
    "\tmask = ~np.eye(N, dtype=bool)\n",
    "\treturn matrix[mask]\n",
    "\n",
    "\n",
    "def _initializeParametersViaNaiveQuantile(fused_statistic_matrix, signal_quantile=0.01):\n",
    "\t\"\"\"\n",
    "\tFallback initialization.\n",
    "\tAssumes bottom 1% are Signal (Exponential), top 99% are Noise (Beta).\n",
    "\t\"\"\"\n",
    "\tdata = _get_off_diagonal_samples(fused_statistic_matrix)\n",
    "\n",
    "\t# 1. Hard split\n",
    "\tthreshold = np.quantile(data, signal_quantile)\n",
    "\tsignal_data = data[data <= threshold]\n",
    "\tnoise_data = data[data > threshold]\n",
    "\n",
    "\t# 2. Estimate Signal (Exponential: lambda = 1 / mean)\n",
    "\t# Add epsilon to avoid divide by zero if perfect match\n",
    "\tlamb = 1.0 / (np.mean(signal_data) + 1e-6)\n",
    "\n",
    "\t# 3. Estimate Noise (Beta: Method of Moments)\n",
    "\tmu = np.mean(noise_data)\n",
    "\tvar = np.var(noise_data)\n",
    "\n",
    "\t# Beta Method of Moments constraints\n",
    "\tif var >= mu * (1 - mu):\n",
    "\t\t# Fallback if variance is too high for Beta (implies bimodal/uniform)\n",
    "\t\talpha, b_param = 1.0, 1.0\n",
    "\telse:\n",
    "\t\tcommon = (mu * (1 - mu) / (var + 1e-9)) - 1\n",
    "\t\talpha = mu * common\n",
    "\t\tb_param = (1 - mu) * common\n",
    "\n",
    "\treturn {\n",
    "\t\t\"pi\": signal_quantile,\n",
    "\t\t\"lambda\": lamb,\n",
    "\t\t\"alpha\": max(alpha, 1.0),\t# Enforce > 1 for bell/flat shapes\n",
    "\t\t\"beta\": max(b_param, 1.0),\n",
    "\t}\n",
    "\n",
    "\n",
    "def _initializeParametersViaConsensus(\n",
    "\tfused_statistic_matrix, percentile_threshold=0.02\n",
    "):\n",
    "\t\"\"\"\n",
    "\tRelaxed threshold to 0.02 (2%) to ensure we catch the signal tail\n",
    "\tbefore the EM algorithm refines it.\n",
    "\t\"\"\"\n",
    "\treturn _initializeParametersViaNaiveQuantile(\n",
    "\t\tfused_statistic_matrix, signal_quantile=percentile_threshold\n",
    "\t)\n",
    "\n",
    "\n",
    "# def _initializeParametersViaConsensus(\n",
    "# \tfused_statistic_matrix, percentile_threshold=0.01\n",
    "# ):\n",
    "# \t\"\"\"\n",
    "# \tBootstraps using a 'High-Precision' assumption.\n",
    "# \tSince we don't have the Jaccard sets here, we assume 'Consensus'\n",
    "# \trefers to the extremely low-rank tail (e.g., top 0.1%).\n",
    "# \tThis is a stricter version of the Naive Quantile.\n",
    "# \t\"\"\"\n",
    "# \t# Use a much stricter quantile to simulate \"Consensus Intersection\"\n",
    "# \treturn _initializeParametersViaNaiveQuantile(\n",
    "# \t\tfused_statistic_matrix, signal_quantile=0.001\n",
    "# \t)\n",
    "\n",
    "\n",
    "def fitWeibullBetaMixture(\n",
    "\tfused_statistic_matrix, initial_params=None, max_iter=20, tol=1e-4\n",
    "):\n",
    "\t\"\"\"\n",
    "\tFits: P(x) = pi * Exp(x|lambda) + (1-pi) * Beta(x|alpha, beta)\n",
    "\tUsing a simplified EM algorithm.\n",
    "\t\"\"\"\n",
    "\tdata = _get_off_diagonal_samples(fused_statistic_matrix)\n",
    "\t# Clip data to (epsilon, 1-epsilon) for numerical stability of Beta/Log\n",
    "\tdata = np.clip(data, 1e-6, 1.0 - 1e-6)\n",
    "\n",
    "\t# --- Initialization ---\n",
    "\tif initial_params is None:\n",
    "\t\tparams = _initializeParametersViaNaiveQuantile(fused_statistic_matrix)\n",
    "\telse:\n",
    "\t\tparams = initial_params\n",
    "\n",
    "\tpi = params[\"pi\"]\n",
    "\tlamb = params[\"lambda\"]\n",
    "\ta = params[\"alpha\"]\n",
    "\tb = params[\"beta\"]\n",
    "\n",
    "\tlog_likelihood_old = -np.inf\n",
    "\n",
    "\t# --- EM Loop ---\n",
    "\tfor i in range(max_iter):\n",
    "\t\t# 1. E-Step: Calculate Responsibilities (Posterior of Hidden Var Z)\n",
    "\n",
    "\t\t# PDF of Signal (Exponential)\n",
    "\t\tpdf_signal = expon.pdf(data, scale=1.0 / lamb)\n",
    "\n",
    "\t\t# PDF of Noise (Beta)\n",
    "\t\tpdf_noise = beta.pdf(data, a, b)\n",
    "\n",
    "\t\t# Weighted Probs\n",
    "\t\tweighted_signal = pi * pdf_signal\n",
    "\t\tweighted_noise = (1 - pi) * pdf_noise\n",
    "\n",
    "\t\t# Normalization (Total Evidence)\n",
    "\t\ttotal_evidence = weighted_signal + weighted_noise + 1e-10\n",
    "\n",
    "\t\t# Gamma: P(Z=Signal | x)\n",
    "\t\tgamma = weighted_signal / total_evidence\n",
    "\n",
    "\t\t# 2. M-Step: Update Parameters\n",
    "\n",
    "\t\t# N_signal (Effective count of signal items)\n",
    "\t\tN_s = np.sum(gamma)\n",
    "\t\tN_n = len(data) - N_s\n",
    "\n",
    "\t\t# Update Pi (Mixing Coefficient)\n",
    "\t\tpi = N_s / len(data)\n",
    "\n",
    "\t\t# Update Lambda (MLE for Weighted Exponential = 1 / weighted_mean)\n",
    "\t\tweighted_sum_x = np.sum(gamma * data)\n",
    "\t\tlamb = 1.0 / (weighted_sum_x / (N_s + 1e-10))\n",
    "\n",
    "\t\t# Update Alpha/Beta (Weighted Method of Moments for speed/stability)\n",
    "\t\t# Calculate weighted mean/var for the Noise component (1-gamma)\n",
    "\t\tw_noise = 1 - gamma\n",
    "\t\tw_noise_sum = np.sum(w_noise) + 1e-10\n",
    "\n",
    "\t\tmu_n = np.sum(w_noise * data) / w_noise_sum\n",
    "\t\tvar_n = np.sum(w_noise * (data - mu_n) ** 2) / w_noise_sum\n",
    "\n",
    "\t\tif var_n < mu_n * (1 - mu_n):\n",
    "\t\t\tcommon = (mu_n * (1 - mu_n) / (var_n + 1e-10)) - 1\n",
    "\t\t\ta = max(mu_n * common, 1.0)\n",
    "\t\t\tb = max((1 - mu_n) * common, 1.0)\n",
    "\n",
    "\t\t# Check convergence\n",
    "\t\tlog_likelihood_new = np.sum(np.log(total_evidence))\n",
    "\t\tif abs(log_likelihood_new - log_likelihood_old) < tol:\n",
    "\t\t\tbreak\n",
    "\t\tlog_likelihood_old = log_likelihood_new\n",
    "\n",
    "\tfinal_params = {\"pi\": pi, \"lambda\": lamb, \"alpha\": a, \"beta\": b}\n",
    "\n",
    "\t# --- Compute Final Posterior Matrix ---\n",
    "\t# Apply the learned model to the FULL matrix (including diagonal, though diag is always P=1)\n",
    "\n",
    "\tpdf_S_full = expon.pdf(fused_statistic_matrix, scale=1.0 / lamb)\n",
    "\tpdf_N_full = beta.pdf(np.clip(fused_statistic_matrix, 1e-6, 1 - 1e-6), a, b)\n",
    "\n",
    "\tnumerator = pi * pdf_S_full\n",
    "\tdenominator = numerator + (1 - pi) * pdf_N_full + 1e-10\n",
    "\n",
    "\tposterior_matrix = numerator / denominator\n",
    "\n",
    "\treturn posterior_matrix, final_params\n",
    "\n",
    "\n",
    "def fitGaussianMixture(fused_statistic_matrix, n_components=2, initial_params=None):\n",
    "\t\"\"\"\n",
    "\tStandard GMM fallback using Sklearn.\n",
    "\tWe assume the component with the smaller Mean is the 'Equivalent' class.\n",
    "\t\"\"\"\n",
    "\tdata = _get_off_diagonal_samples(fused_statistic_matrix).reshape(-1, 1)\n",
    "\n",
    "\tgmm = GaussianMixture(\n",
    "\t\tn_components=n_components, covariance_type=\"full\", random_state=42\n",
    "\t)\n",
    "\tgmm.fit(data)\n",
    "\n",
    "\t# Identify which component is the \"Signal\" (Smallest Mean)\n",
    "\tmeans = gmm.means_.flatten()\n",
    "\tsignal_idx = np.argmin(means)\n",
    "\n",
    "\t# Compute Posteriors for the full matrix\n",
    "\tN = fused_statistic_matrix.shape[0]\n",
    "\treshaped_full = fused_statistic_matrix.reshape(-1, 1)\n",
    "\tprobs = gmm.predict_proba(reshaped_full)\t# Shape (N*N, 2)\n",
    "\n",
    "\t# Extract probability of the Signal Class\n",
    "\tposterior_flat = probs[:, signal_idx]\n",
    "\tposterior_matrix = posterior_flat.reshape(N, N)\n",
    "\n",
    "\tparams = {\n",
    "\t\t\"means\": means.tolist(),\n",
    "\t\t\"signal_idx\": int(signal_idx),\n",
    "\t\t\"weights\": gmm.weights_.tolist(),\n",
    "\t}\n",
    "\n",
    "\treturn posterior_matrix, params\n",
    "\n",
    "\n",
    "def estimatePairwiseLikelihood(\n",
    "\tfused_statistic_matrix, method=\"evt\", initial_params=None\n",
    "):\n",
    "\t\"\"\"\n",
    "\tOrchestrator for the likelihood step.\n",
    "\t\"\"\"\n",
    "\tif method == \"gmm\":\n",
    "\t\treturn fitGaussianMixture(fused_statistic_matrix, initial_params=initial_params)\n",
    "\telse:\n",
    "\t\treturn fitWeibullBetaMixture(fused_statistic_matrix, initial_params=initial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ddab11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Test: Likelihood Estimation (EVT) on 5x5 Data ---\n",
      "\n",
      "Fitted Parameters (likely unstable due to N=20):\n",
      "{'pi': np.float64(2.1460587414674316e-06), 'lambda': np.float64(1.6512690453805945), 'alpha': np.float64(9.369808231619515), 'beta': np.float64(9.002372162148973)}\n",
      "\n",
      "Posterior Probability Matrix (P(Equivalent | X)):\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "\n",
      "\n",
      "--- Test: Likelihood Estimation (GMM) on 5x5 Data ---\n",
      "\n",
      "GMM Signal Index: 1\n",
      "Posterior Probability Matrix (GMM):\n",
      "[[1.     1.     1.     1.     1.    ]\n",
      " [1.     1.     1.     1.     0.0011]\n",
      " [1.     1.     1.     1.     1.    ]\n",
      " [1.     1.     1.     1.     1.    ]\n",
      " [1.     1.     1.     1.     1.    ]]\n"
     ]
    }
   ],
   "source": [
    "# print(\"--- Test: Likelihood Estimation (EVT) on 5x5 Data ---\")\n",
    "\n",
    "# # Run the EVT Mixture Model\n",
    "# posterior_evt, params_evt = estimatePairwiseLikelihood(fused, method=\"evt\")\n",
    "\n",
    "# print(\"\\nFitted Parameters (likely unstable due to N=20):\")\n",
    "# print(params_evt)\n",
    "\n",
    "# print(\"\\nPosterior Probability Matrix (P(Equivalent | X)):\")\n",
    "# print(np.round(posterior_evt, 4))\n",
    "\n",
    "\n",
    "# print(\"\\n\\n--- Test: Likelihood Estimation (GMM) on 5x5 Data ---\")\n",
    "\n",
    "# # Run the Gaussian Mixture Model\n",
    "# posterior_gmm, params_gmm = estimatePairwiseLikelihood(fused, method=\"gmm\")\n",
    "\n",
    "# print(\"\\nGMM Signal Index:\", params_gmm[\"signal_idx\"])\n",
    "# print(\"Posterior Probability Matrix (GMM):\")\n",
    "# print(np.round(posterior_gmm, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323a63a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTransitivityPrior(likelihood_matrix):\n",
    "\t\"\"\"\n",
    "\tCalculates the Transitivity Score T_ij based on the Triangle Inequality assumption.\n",
    "\n",
    "\tT_ij represents the \"Flow of Probability\" from i to j through all possible intermediates k.\n",
    "\n",
    "\tFormula:\n",
    "\t    T_ij = (1 / N) * Sum_k ( P_ik * P_kj )\n",
    "\t         = (1 / N) * (Likelihood_Matrix @ Likelihood_Matrix)\n",
    "\n",
    "\tNote: We normalize by N to keep the prior in a [0, 1]-like range,\n",
    "\tpreventing it from exploding for large graphs.\n",
    "\n",
    "\tReturns:\n",
    "\t    prior_matrix: Matrix of structural support scores (N x N).\n",
    "\t\"\"\"\n",
    "\t# Matrix multiplication calculates sum_k(P_ik * P_kj) for all pairs (i,j)\n",
    "\t# This is effectively counting expected paths of length 2\n",
    "\traw_support = likelihood_matrix @ likelihood_matrix\n",
    "\n",
    "\t# Normalize by the number of possible intermediates (N) to keep scale consistent\n",
    "\t# Or normalize by the maximum support found to keep it relative.\n",
    "\t# Here we normalize by Max to treat the 'Best Connected' pair as Prior=1.0\n",
    "\tmax_val = np.max(raw_support)\n",
    "\tif max_val > 0:\n",
    "\t\tprior_matrix = raw_support / max_val\n",
    "\telse:\n",
    "\t\tprior_matrix = raw_support\t# All zeros\n",
    "\n",
    "\treturn prior_matrix\n",
    "\n",
    "\n",
    "def computeFinalPosterior(likelihood_matrix, prior_matrix, weight_factor=0.5):\n",
    "\t\"\"\"\n",
    "\tSynthesizes the Observation Likelihood and the Structural Prior.\n",
    "\n",
    "\tBayesian-ish Update:\n",
    "\t    P(Z|X, Graph) ~ P(X|Z) * P(Z|Graph)\n",
    "\n",
    "\tSince our 'Likelihood Matrix' is technically already a posterior P(Z|X) from the mixture model,\n",
    "\twe are effectively fusing two expert opinions:\n",
    "\t1. The Feature Expert (EVT Model)\n",
    "\t2. The Topology Expert (Transitivity Model)\n",
    "\n",
    "\tFormula:\n",
    "\t    P_final = (P_evt * (P_transitivity ^ weight))\n",
    "\n",
    "\tWe use geometric mean logic. If weight=0, topology is ignored.\n",
    "\tIf weight is high, we require strong structural support to believe the signal.\n",
    "\n",
    "\tReturns:\n",
    "\t    final_probability_matrix: The unified probability of equivalence.\n",
    "\t\"\"\"\n",
    "\t# Add epsilon to avoid log(0) issues if we were doing log-space,\n",
    "\t# but for direct multiplication, 0 * anything = 0 is desired behavior (Veto power).\n",
    "\n",
    "\t# We clip the prior to [epsilon, 1.0] to ensure we don't zero out\n",
    "\t# a strong signal just because it has NO triangles yet (isolated pair).\n",
    "\t# A prior of 0.0 implies \"Impossible\", which is too strong.\n",
    "\t# We set a floor of 0.1 for the prior: \"Topology is neutral/weak, rely on signal.\"\n",
    "\tsafe_prior = np.clip(prior_matrix, 0.1, 1.0)\n",
    "\n",
    "\tfinal_posterior = likelihood_matrix * (safe_prior**weight_factor)\n",
    "\n",
    "\treturn final_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811273db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Test: Transitivity Logic ---\n",
      "Original Likelihood (0 <-> 2): 0.4\n",
      "\n",
      "Computed Prior Matrix (Support Score):\n",
      "[[0.5988 0.2222 0.5    0.     0.    ]\n",
      " [0.2222 1.     0.2222 0.     0.    ]\n",
      " [0.5    0.2222 0.5988 0.     0.    ]\n",
      " [0.     0.     0.     0.     0.    ]\n",
      " [0.     0.     0.     0.     0.    ]]\n",
      "Prior Support for (0, 2): 0.5000\n",
      "\n",
      "Final Posterior (0 <-> 2): 0.2828\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Test: Transitivity Logic ---\")\n",
    "\n",
    "# 1. Create a synthetic Likelihood Matrix (5 nodes)\n",
    "# Everything is 0.0 except our triangle\n",
    "test_probs = np.zeros((5, 5))\n",
    "\n",
    "# Strong Link 0-1\n",
    "test_probs[0, 1] = 0.9\n",
    "test_probs[1, 0] = 0.9\n",
    "\n",
    "# Strong Link 1-2\n",
    "test_probs[1, 2] = 0.9\n",
    "test_probs[2, 1] = 0.9\n",
    "\n",
    "# Weak Link 0-2 (The one we hope Transitivity boosts)\n",
    "test_probs[0, 2] = 0.4\n",
    "test_probs[2, 0] = 0.4\n",
    "\n",
    "print(\"Original Likelihood (0 <-> 2):\", test_probs[0, 2])\n",
    "\n",
    "# 2. Compute Prior\n",
    "prior = computeTransitivityPrior(test_probs)\n",
    "\n",
    "print(\"\\nComputed Prior Matrix (Support Score):\")\n",
    "print(np.round(prior, 4))\n",
    "print(f\"Prior Support for (0, 2): {prior[0, 2]:.4f}\")\n",
    "# Explanation: Path 0->1->2 exists (0.9*0.9 = 0.81 support).\n",
    "# This should be high relative to max.\n",
    "\n",
    "# 3. Compute Final Posterior\n",
    "# We expect 0-2 to increase relative to its original weak standing,\n",
    "# or at least be confirmed as valid.\n",
    "final = computeFinalPosterior(test_probs, prior, weight_factor=0.5)\n",
    "\n",
    "print(f\"\\nFinal Posterior (0 <-> 2): {final[0, 2]:.4f}\")\n",
    "\n",
    "# Verification:\n",
    "# If prior is high (near 1.0), Final ~ 0.4 * 1.0 = 0.4\n",
    "# If prior was low (0.1), Final ~ 0.4 * 0.3 = 0.12 (Suppressed)\n",
    "# Since 0-2 has a strong friend (Node 1), it survives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28404702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateEquivalenceProbabilities(\n",
    "\tmodel_artifacts,\n",
    "\tk_neighbors=50,\n",
    "\tmodeling_method=\"evt\",\n",
    "\tuse_consensus_init=True,\n",
    "\tapply_transitivity=True,\n",
    "):\n",
    "\t\"\"\"\n",
    "\tMain Execution Pipeline.\n",
    "\n",
    "\tFlow:\n",
    "\t    1. Local Density: Convert distances -> Local Ranks.\n",
    "\t    2. Fusion: Combine ranks via Minimum (EVT).\n",
    "\t    3. Modeling: Fit Mixture Model to estimate P(E|X).\n",
    "\t    4. Topology: Refine P(E|X) using Graph Transitivity.\n",
    "\n",
    "\tArgs:\n",
    "\t    model_artifacts: Dict of prepared artifacts (dist_matrix, etc).\n",
    "\t    k_neighbors: Context window for local density.\n",
    "\t    modeling_method: \"evt\" or \"gmm\".\n",
    "\t    use_consensus_init: Bool, whether to use strict quantile init.\n",
    "\t    apply_transitivity: Bool, whether to apply structural prior.\n",
    "\n",
    "\tReturns:\n",
    "\t    results: Dictionary containing final matrices and params.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# --- 1. Local Density & Fusion ---\n",
    "\tprint(f\"--- Stage 1: Computing Local Density (k={k_neighbors}) ---\")\n",
    "\trank_matrices = {}\n",
    "\tfor key, artifact in model_artifacts.items():\n",
    "\t\t# Input validation\n",
    "\t\tif \"dist_matrix\" not in artifact:\n",
    "\t\t\tprint(f\"Skipping {key}: No dist_matrix found.\")\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\trank_matrices[key] = computeLocalDensityRanks(artifact[\"dist_matrix\"], k_neighbors)\n",
    "\n",
    "\tfused_matrix = computeFusedEVTStatistic(rank_matrices)\n",
    "\tprint(f\"Fused Statistics Computed. Shape: {fused_matrix.shape}\")\n",
    "\n",
    "\t# --- 2. Initialization Strategy ---\n",
    "\tinit_params = None\n",
    "\tif use_consensus_init:\n",
    "\t\tprint(\"--- Stage 2: Initializing via Consensus Heuristic ---\")\n",
    "\t\tinit_params = _initializeParametersViaConsensus(fused_matrix)\n",
    "\telse:\n",
    "\t\tprint(\"--- Stage 2: Initializing via Naive Quantile ---\")\n",
    "\t\tinit_params = _initializeParametersViaNaiveQuantile(fused_matrix)\n",
    "\n",
    "\tprint(f\"Initial Params: {init_params}\")\n",
    "\n",
    "\t# --- 3. Probabilistic Modeling ---\n",
    "\tprint(f\"--- Stage 3: Fitting Mixture Model ({modeling_method.upper()}) ---\")\n",
    "\n",
    "\tlikelihood_matrix, final_params = estimatePairwiseLikelihood(\n",
    "\t\tfused_matrix, method=modeling_method, initial_params=init_params\n",
    "\t)\n",
    "\n",
    "\tprint(\"Model Converged.\")\n",
    "\n",
    "\t# --- 4. Structural Topology ---\n",
    "\tposterior_matrix = likelihood_matrix\n",
    "\tprior_matrix = None\n",
    "\n",
    "\tif apply_transitivity:\n",
    "\t\tprint(\"--- Stage 4: Applying Transitivity Prior ---\")\n",
    "\t\tprior_matrix = computeTransitivityPrior(likelihood_matrix)\n",
    "\t\tposterior_matrix = computeFinalPosterior(likelihood_matrix, prior_matrix)\n",
    "\telse:\n",
    "\t\tprint(\"--- Stage 4: Transitivity Skipped ---\")\n",
    "\n",
    "\treturn {\n",
    "\t\t\"posterior_matrix\": posterior_matrix,\n",
    "\t\t\"likelihood_matrix\": likelihood_matrix,\t# Return raw model output too\n",
    "\t\t\"prior_matrix\": prior_matrix,\n",
    "\t\t\"fused_stats\": fused_matrix,\n",
    "\t\t\"model_params\": final_params,\n",
    "\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedff768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def initializeFromJaccardSet(fused_matrix, p_true_set):\n",
    "# \t\"\"\"\n",
    "# \tUses the High-Precision 'Jaccard Set' to calculate exact initial parameters.\n",
    "\n",
    "# \tArgs:\n",
    "# \t    fused_matrix: (N x N) The EVT statistic matrix.\n",
    "# \t    p_true_set: Set of tuples {(i, j), ...} identified by findConsensusStructure.\n",
    "\n",
    "# \tReturns:\n",
    "# \t    params: Dictionary {pi, lambda, alpha, beta}\n",
    "# \t\"\"\"\n",
    "# \tN = fused_matrix.shape[0]\n",
    "\n",
    "# \t# 1. Extract Signal Values (The X values for the Consensus Pairs)\n",
    "# \tsignal_values = []\n",
    "# \tfor i, j in p_true_set:\n",
    "# \t\tsignal_values.append(fused_matrix[i, j])\n",
    "# \t\t# Also add symmetric if not present, though p_true usually has sorted tuples\n",
    "# \t\tif (j, i) not in p_true_set:\n",
    "# \t\t\tsignal_values.append(fused_matrix[j, i])\n",
    "\n",
    "# \tsignal_values = np.array(signal_values)\n",
    "\n",
    "# \t# Handle Edge Case: No consensus found\n",
    "# \tif len(signal_values) == 0:\n",
    "# \t\tprint(\"Warning: Jaccard Set is empty. Falling back to Naive Quantile.\")\n",
    "# \t\treturn _initializeParametersViaNaiveQuantile(fused_matrix)\n",
    "\n",
    "# \t# 2. Calculate Signal Parameters (Exponential MLE)\n",
    "# \t# Lambda = 1 / Mean\n",
    "# \t# If the set is perfect duplicates, mean might be 0. Add epsilon.\n",
    "# \tmu_s = np.mean(signal_values)\n",
    "# \tlamb = 1.0 / (mu_s + 1e-6)\n",
    "\n",
    "# \t# 3. Calculate Pi (Prior Probability of Equivalence)\n",
    "# \t# Count of True Pairs / Total Pairs (N*N - N)\n",
    "# \tpi = len(signal_values) / (N * N - N)\n",
    "\n",
    "# \t# 4. Calculate Noise Parameters (Beta Method of Moments on the REST)\n",
    "# \t# We sample the background by excluding the specific signal pairs is expensive,\n",
    "# \t# so we just sample the whole matrix and ignore the tiny fraction of signal.\n",
    "# \t# (The signal is usually < 1% of data, so it won't skew the noise calc much).\n",
    "# \tdata_flat = fused_matrix[~np.eye(N, dtype=bool)]\n",
    "# \tmu_n = np.mean(data_flat)\n",
    "# \tvar_n = np.var(data_flat)\n",
    "\n",
    "# \t# Standard Beta MOM\n",
    "# \tif var_n < mu_n * (1 - mu_n):\n",
    "# \t\tcommon = (mu_n * (1 - mu_n) / (var_n + 1e-9)) - 1\n",
    "# \t\talpha = mu_n * common\n",
    "# \t\tbeta_param = (1 - mu_n) * common\n",
    "# \telse:\n",
    "# \t\talpha, beta_param = 1.0, 1.0\n",
    "\n",
    "# \tparams = {\n",
    "# \t\t\"pi\": pi,\n",
    "# \t\t\"lambda\": lamb,\n",
    "# \t\t\"alpha\": max(alpha, 1.0),\n",
    "# \t\t\"beta\": max(beta_param, 1.0),\n",
    "# \t}\n",
    "\n",
    "# \treturn params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a84381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Preparing Model Artifacts...\n",
      "Processing embedding_vector...\n",
      "Processing retrieval_embedding_vector...\n",
      "\n",
      ">>> Running Probabilistic Pipeline...\n",
      "--- Stage 1: Computing Local Density (k=50) ---\n",
      "Fused Statistics Computed. Shape: (520, 520)\n",
      "--- Stage 2: Initializing via Consensus Heuristic ---\n",
      "Initial Params: {'pi': 0.001, 'lambda': np.float64(49.997500124993756), 'alpha': 1.0, 'beta': 1.0}\n",
      "--- Stage 3: Fitting Mixture Model (EVT) ---\n",
      "Model Converged.\n",
      "--- Stage 4: Applying Transitivity Prior ---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "def extractAndPrintClusters(posterior_matrix, semantic_data, threshold=0.5):\n",
    "\t\"\"\"\n",
    "\tConverts the probabilistic output into discrete clusters and prints them.\n",
    "\n",
    "\tLogic:\n",
    "\t    1. Convert Probability -> Distance (Distance = 1.0 - Probability).\n",
    "\t    2. Apply Agglomerative Clustering with 'complete' linkage.\n",
    "\t       (Complete linkage ensures every member in the group has P > threshold connection to the group diameter).\n",
    "\t\"\"\"\n",
    "\t# Invert Probability to Distance\n",
    "\tdist_matrix = 1.0 - posterior_matrix\n",
    "\t# Clip for numerical safety\n",
    "\tdist_matrix = np.clip(dist_matrix, 0.0, 1.0)\n",
    "\n",
    "\t# Run Clustering\n",
    "\t# distance_threshold = 1 - P_threshold\n",
    "\t# If P > 0.5, then Dist < 0.5.\n",
    "\tmodel = AgglomerativeClustering(\n",
    "\t\tn_clusters=None,\n",
    "\t\tdistance_threshold=(1.0 - threshold),\n",
    "\t\tmetric=\"precomputed\",\n",
    "\t\tlinkage=\"complete\",\n",
    "\t)\n",
    "\n",
    "\tlabels = model.fit_predict(dist_matrix)\n",
    "\n",
    "\t# Organize into Groups\n",
    "\tgroups = {}\n",
    "\tfor idx, label in enumerate(labels):\n",
    "\t\tgroups.setdefault(label, []).append(idx)\n",
    "\n",
    "\t# Filter out Singletons (Noise)\n",
    "\tsignificant_groups = [g for g in groups.values() if len(g) > 1]\n",
    "\n",
    "\t# Sort by group size (descending)\n",
    "\tsignificant_groups.sort(key=len, reverse=True)\n",
    "\n",
    "\t# Print Results\n",
    "\tprint(f\"\\n{'='*80}\")\n",
    "\tprint(f\"PROBABILISTIC CLUSTERING RESULTS (Threshold P > {threshold})\")\n",
    "\tprint(f\"{'='*80}\")\n",
    "\tprint(f\"Found {len(significant_groups)} significant groups.\\n\")\n",
    "\n",
    "\tfor i, indices in enumerate(significant_groups):\n",
    "\t\t# Calculate 'Cluster Cohesion': Average probability of off-diagonal pairs\n",
    "\t\tsub_probs = posterior_matrix[np.ix_(indices, indices)]\n",
    "\t\tmask = ~np.eye(len(indices), dtype=bool)\n",
    "\t\tif len(indices) > 1:\n",
    "\t\t\tavg_prob = np.mean(sub_probs[mask])\n",
    "\t\telse:\n",
    "\t\t\tavg_prob = 1.0\n",
    "\n",
    "\t\tprint(f\"GROUP {i+1} (Size: {len(indices)}) [Cohesion: {avg_prob:.4f}]\")\n",
    "\n",
    "\t\t# Print strings\n",
    "\t\tfor idx in indices:\n",
    "\t\t\tprint(f\" - {semantic_data[idx]}\")\n",
    "\t\tprint(\"-\" * 80)\n",
    "\n",
    "\n",
    "# --- Execution Pipeline ---\n",
    "\n",
    "# 1. Define Model Keys (Assuming data has these keys)\n",
    "model_keys = [\"embedding_vector\", \"retrieval_embedding_vector\"]\n",
    "\n",
    "print(\">>> Preparing Model Artifacts...\")\n",
    "# prepareModelArtifacts is defined in your earlier block\n",
    "artifacts = prepareModelArtifacts(qdata, model_keys, truncation_dim=256, debug=True)\n",
    "\n",
    "# 2. Run the EVT-Topology Pipeline\n",
    "print(\"\\n>>> Running Probabilistic Pipeline...\")\n",
    "pipeline_results = generateEquivalenceProbabilities(\n",
    "\tartifacts,\n",
    "\tk_neighbors=50,\t# Context window for Local Density\n",
    "\tmodeling_method=\"evt\",\t# The Weibull/Beta Mixture\n",
    "\tuse_consensus_init=True,\t# Use consensus to bootstrap parameters\n",
    "\tapply_transitivity=True,\t# Apply graph repair\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5d10c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PROBABILISTIC CLUSTERING RESULTS (Threshold P > 0.5)\n",
      "================================================================================\n",
      "Found 0 significant groups.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Print the Final Clusters\n",
    "# semantic_data is the list of strings extracted in prepareModelArtifacts\n",
    "semantic_data_list = artifacts[model_keys[0]][\"semantic_data\"]\n",
    "\n",
    "extractAndPrintClusters(\n",
    "\tpipeline_results[\"posterior_matrix\"],\n",
    "\tsemantic_data_list,\n",
    "\tthreshold=0.5,\t# Natural probabilistic decision boundary\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
