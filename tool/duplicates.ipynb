{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44b5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Task\n",
    "I would like you to help me fill out relevant factors and metrics we can use in our model.\n",
    "\n",
    "Please determine any other potentially relevant things to consider that may be valuable to our model, some examples are:\n",
    "\n",
    "The initial start that I have determined in our solution space is as following.\n",
    "- Quantify the distance delta between two embedding vectors, using some distance metric D (e.g Mahalanobis, Cosine, L1, L2)\n",
    "- We can determine some useful empirical distributions and utilise (Extreme Value Theory) and other mathematical frameworks, e.g distributions of nearest neighbours.\n",
    "- We can then cluster distances via AgglomerativeClustering\n",
    "- We can threshold distances from Agglomerative Clustering, this can give us some other useful metrics like:\n",
    "\t- **Threshold Minimisations**:\n",
    "\t\t- Maximise the Jaccard Index of pairs between embedding_vector types: `len(set.intersection(*pair_sets))/set.union(*pair_sets)`\n",
    "\t\t- Maximise our Average Consensus-Supporting Group Count (ACGC) i.e the average count of clusters per model that are validated by the intersection of all pair sets.\n",
    "\t- **Threshold Maximisation**:\n",
    "\t\t- Given a metric score find the maximum distance threshold that retains this index, or more specific constraints e.g:\n",
    "\t\tFor `Jaccard Index` we can maximize distance threshold T, subject to the constraint that the set of identified duplicate pairs remains invariant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7734ff5",
   "metadata": {},
   "source": [
    "# Background and Problem Space\n",
    "## Background\n",
    "I am looking to find some probabilistic model to determine likely semantically or functionalally equivalent strings. The strings have a common grammar and structure, additionally assertained from similar texts. \n",
    "\n",
    "For an example that we will use for the remainder of the task our strings are of the form \"Does the privacy policy affirm {X}?\", where `X` is some variable statement. And they're all assertained from different privacy policies about similar products.\n",
    "\n",
    "I have then for each string in our corpus a selection of embedding vectors, each embedding vector is assertained from the same fundamental model, however there are slight differences. Some have been assertained by manipulating the string, e.g via these two example functions (written in python as a demonstration):\n",
    "```\n",
    "def example_question_manipulation_1(question):\n",
    "\tprefix = \"Does the privacy policy affirm\"\n",
    "\treplacement = \"The privacy policy affirms\"\n",
    "\tprocessed_text = (\"The privacy policy affirms\" + question[len(prefix) :])[:-1] + \".\"\n",
    "\n",
    "\treturn processed_text\n",
    "\n",
    "def example_question_manipulation_2(question):\n",
    "\tprefix = \"Does the privacy policy affirm that \"\n",
    "\tprocessed_text = (question[len(prefix) :].capitalize())[:-1] + \".\"\n",
    "\treturn processed_text\n",
    "```\n",
    "And the specific task type e.g \"SEMANTIC_SIMILARITY\", \"FACT_VERIFICATION\" etc, we will use `gemini-embedding-001` as our cannonical embedding model.\n",
    "## Problem Space\n",
    "We want to derive the factors required to construct a probabilistic model $P(\\text{Equivalent} \\mid X)$, where $X$ is a multivariate state including, assume that true semantic equivalents are latent variables that generate the observed embedding vectors with some noise.\":\n",
    "\n",
    "-   The distance vector $\\vec{d}$ (Mahalanobis, Cosine, etc.).\n",
    "-   The embedding source type (e.g., specific manipulation functions).\n",
    "-   Consensus signals (whether the pair is a nearest neighbor across multiple embedding views, whether we have highly overlapping clusters).\n",
    "-   Local topology of clusters and vector spaces\n",
    "-   Similarities in topology of vector spaces, since we expect different embedding types to be correlated since they all represent \"views\" on the same fundamental information.\n",
    "\n",
    "**Constraints & Assumptions:**\n",
    "1. **Zero-Shot / Unsupervised:** We have no labeled 'true semantically equivalents'.\n",
    "2. **No Arbitrary Heuristics:** We reject metrics like \"top 1% nearest neighbors.\" All probabilities must be inferred from the structural properties of the vector space and the consensus between embedding views.\n",
    "3. **Multivariate Dependencies:** The probability is not solely a function of scalar distance. It may be conditioned on the specific embedding model used, the stability of the nearest-neighbor relationship across models, etc.\n",
    "\n",
    "## Tentative Considerations for the Solution Space\n",
    "I have identified the preliminarr empirical signals (metrics). Please analyze how these function as variables in a probabilistic framework (e.g., as priors, likelihood ratios, or density estimation parameters):\n",
    "\n",
    "1.  **Metric: Pairwise Jaccard Index**\n",
    "    The intersection over union of pair sets identified by different embedding models.\n",
    "2.  **Metric: Average Consensus-Supporting Group Count (ACGC):**\n",
    "\tThe average count of clusters per model that are validated by the intersection of all pair sets.\n",
    "3.  **Factor: Empirical distributions of Nearest Neighbours**\n",
    "4.  **Factor:Vector & Distance Deltas**\n",
    "    The raw distance metrics ($D$) and the variance of $D$ across different embedding manipulations for individial vectors and pairs.\n",
    "5.  **Idea: Pairwise filtering:**\n",
    "\tFilter pairs for our metrics based on some conditions, i.e using all pairs in some cluster, only using nearest neighbour pairs. This may help us determine the relationship between equivalents, being a nearest neighbour vs not being a nearest neighbour\n",
    "\n",
    "**Important:**\n",
    "In the following task is about defining the problem space, metrics, distributions and assumptions **NOT** creating the equivalent detection model.\n",
    "1.  **Likelihood Estimation:** \n",
    "    How can we model the conditional distribution `P(distance | Equivalent)` vs `P(distance | Non-Equivalent)` without labels?\n",
    "\n",
    "2.  **The Role of Embedding Views:**\n",
    "    We have multiple embedding vectors for the same string (via manipulation). How do we formally model the correlation of these vectors?\n",
    "    *   If `P(Dup | Model_A)` and `P(Dup | Model_B)` are derived separately, how should they be mathematically utilised.\n",
    "\n",
    "3.  **Local Topology & Stability:**\n",
    "    How do we quantify the probability that a pair is a equivalent given that they are *not* nearest neighbors in one model, but *are* in another?\n",
    "\n",
    "\n",
    "# Expected Output Behaviour:\n",
    "We are not solving the actual problem yet, we are only expanding the factors in our solution space, therefore I expect:\n",
    "- Mainly mathematical derivations and qualitiative discussions. I expect you will only return codeblocks if absolutely necessary to demonstrate some example or test.\n",
    "- If I ask specific questions, your answer should be targetted and precise. Treat each question as self contained, and the answer does not modify the context of our discussion and solution space until I explicitly say we will add it to. No premature implementations/incorperations before we have fully discussed the answer.\n",
    "- Treat this as a Zero-Shot Unsupervised problem. We have no labeled equivalent. The definition of a 'equivalent' must be inferred from the structural properties of the vector space and the consensus between embedding views.\"\n",
    "- We strictly reject the use of arbitrary, exogenous thresholds (e.g., \"fixed distance < 0.5\" or \"top 1% nearest neighbors\") as they fail to capture the underlying uncertainty of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40968081",
   "metadata": {},
   "source": [
    "For a question A to be similar to B:\n",
    "\n",
    "\n",
    "Deny_A $\\sim$ B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471214ad",
   "metadata": {},
   "source": [
    "I am interested in how injecting random noise into a set of embedding vectors, affects my clustering, at some defined distance threshold $\\tau$ which I will define using my Average Group Consensus Metric (ACGC). I first want to determine how my ACGC changes with the addition of noise, and if it doesnt fluctuate much, or in a defined way. I want to see how the clusters of strings behave:\n",
    "\n",
    "It will take several test phases, firstly how random noise affects at a fixed threshold the current Optimal Thresholds which will be provided later, at this stage I also want to record the coincidence or two strings being in some cluster. Then how adding noise affects the threshold devised by our ACGC.\n",
    "\n",
    "My current code for defining and determing our ACGC is below\n",
    "\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.covariance import LedoitWolf\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "def getMahalanobisDistances(vectors_a, vectors_b):\n",
    "\t# Mahalanobis helper kept in snake_case internally\n",
    "\tnorms = np.linalg.norm(vectors_a, axis=1, keepdims=True)\n",
    "\tcleaned_vectors = vectors_a / (norms + 1e-10)\n",
    "\n",
    "\tlw = LedoitWolf()\n",
    "\tlw.fit(cleaned_vectors)\t# assumption vectors_a=vectors_b\n",
    "\tprecision_matrix = lw.precision_\n",
    "\n",
    "\tdist_matrix = cdist(\n",
    "\t\tcleaned_vectors, cleaned_vectors, metric=\"mahalanobis\", VI=precision_matrix\n",
    "\t)\n",
    "\treturn dist_matrix, precision_matrix\n",
    "\n",
    "\n",
    "Distance_Processors = {\n",
    "\t\"cosine\": lambda emb_a, emb_b: 1.0\n",
    "\t- (emb_a @ emb_b.T)\n",
    "\t/ (\n",
    "\t\tnp.linalg.norm(emb_a, axis=1, keepdims=True)\n",
    "\t\t@ np.linalg.norm(emb_b, axis=1, keepdims=True).T\n",
    "\t\t+ 1e-10\n",
    "\t),\n",
    "\t\"l1\": lambda emb_a, emb_b: np.sum(np.abs(emb_a[..., np.newaxis] - emb_b.T), axis=1),\n",
    "\t\"l2\": lambda emb_a, emb_b: np.linalg.norm(emb_a[..., np.newaxis] - emb_b.T, axis=1),\n",
    "\t\"dot\": lambda emb_a, emb_b: emb_a @ emb_b.T,\n",
    "\t\"mahalanobis\": lambda emb_a, emb_b: getMahalanobisDistances(emb_a, emb_b),\n",
    "}\n",
    "\n",
    "\n",
    "def _prepareModelArtifact(\n",
    "\traw_vectors,\n",
    "\tsemantic_data,\n",
    "\ttruncation_dim=256,\n",
    "\tdistance_metric=\"mahalanobis\",\n",
    "\tdebug=True,\n",
    "):\n",
    "\t# 1. Truncation\n",
    "\tdata_matrix = np.array(raw_vectors)\n",
    "\tinput_dim = data_matrix.shape[1]\n",
    "\n",
    "\tif input_dim < truncation_dim and debug:\n",
    "\t\tprint(\n",
    "\t\t\tf\"Warning: Vector dimension ({input_dim}) is smaller than truncation limit ({truncation_dim}). Proceeding without truncation.\"\n",
    "\t\t)\n",
    "\n",
    "\tdata_truncated = data_matrix[:, :truncation_dim]\n",
    "\n",
    "\t# 2. Distance Calculation\n",
    "\tdist_output = Distance_Processors[distance_metric](data_truncated, data_truncated)\n",
    "\n",
    "\tprecision_matrix = None\n",
    "\tif distance_metric == \"mahalanobis\":\n",
    "\t\tdist_matrix, precision_matrix = dist_output\n",
    "\telse:\n",
    "\t\tdist_matrix = dist_output\n",
    "\n",
    "\t# 3. NN Indices (In-place modification to avoid copy overhead)\n",
    "\tnp.fill_diagonal(dist_matrix, float(\"inf\"))\n",
    "\tnn_indices = np.argmin(dist_matrix, axis=1)\n",
    "\tnp.fill_diagonal(dist_matrix, 0.0)\n",
    "\n",
    "\treturn {\n",
    "\t\t\"dist_matrix\": dist_matrix,\n",
    "\t\t\"vectors\": data_truncated,\n",
    "\t\t\"precision\": precision_matrix,\n",
    "\t\t\"semantic_data\": semantic_data,\n",
    "\t\t\"metric\": distance_metric,\n",
    "\t\t\"nn_indices\": nn_indices,\n",
    "\t}\n",
    "\n",
    "\n",
    "def prepareModelArtifacts(\n",
    "\tdata_set, vector_keys, truncation_dim=256, distance_metric=\"mahalanobis\", debug=True\n",
    "):\n",
    "\tsemantic_data = list(data_set.keys())\n",
    "\tmodel_artifacts = {}\n",
    "\traw_vectors = {}\n",
    "\tfor key in vector_keys:\n",
    "\t\traw_vectors[key] = [data_set[s][key] for s in semantic_data]\n",
    "\texecutor = concurrent.futures.ThreadPoolExecutor(max_workers=len(vector_keys))\n",
    "\n",
    "\tfutures = dict()\n",
    "\tfor key in vector_keys:\n",
    "\t\tif debug:\n",
    "\t\t\tprint(f\"Processing {key}...\")\n",
    "\n",
    "\t\tfutures[key] = executor.submit(\n",
    "\t\t\t_prepareModelArtifact,\n",
    "\t\t\traw_vectors[key],\n",
    "\t\t\tsemantic_data,\n",
    "\t\t\ttruncation_dim,\n",
    "\t\t\tdistance_metric,\n",
    "\t\t\tdebug,\n",
    "\t\t)\n",
    "\texecutor.shutdown(wait=True)\n",
    "\tfor key in vector_keys:\n",
    "\t\tmodel_artifacts[key] = futures[key].result()\n",
    "\treturn model_artifacts\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# --- Clustering Utilities ---\n",
    "\n",
    "\n",
    "def getGroupsFromLabels(labels):\n",
    "\tgroups = {}\n",
    "\tfor idx, label in enumerate(labels):\n",
    "\t\tgroups.setdefault(label, []).append(idx)\n",
    "\treturn [g for g in groups.values() if len(g) > 1]\n",
    "\n",
    "\n",
    "def getNNPairsFromGroups(groups, nn_indices):\n",
    "\tpairs = set()\n",
    "\tfor group in groups:\n",
    "\t\tif len(group) < 2:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tgroup_set = set(group)\n",
    "\t\tfor idx in group:\n",
    "\t\t\tnn_idx = nn_indices[idx]\n",
    "\t\t\tif nn_idx in group_set:\n",
    "\t\t\t\tpairs.add(tuple(sorted((idx, nn_idx))))\n",
    "\treturn pairs\n",
    "\n",
    "\n",
    "def clusterAndGetArtifacts(dist_matrix, threshold):\n",
    "\tmodel = AgglomerativeClustering(\n",
    "\t\tn_clusters=None,\n",
    "\t\tdistance_threshold=threshold,\n",
    "\t\tmetric=\"precomputed\",\n",
    "\t\tlinkage=\"complete\",\n",
    "\t)\n",
    "\tlabels = model.fit_predict(dist_matrix)\n",
    "\treturn getGroupsFromLabels(labels), labels\n",
    "\n",
    "\n",
    "def calculateNTrue(labels_array, target_pairs):\n",
    "\tif not target_pairs or labels_array is None:\n",
    "\t\treturn 0\n",
    "\n",
    "\tinvolved_indices = {idx for pair in target_pairs for idx in pair}\n",
    "\n",
    "\tif not involved_indices:\n",
    "\t\treturn 0\n",
    "\n",
    "\treturn len({labels_array[idx] for idx in involved_indices})\n",
    "\n",
    "\n",
    "# --- Cache & Optimization ---\n",
    "\n",
    "\n",
    "def createClusteringCache(\n",
    "\tmodel_artifacts, tau_range, optimization_mode=\"Jaccard\", debug=True\n",
    "):\n",
    "\tcache = {name: {} for name in model_artifacts.keys()}\n",
    "\tlast_states = {name: {\"groups\": [], \"pairs\": set()} for name in model_artifacts.keys()}\n",
    "\n",
    "\tif debug:\n",
    "\t\tprint(\n",
    "\t\t\tf\"Building Clustering Cache ({len(tau_range)} steps) [Mode: {optimization_mode}]...\"\n",
    "\t\t)\n",
    "\n",
    "\tfor t in tau_range:\n",
    "\t\tfor name, artifact in model_artifacts.items():\n",
    "\n",
    "\t\t\tgroups, labels = clusterAndGetArtifacts(artifact[\"dist_matrix\"], t)\n",
    "\t\t\tnn_pairs = getNNPairsFromGroups(groups, artifact[\"nn_indices\"])\n",
    "\n",
    "\t\t\tprev_state = last_states[name]\n",
    "\n",
    "\t\t\tgroups_changed = (len(groups) != len(prev_state[\"groups\"])) or (\n",
    "\t\t\t\tgroups != prev_state[\"groups\"]\n",
    "\t\t\t)\n",
    "\t\t\tpairs_changed = (len(nn_pairs) != len(prev_state[\"pairs\"])) or (\n",
    "\t\t\t\tnn_pairs != prev_state[\"pairs\"]\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# Jaccard only cares if pairs change. ACGC cares if groups OR pairs change.\n",
    "\t\t\tsignificant_change = pairs_changed\n",
    "\t\t\tif optimization_mode == \"ACGC\":\n",
    "\t\t\t\tsignificant_change = significant_change or groups_changed\n",
    "\n",
    "\t\t\tif significant_change:\n",
    "\t\t\t\tcache[name][t] = [groups, labels, nn_pairs]\n",
    "\t\t\t\tlast_states[name][\"groups\"] = groups\n",
    "\t\t\t\tlast_states[name][\"pairs\"] = nn_pairs\n",
    "\n",
    "\tif debug:\n",
    "\t\tfor name, data in cache.items():\n",
    "\t\t\tprint(f\" - {name}: Pruned {len(tau_range)} -> {len(data)} significant states.\")\n",
    "\n",
    "\treturn cache\n",
    "\n",
    "\n",
    "def findConsensusStructure(clustering_cache, model_keys, metric=\"Jaccard\"):\n",
    "\tthreshold_axes = [list(clustering_cache[m].keys()) for m in model_keys]\n",
    "\n",
    "\tbest_score = -1\n",
    "\tbest_state = {\n",
    "\t\t\"score\": -1,\n",
    "\t\t\"P_true\": set(),\n",
    "\t\t\"optimal_taus\": {},\n",
    "\t\t\"N_target\": 0,\n",
    "\t\t\"AVG_groups\": 0,\t# New field\n",
    "\t}\n",
    "\n",
    "\tfor thresholds in product(*threshold_axes):\n",
    "\n",
    "\t\tcurrent_config = dict(zip(model_keys, thresholds))\n",
    "\n",
    "\t\tpair_sets = []\n",
    "\t\t# Calculate average total groups found in this config\n",
    "\t\ttotal_groups_found = 0\n",
    "\n",
    "\t\tfor m, t in current_config.items():\n",
    "\t\t\tpair_sets.append(clustering_cache[m][t][2])\n",
    "\t\t\ttotal_groups_found += len(clustering_cache[m][t][0])\t# Index 0 is 'groups'\n",
    "\n",
    "\t\tcurrent_avg_groups = total_groups_found / len(model_keys)\n",
    "\n",
    "\t\tp_true = set.intersection(*pair_sets)\n",
    "\n",
    "\t\tif not p_true:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tcurrent_score = 0\n",
    "\n",
    "\t\tif metric == \"Jaccard\":\n",
    "\t\t\tp_union = set.union(*pair_sets)\n",
    "\t\t\tif len(p_union) > 0:\n",
    "\t\t\t\tcurrent_score = len(p_true) / len(p_union)\n",
    "\n",
    "\t\telif metric == \"ACGC\":\n",
    "\t\t\tn_true_sum = 0\n",
    "\t\t\tfor m, t in current_config.items():\n",
    "\t\t\t\tlabels = clustering_cache[m][t][1]\n",
    "\t\t\t\tn_true_sum += calculateNTrue(labels, p_true)\n",
    "\t\t\tcurrent_score = n_true_sum / len(model_keys)\n",
    "\n",
    "\t\tif current_score > best_score:\n",
    "\t\t\tbest_score = current_score\n",
    "\n",
    "\t\t\tbest_state[\"score\"] = best_score\n",
    "\t\t\tbest_state[\"P_true\"] = p_true\n",
    "\t\t\tbest_state[\"optimal_taus\"] = current_config\n",
    "\t\t\tbest_state[\"AVG_groups\"] = current_avg_groups\n",
    "\n",
    "\t\t\tif metric == \"ACGC\":\n",
    "\t\t\t\tbest_state[\"N_target\"] = best_score\n",
    "\t\t\telse:\n",
    "\t\t\t\t# For Jaccard, calculate N_target post-hoc for reference\n",
    "\t\t\t\tn_sum_ref = 0\n",
    "\t\t\t\tfor m, t in current_config.items():\n",
    "\t\t\t\t\tlabels = clustering_cache[m][t][1]\n",
    "\t\t\t\t\tn_sum_ref += calculateNTrue(labels, p_true)\n",
    "\t\t\t\tbest_state[\"N_target\"] = n_sum_ref / len(model_keys)\n",
    "\n",
    "\treturn best_state\n",
    "\n",
    "\n",
    "# Keys corresponding to data dictionary\n",
    "model_keys = [\"embedding_vector\", \"retrieval_embedding_vector\"]\n",
    "metric = \"Jaccard\"\n",
    "artifacts = prepareModelArtifacts(qdata, model_keys)\n",
    "\n",
    "max_dist = max(np.max(a[\"dist_matrix\"]) for a in artifacts.values())\n",
    "tau_range = np.arange(0.1, max_dist, 0.02)\n",
    "\n",
    "cache = createClusteringCache(artifacts, tau_range, optimization_mode=\"ACGC\")\n",
    "\n",
    "consensus_ACGC = findConsensusStructure(cache, model_keys, metric=\"ACGC\")\n",
    "\n",
    "print(f\"\\n--- Optimization Results (ACGC) ---\")\n",
    "print(f\"Best Score: {consensus_ACGC['score']:.5f}\")\n",
    "print(f\"Platinum Pairs Identified: {len(consensus_ACGC['P_true'])}\")\n",
    "print(f\"Target Group Count (Inferred): {consensus_ACGC['N_target']:.1f}\")\n",
    "\n",
    "# 5. Visualize Results\n",
    "optimal_taus = consensus_ACGC[\"optimal_taus\"]\n",
    "\n",
    "for key in model_keys:\n",
    "\tbest_t = optimal_taus[key]\n",
    "\n",
    "\t# Retrieve cached state: [groups, labels, pairs]\n",
    "\tbest_groups = cache[key][best_t][0]\n",
    "\n",
    "\tprintClusterGroups(\n",
    "\t\tbest_groups, artifacts[key], f\"{key} (Optimal t={best_t:.2f})\", sort_order=\"ascending\"\n",
    "\t)\n",
    "```\n",
    "\n",
    "After determining this I would like to also quantify how it behaves with pairs defined by combinations rather than NN:\n",
    "```\n",
    "def getPairsFromLablesCombinations(labels):\n",
    "\t\"\"\"\n",
    "\tConverts cluster labels into a Set of unique pairs (indices).\n",
    "\tReturns: set of tuples {(min_id, max_id), ...}\n",
    "\t\"\"\"\n",
    "\tdf = pd.DataFrame({\"label\": labels, \"id\": range(len(labels))})\n",
    "\tpairs = set()\n",
    "\n",
    "\t# Group by label\n",
    "\tfor label, group in df.groupby(\"label\"):\n",
    "\t\tindices = group[\"id\"].tolist()\n",
    "\t\tif len(indices) > 1:\n",
    "\t\t\t# Generate all unique pairs in this cluster\n",
    "\t\t\tfor p in combinations(sorted(indices), 2):\n",
    "\t\t\t\tpairs.add(p)\n",
    "\treturn pairs\n",
    "```\n",
    "\n",
    "I am interested in this in the study of Suprathreshold Stochastic Resonance, but first I do not know what scale we should use for the noise. \n",
    "\n",
    "We"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75dd5660",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "QUESTIONS_FILE = \"./data/questions_filter_after.json\"\n",
    "POLICIES_FILE = \"./data/policies_testing.json\"\n",
    "OUTPUT_Q_FILE = \"./output_q.json\"\n",
    "OUTPUT_P_FILE = \"./output_p.json\"\n",
    "\n",
    "\n",
    "def _loadJson(filepath):\n",
    "\tif not os.path.exists(filepath):\n",
    "\t\tprint(f\"Warning: File not found: {filepath}\")\n",
    "\t\treturn {}\n",
    "\ttry:\n",
    "\t\twith open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "\t\t\treturn json.load(f)\n",
    "\texcept json.JSONDecodeError:\n",
    "\t\tprint(f\"Error decoding JSON: {filepath}\")\n",
    "\t\treturn {}\n",
    "\n",
    "\n",
    "qdata = _loadJson(QUESTIONS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad13d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cd8f27e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# executor = concurrent.futures.ThreadPoolExecutor(max_workers=num_policy_chunks)\n",
    "# e_dict = dict()\n",
    "# for index, chunk in enumerate(policy_chunks):\n",
    "# \te_dict[index] = executor.submit(\n",
    "# \t\tself.processPolicyChunks, chunk, policy_hash, policy_name, index\n",
    "# \t)\n",
    "# executor.shutdown(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bc74ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4738fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.covariance import LedoitWolf\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "# abstracted\n",
    "def getMahalanobisDistances(vectors_a, vectors_b):\n",
    "\t# Mahalanobis helper kept in snake_case internally\n",
    "\tnorms = np.linalg.norm(vectors_a, axis=1, keepdims=True)\n",
    "\tcleaned_vectors = vectors_a / (norms + 1e-10)\n",
    "\n",
    "\tlw = LedoitWolf()\n",
    "\tlw.fit(cleaned_vectors)\t# assumption vectors_a=vectors_b\n",
    "\tprecision_matrix = lw.precision_\n",
    "\n",
    "\tdist_matrix = cdist(\n",
    "\t\tcleaned_vectors, cleaned_vectors, metric=\"mahalanobis\", VI=precision_matrix\n",
    "\t)\n",
    "\treturn dist_matrix, precision_matrix\n",
    "\n",
    "\n",
    "Distance_Processors = {\n",
    "\t\"cosine\": lambda emb_a, emb_b: 1.0\n",
    "\t- (emb_a @ emb_b.T)\n",
    "\t/ (\n",
    "\t\tnp.linalg.norm(emb_a, axis=1, keepdims=True)\n",
    "\t\t@ np.linalg.norm(emb_b, axis=1, keepdims=True).T\n",
    "\t\t+ 1e-10\n",
    "\t),\n",
    "\t\"l1\": lambda emb_a, emb_b: np.sum(np.abs(emb_a[..., np.newaxis] - emb_b.T), axis=1),\n",
    "\t\"l2\": lambda emb_a, emb_b: np.linalg.norm(emb_a[..., np.newaxis] - emb_b.T, axis=1),\n",
    "\t\"dot\": lambda emb_a, emb_b: emb_a @ emb_b.T,\n",
    "\t\"mahalanobis\": lambda emb_a, emb_b: getMahalanobisDistances(emb_a, emb_b),\n",
    "}\n",
    "\n",
    "\n",
    "def _prepareModelArtifact(\n",
    "\traw_vectors,\n",
    "\tsemantic_data,\n",
    "\ttruncation_dim=256,\n",
    "\tdistance_metric=\"mahalanobis\",\n",
    "\tdebug=True,\n",
    "):\n",
    "\t# 1. Truncation\n",
    "\tdata_matrix = np.array(raw_vectors)\n",
    "\tinput_dim = data_matrix.shape[1]\n",
    "\n",
    "\tif input_dim < truncation_dim and debug:\n",
    "\t\tprint(\n",
    "\t\t\tf\"Warning: Vector dimension ({input_dim}) is smaller than truncation limit ({truncation_dim}). Proceeding without truncation.\"\n",
    "\t\t)\n",
    "\n",
    "\tdata_truncated = data_matrix[:, :truncation_dim]\n",
    "\n",
    "\t# 2. Distance Calculation\n",
    "\tdist_output = Distance_Processors[distance_metric](data_truncated, data_truncated)\n",
    "\n",
    "\tprecision_matrix = None\n",
    "\tif distance_metric == \"mahalanobis\":\n",
    "\t\tdist_matrix, precision_matrix = dist_output\n",
    "\telse:\n",
    "\t\tdist_matrix = dist_output\n",
    "\n",
    "\t# 3. NN Indices (In-place modification to avoid copy overhead)\n",
    "\tnp.fill_diagonal(dist_matrix, float(\"inf\"))\n",
    "\tnn_indices = np.argmin(dist_matrix, axis=1)\n",
    "\tnp.fill_diagonal(dist_matrix, 0.0)\n",
    "\n",
    "\treturn {\n",
    "\t\t\"dist_matrix\": dist_matrix,\n",
    "\t\t\"vectors\": data_truncated,\n",
    "\t\t\"precision\": precision_matrix,\n",
    "\t\t\"semantic_data\": semantic_data,\n",
    "\t\t\"metric\": distance_metric,\n",
    "\t\t\"nn_indices\": nn_indices,\n",
    "\t}\n",
    "\n",
    "\n",
    "def prepareModelArtifacts(\n",
    "\tdata_set, vector_keys, truncation_dim=256, distance_metric=\"mahalanobis\", debug=True\n",
    "):\n",
    "\tsemantic_data = list(data_set.keys())\n",
    "\tmodel_artifacts = {}\n",
    "\traw_vectors = {}\n",
    "\tfor key in vector_keys:\n",
    "\t\traw_vectors[key] = [data_set[s][key] for s in semantic_data]\n",
    "\texecutor = concurrent.futures.ThreadPoolExecutor(max_workers=len(vector_keys))\n",
    "\n",
    "\tfutures = dict()\n",
    "\tfor key in vector_keys:\n",
    "\t\tif debug:\n",
    "\t\t\tprint(f\"Processing {key}...\")\n",
    "\n",
    "\t\tfutures[key] = executor.submit(\n",
    "\t\t\t_prepareModelArtifact,\n",
    "\t\t\traw_vectors[key],\n",
    "\t\t\tsemantic_data,\n",
    "\t\t\ttruncation_dim,\n",
    "\t\t\tdistance_metric,\n",
    "\t\t\tdebug,\n",
    "\t\t)\n",
    "\texecutor.shutdown(wait=True)\n",
    "\tfor key in vector_keys:\n",
    "\t\tmodel_artifacts[key] = futures[key].result()\n",
    "\treturn model_artifacts\n",
    "\n",
    "\n",
    "# def prepareModelArtifacts(\n",
    "# \tdata_set, vector_keys, truncation_dim=256, distance_metric=\"mahalanobis\", debug=True\n",
    "# ):\n",
    "# \t# I only use docstrings for non-trivial information\n",
    "\n",
    "# \tsemantic_data = list(data_set.keys())\n",
    "# \tmodel_artifacts = {}\n",
    "\n",
    "\n",
    "# \tfor key in vector_keys:\n",
    "# \t\tif debug:\n",
    "# \t\t\tprint(f\"Processing {key}...\")\n",
    "\n",
    "# \t\traw_vectors = [data_set[s][key] for s in semantic_data]\n",
    "# \t\tprecision_matrix = None\n",
    "# \t\tinput_dim = len(raw_vectors[0])\n",
    "\n",
    "# \t\tif input_dim < truncation_dim and debug:\n",
    "# \t\t\tif debug:\n",
    "# \t\t\t\tprint(\n",
    "# \t\t\t\t\tf\"Warning: Vector dimension ({input_dim}) is smaller than truncation limit ({truncation_dim}). Proceeding without truncation.\"\n",
    "# \t\t\t\t)\n",
    "# \t\t\t# current_truncation = input_dim\t# was unecessary\n",
    "\n",
    "# \t\t# i only use explicit variable names if that particular information us used more than once\n",
    "# \t\tdata_truncated = np.array(raw_vectors)[:, :truncation_dim]\n",
    "\n",
    "# \t\tdist_output = Distance_Processors[distance_metric](\n",
    "# \t\t\tdata_truncated, data_truncated\n",
    "# \t\t)\t# assume any normalisation shall be done by our defined distance_metric\n",
    "\n",
    "# \t\tprecision_matrix = None\n",
    "# \t\tif distance_metric == \"mahalanobis\":\n",
    "# \t\t\tdist_matrix, precision_matrix = dist_output\n",
    "# \t\telse:\n",
    "# \t\t\tdist_matrix = dist_output\n",
    "# \t\ttemp_dist = dist_matrix.copy()\n",
    "# \t\tnp.fill_diagonal(temp_dist, float(\"inf\"))\n",
    "# \t\tnn_indices = np.argmin(temp_dist, axis=1)\n",
    "# \t\t# ---------------------------------------------\n",
    "\n",
    "# \t\tmodel_artifacts[key] = {\n",
    "# \t\t\t\"dist_matrix\": dist_matrix,\n",
    "# \t\t\t\"vectors\": data_truncated,\n",
    "# \t\t\t\"precision\": precision_matrix,\n",
    "# \t\t\t\"semantic_data\": semantic_data,\n",
    "# \t\t\t\"metric\": distance_metric,\n",
    "# \t\t\t\"nn_indices\": nn_indices,\t# Stored here\n",
    "# \t\t}\n",
    "\n",
    "\n",
    "# \treturn model_artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cee79346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateGroupDiameter(dist_matrix, indices):\n",
    "\t\"\"\"\n",
    "\tCalculates the diameter of a cluster based on the maximum pairwise distance\n",
    "\tbetween its members. This corresponds to 'complete' linkage logic.\n",
    "\t\"\"\"\n",
    "\tif len(indices) < 2:\n",
    "\t\treturn 0.0\n",
    "\n",
    "\tsub_matrix = dist_matrix[np.ix_(indices, indices)]\n",
    "\n",
    "\treturn np.max(sub_matrix)\n",
    "\n",
    "\n",
    "def printClusterGroups(groups, model_artifact, title, sort_order=\"ascending\"):\n",
    "\tprint(f\"\\n{'='*80}\")\n",
    "\tprint(f\"GROUPINGS: {title}\")\n",
    "\tprint(f\"{'='*80}\")\n",
    "\n",
    "\tif not groups:\n",
    "\t\tprint(\"No groupings found.\")\n",
    "\t\treturn\n",
    "\n",
    "\tsemantic_data = model_artifact[\"semantic_data\"]\n",
    "\tdist_matrix = model_artifact[\"dist_matrix\"]\n",
    "\n",
    "\tprocessed_groups = []\n",
    "\n",
    "\tfor indices in groups:\n",
    "\t\tdiameter = calculateGroupDiameter(dist_matrix, indices)\n",
    "\n",
    "\t\tmembers = [semantic_data[i] for i in indices]\n",
    "\n",
    "\t\tprocessed_groups.append(\n",
    "\t\t\t{\"diameter\": diameter, \"members\": members, \"size\": len(indices)}\n",
    "\t\t)\n",
    "\n",
    "\tprocessed_groups.sort(\n",
    "\t\tkey=lambda x: x[\"diameter\"], reverse=(sort_order == \"descending\")\n",
    "\t)\n",
    "\n",
    "\tprint(f\"Found {len(processed_groups)} significant groups.\\n\")\n",
    "\n",
    "\tfor i, g in enumerate(processed_groups):\n",
    "\t\tprint(f\"GROUP {i+1} (Size: {g['size']}) [Diameter: {g['diameter']:.4f}]\")\n",
    "\t\tfor s in g[\"members\"]:\n",
    "\t\t\tprint(f\" - {s}\")\n",
    "\t\tprint(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "28404702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateGroupDiameter(dist_matrix, indices):\n",
    "\t\"\"\"\n",
    "\tCalculates the diameter of a cluster based on the maximum pairwise distance\n",
    "\tbetween its members. This corresponds to 'complete' linkage logic.\n",
    "\t\"\"\"\n",
    "\tif len(indices) < 2:\n",
    "\t\treturn 0.0\n",
    "\n",
    "\tsub_matrix = dist_matrix[np.ix_(indices, indices)]\n",
    "\n",
    "\treturn np.max(sub_matrix)\n",
    "\n",
    "\n",
    "def printClusterGroups(groups, model_artifact, title, sort_order=\"ascending\"):\n",
    "\tprint(f\"\\n{'='*80}\")\n",
    "\tprint(f\"GROUPINGS: {title}\")\n",
    "\tprint(f\"{'='*80}\")\n",
    "\n",
    "\tif not groups:\n",
    "\t\tprint(\"No groupings found.\")\n",
    "\t\treturn\n",
    "\n",
    "\tsemantic_data = model_artifact[\"semantic_data\"]\n",
    "\tdist_matrix = model_artifact[\"dist_matrix\"]\n",
    "\n",
    "\tprocessed_groups = []\n",
    "\n",
    "\tfor indices in groups:\n",
    "\t\tdiameter = calculateGroupDiameter(dist_matrix, indices)\n",
    "\n",
    "\t\tmembers = [semantic_data[i] for i in indices]\n",
    "\n",
    "\t\tprocessed_groups.append(\n",
    "\t\t\t{\"diameter\": diameter, \"members\": members, \"size\": len(indices)}\n",
    "\t\t)\n",
    "\n",
    "\tprocessed_groups.sort(\n",
    "\t\tkey=lambda x: x[\"diameter\"], reverse=(sort_order == \"descending\")\n",
    "\t)\n",
    "\n",
    "\tprint(f\"Found {len(processed_groups)} significant groups.\\n\")\n",
    "\n",
    "\tfor i, g in enumerate(processed_groups):\n",
    "\t\tprint(f\"GROUP {i+1} (Size: {g['size']}) [Diameter: {g['diameter']:.4f}]\")\n",
    "\t\tfor s in g[\"members\"]:\n",
    "\t\t\tprint(f\" - {s}\")\n",
    "\t\tprint(\"-\" * 80)\n",
    "\n",
    "\n",
    "def getGroupsFromLabels(labels):\n",
    "\tgroups = {}\n",
    "\tfor idx, label in enumerate(labels):\n",
    "\t\tgroups.setdefault(label, []).append(idx)\n",
    "\treturn [g for g in groups.values() if len(g) > 1]\n",
    "\n",
    "\n",
    "def getNNPairsFromGroups(groups, nn_indices):\n",
    "\tpairs = set()\n",
    "\tfor group in groups:\n",
    "\t\tif len(group) < 2:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tgroup_set = set(group)\n",
    "\t\tfor idx in group:\n",
    "\t\t\tnn_idx = nn_indices[idx]\n",
    "\t\t\tif nn_idx in group_set:\n",
    "\t\t\t\tpairs.add(tuple(sorted((idx, nn_idx))))\n",
    "\treturn pairs\n",
    "\n",
    "\n",
    "def clusterAndGetArtifacts(dist_matrix, threshold):\n",
    "\tmodel = AgglomerativeClustering(\n",
    "\t\tn_clusters=None,\n",
    "\t\tdistance_threshold=threshold,\n",
    "\t\tmetric=\"precomputed\",\n",
    "\t\tlinkage=\"complete\",\n",
    "\t)\n",
    "\tlabels = model.fit_predict(dist_matrix)\n",
    "\treturn getGroupsFromLabels(labels), labels\n",
    "\n",
    "\n",
    "def createClusteringCache(model_artifacts, tau_range, debug=True):\n",
    "\tcache = {name: {} for name in model_artifacts.keys()}\n",
    "\tlast_states = {name: {\"groups\": [], \"pairs\": set()} for name in model_artifacts.keys()}\n",
    "\n",
    "\tif debug:\n",
    "\t\tprint(f\"Building Clustering Cache ({len(tau_range)} steps)...\")\n",
    "\n",
    "\tfor t in tau_range:\n",
    "\t\tfor name, artifact in model_artifacts.items():\n",
    "\n",
    "\t\t\tgroups, labels = clusterAndGetArtifacts(artifact[\"dist_matrix\"], t)\n",
    "\t\t\tnn_pairs = getNNPairsFromGroups(groups, artifact[\"nn_indices\"])\n",
    "\n",
    "\t\t\tprev_state = last_states[name]\n",
    "\n",
    "\t\t\tgroups_changed = (len(groups) != len(prev_state[\"groups\"])) or (\n",
    "\t\t\t\tgroups != prev_state[\"groups\"]\n",
    "\t\t\t)\n",
    "\t\t\tpairs_changed = (len(nn_pairs) != len(prev_state[\"pairs\"])) or (\n",
    "\t\t\t\tnn_pairs != prev_state[\"pairs\"]\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\tif groups_changed or pairs_changed:\n",
    "\t\t\t\tcache[name][t] = [groups, labels, nn_pairs]\n",
    "\t\t\t\tlast_states[name][\"groups\"] = groups\n",
    "\t\t\t\tlast_states[name][\"pairs\"] = nn_pairs\n",
    "\n",
    "\tif debug:\n",
    "\t\tfor name, data in cache.items():\n",
    "\t\t\tprint(f\" - {name}: Pruned {len(tau_range)} -> {len(data)} significant states.\")\n",
    "\n",
    "\treturn cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c7f644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# --- Clustering Utilities ---\n",
    "\n",
    "\n",
    "def getGroupsFromLabels(labels):\n",
    "\tgroups = {}\n",
    "\tfor idx, label in enumerate(labels):\n",
    "\t\tgroups.setdefault(label, []).append(idx)\n",
    "\treturn [g for g in groups.values() if len(g) > 1]\n",
    "\n",
    "\n",
    "def getNNPairsFromGroups(groups, nn_indices):\n",
    "\tpairs = set()\n",
    "\tfor group in groups:\n",
    "\t\tif len(group) < 2:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tgroup_set = set(group)\n",
    "\t\tfor idx in group:\n",
    "\t\t\tnn_idx = nn_indices[idx]\n",
    "\t\t\tif nn_idx in group_set:\n",
    "\t\t\t\tpairs.add(tuple(sorted((idx, nn_idx))))\n",
    "\treturn pairs\n",
    "\n",
    "\n",
    "def clusterAndGetArtifacts(dist_matrix, threshold):\n",
    "\tmodel = AgglomerativeClustering(\n",
    "\t\tn_clusters=None,\n",
    "\t\tdistance_threshold=threshold,\n",
    "\t\tmetric=\"precomputed\",\n",
    "\t\tlinkage=\"complete\",\n",
    "\t)\n",
    "\tlabels = model.fit_predict(dist_matrix)\n",
    "\treturn getGroupsFromLabels(labels), labels\n",
    "\n",
    "\n",
    "def calculateNTrue(labels_array, target_pairs):\n",
    "\tif not target_pairs or labels_array is None:\n",
    "\t\treturn 0\n",
    "\n",
    "\tinvolved_indices = {idx for pair in target_pairs for idx in pair}\n",
    "\n",
    "\tif not involved_indices:\n",
    "\t\treturn 0\n",
    "\n",
    "\treturn len({labels_array[idx] for idx in involved_indices})\n",
    "\n",
    "\n",
    "# --- Cache & Optimization ---\n",
    "\n",
    "\n",
    "def createClusteringCache(\n",
    "\tmodel_artifacts, tau_range, optimization_mode=\"Jaccard\", debug=True\n",
    "):\n",
    "\tcache = {name: {} for name in model_artifacts.keys()}\n",
    "\tlast_states = {name: {\"groups\": [], \"pairs\": set()} for name in model_artifacts.keys()}\n",
    "\n",
    "\tif debug:\n",
    "\t\tprint(\n",
    "\t\t\tf\"Building Clustering Cache ({len(tau_range)} steps) [Mode: {optimization_mode}]...\"\n",
    "\t\t)\n",
    "\n",
    "\tfor t in tau_range:\n",
    "\t\tfor name, artifact in model_artifacts.items():\n",
    "\n",
    "\t\t\tgroups, labels = clusterAndGetArtifacts(artifact[\"dist_matrix\"], t)\n",
    "\t\t\tnn_pairs = getNNPairsFromGroups(groups, artifact[\"nn_indices\"])\n",
    "\n",
    "\t\t\tprev_state = last_states[name]\n",
    "\n",
    "\t\t\tgroups_changed = (len(groups) != len(prev_state[\"groups\"])) or (\n",
    "\t\t\t\tgroups != prev_state[\"groups\"]\n",
    "\t\t\t)\n",
    "\t\t\tpairs_changed = (len(nn_pairs) != len(prev_state[\"pairs\"])) or (\n",
    "\t\t\t\tnn_pairs != prev_state[\"pairs\"]\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# Jaccard only cares if pairs change. ACGC cares if groups OR pairs change.\n",
    "\t\t\tsignificant_change = pairs_changed\n",
    "\t\t\tif optimization_mode == \"ACGC\":\n",
    "\t\t\t\tsignificant_change = significant_change or groups_changed\n",
    "\n",
    "\t\t\tif significant_change:\n",
    "\t\t\t\tcache[name][t] = [groups, labels, nn_pairs]\n",
    "\t\t\t\tlast_states[name][\"groups\"] = groups\n",
    "\t\t\t\tlast_states[name][\"pairs\"] = nn_pairs\n",
    "\n",
    "\tif debug:\n",
    "\t\tfor name, data in cache.items():\n",
    "\t\t\tprint(f\" - {name}: Pruned {len(tau_range)} -> {len(data)} significant states.\")\n",
    "\n",
    "\treturn cache\n",
    "\n",
    "\n",
    "def findConsensusStructure(clustering_cache, model_keys, metric=\"Jaccard\"):\n",
    "\tthreshold_axes = [list(clustering_cache[m].keys()) for m in model_keys]\n",
    "\n",
    "\tbest_score = -1\n",
    "\tbest_state = {\n",
    "\t\t\"score\": -1,\n",
    "\t\t\"P_true\": set(),\n",
    "\t\t\"optimal_taus\": {},\n",
    "\t\t\"N_target\": 0,\n",
    "\t\t\"AVG_groups\": 0,\t# New field\n",
    "\t}\n",
    "\n",
    "\tfor thresholds in product(*threshold_axes):\n",
    "\n",
    "\t\tcurrent_config = dict(zip(model_keys, thresholds))\n",
    "\n",
    "\t\tpair_sets = []\n",
    "\t\t# Calculate average total groups found in this config\n",
    "\t\ttotal_groups_found = 0\n",
    "\n",
    "\t\tfor m, t in current_config.items():\n",
    "\t\t\tpair_sets.append(clustering_cache[m][t][2])\n",
    "\t\t\ttotal_groups_found += len(clustering_cache[m][t][0])\t# Index 0 is 'groups'\n",
    "\n",
    "\t\tcurrent_avg_groups = total_groups_found / len(model_keys)\n",
    "\n",
    "\t\tp_true = set.intersection(*pair_sets)\n",
    "\n",
    "\t\tif not p_true:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tcurrent_score = 0\n",
    "\n",
    "\t\tif metric == \"Jaccard\":\n",
    "\t\t\tp_union = set.union(*pair_sets)\n",
    "\t\t\tif len(p_union) > 0:\n",
    "\t\t\t\tcurrent_score = len(p_true) / len(p_union)\n",
    "\n",
    "\t\telif metric == \"ACGC\":\n",
    "\t\t\tn_true_sum = 0\n",
    "\t\t\tfor m, t in current_config.items():\n",
    "\t\t\t\tlabels = clustering_cache[m][t][1]\n",
    "\t\t\t\tn_true_sum += calculateNTrue(labels, p_true)\n",
    "\t\t\tcurrent_score = n_true_sum / len(model_keys)\n",
    "\n",
    "\t\tif current_score > best_score:\n",
    "\t\t\tbest_score = current_score\n",
    "\n",
    "\t\t\tbest_state[\"score\"] = best_score\n",
    "\t\t\tbest_state[\"P_true\"] = p_true\n",
    "\t\t\tbest_state[\"optimal_taus\"] = current_config\n",
    "\t\t\tbest_state[\"AVG_groups\"] = current_avg_groups\n",
    "\n",
    "\t\t\tif metric == \"ACGC\":\n",
    "\t\t\t\tbest_state[\"N_target\"] = best_score\n",
    "\t\t\telse:\n",
    "\t\t\t\t# For Jaccard, calculate N_target post-hoc for reference\n",
    "\t\t\t\tn_sum_ref = 0\n",
    "\t\t\t\tfor m, t in current_config.items():\n",
    "\t\t\t\t\tlabels = clustering_cache[m][t][1]\n",
    "\t\t\t\t\tn_sum_ref += calculateNTrue(labels, p_true)\n",
    "\t\t\t\tbest_state[\"N_target\"] = n_sum_ref / len(model_keys)\n",
    "\n",
    "\treturn best_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9340a3e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36693b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing embedding_vector...\n",
      "Processing retrieval_embedding_vector...\n",
      "Building Clustering Cache (1263 steps) [Mode: ACGC]...\n",
      " - embedding_vector: Pruned 1263 -> 360 significant states.\n",
      " - retrieval_embedding_vector: Pruned 1263 -> 364 significant states.\n"
     ]
    }
   ],
   "source": [
    "# Keys corresponding to data dictionary\n",
    "model_keys = [\"embedding_vector\", \"retrieval_embedding_vector\"]\n",
    "metric = \"Jaccard\"\n",
    "artifacts = prepareModelArtifacts(qdata, model_keys)\n",
    "\n",
    "max_dist = max(np.max(a[\"dist_matrix\"]) for a in artifacts.values())\n",
    "tau_range = np.arange(0.1, max_dist, 0.02)\n",
    "\n",
    "cache = createClusteringCache(artifacts, tau_range, optimization_mode=\"ACGC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1fd8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Optimization Results (Jaccard) ---\n",
      "Best Score: 1.00000\n",
      "Platinum Pairs Identified: 5\n",
      "Target Group Count (Inferred): 5.0\n",
      "9.220000000000002\n",
      "8.800000000000002\n"
     ]
    }
   ],
   "source": [
    "consensus_JAC = findConsensusStructure(cache, model_keys, metric=\"Jaccard\")\n",
    "\n",
    "\n",
    "print(f\"\\n--- Optimization Results (Jaccard) ---\")\n",
    "print(f\"Best Score: {consensus_JAC['score']:.5f}\")\n",
    "print(f\"Platinum Pairs Identified: {len(consensus_JAC['P_true'])}\")\n",
    "print(f\"Target Group Count (Inferred): {consensus_JAC['N_target']:.1f}\")\n",
    "\n",
    "# 5. Visualize Results\n",
    "optimal_taus = consensus_JAC[\"optimal_taus\"]\n",
    "\n",
    "for key in model_keys:\n",
    "\tbest_t = optimal_taus[key]\n",
    "\n",
    "\t# Retrieve cached state: [groups, labels, pairs]\n",
    "\tbest_groups = cache[key][best_t][0]\n",
    "\t# print(best_t)\n",
    "\tprintClusterGroups(\n",
    "\t\tbest_groups, artifacts[key], f\"{key} (Optimal t={best_t:.2f})\", sort_order=\"ascending\"\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958153ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_ACGC = findConsensusStructure(cache, model_keys, metric=\"ACGC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dfb1ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Optimization Results (ACGC) ---\n",
      "Best Score: 104.00000\n",
      "Platinum Pairs Identified: 128\n",
      "Target Group Count (Inferred): 104.0\n",
      "17.040000000000003\n",
      "17.080000000000002\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n--- Optimization Results (ACGC) ---\")\n",
    "print(f\"Best Score: {consensus_ACGC['score']:.5f}\")\n",
    "print(f\"Platinum Pairs Identified: {len(consensus_ACGC['P_true'])}\")\n",
    "print(f\"Target Group Count (Inferred): {consensus_ACGC['N_target']}\")\n",
    "\n",
    "# 5. Visualize Results\n",
    "optimal_taus = consensus_ACGC[\"optimal_taus\"]\n",
    "\n",
    "for key in model_keys:\n",
    "\tbest_t = optimal_taus[key]\n",
    "\n",
    "\t# Retrieve cached state: [groups, labels, pairs]\n",
    "\tbest_groups = cache[key][best_t][0]\n",
    "\tprint(best_t)\n",
    "\n",
    "\t# printClusterGroups(\n",
    "\t# \tbest_groups, artifacts[key], f\"{key} (Optimal t={best_t})\", sort_order=\"ascending\"\n",
    "\t# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "377333b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 104.0,\n",
       " 'P_true': {(3, np.int64(188)),\n",
       "  (4, np.int64(5)),\n",
       "  (8, np.int64(9)),\n",
       "  (11, np.int64(20)),\n",
       "  (15, np.int64(16)),\n",
       "  (17, np.int64(19)),\n",
       "  (18, np.int64(19)),\n",
       "  (21, np.int64(22)),\n",
       "  (np.int64(23), 24),\n",
       "  (23, np.int64(25)),\n",
       "  (np.int64(28), 29),\n",
       "  (28, np.int64(30)),\n",
       "  (34, np.int64(172)),\n",
       "  (np.int64(35), 37),\n",
       "  (36, np.int64(318)),\n",
       "  (48, np.int64(49)),\n",
       "  (51, np.int64(84)),\n",
       "  (52, np.int64(85)),\n",
       "  (53, np.int64(54)),\n",
       "  (56, np.int64(58)),\n",
       "  (np.int64(65), 66),\n",
       "  (67, np.int64(68)),\n",
       "  (70, np.int64(136)),\n",
       "  (72, np.int64(73)),\n",
       "  (78, np.int64(275)),\n",
       "  (87, np.int64(88)),\n",
       "  (89, np.int64(92)),\n",
       "  (93, np.int64(94)),\n",
       "  (96, np.int64(97)),\n",
       "  (99, np.int64(100)),\n",
       "  (103, np.int64(105)),\n",
       "  (104, np.int64(105)),\n",
       "  (np.int64(106), 108),\n",
       "  (np.int64(111), 116),\n",
       "  (114, np.int64(468)),\n",
       "  (115, np.int64(469)),\n",
       "  (np.int64(117), 316),\n",
       "  (121, np.int64(194)),\n",
       "  (122, np.int64(195)),\n",
       "  (np.int64(122), 197),\n",
       "  (124, np.int64(183)),\n",
       "  (127, np.int64(130)),\n",
       "  (128, np.int64(129)),\n",
       "  (np.int64(131), 132),\n",
       "  (134, np.int64(135)),\n",
       "  (137, np.int64(505)),\n",
       "  (139, np.int64(190)),\n",
       "  (140, np.int64(142)),\n",
       "  (142, np.int64(143)),\n",
       "  (147, np.int64(148)),\n",
       "  (158, np.int64(159)),\n",
       "  (161, np.int64(424)),\n",
       "  (163, np.int64(164)),\n",
       "  (165, np.int64(166)),\n",
       "  (np.int64(169), 170),\n",
       "  (173, np.int64(174)),\n",
       "  (176, np.int64(384)),\n",
       "  (179, np.int64(181)),\n",
       "  (181, np.int64(182)),\n",
       "  (np.int64(185), 492),\n",
       "  (185, np.int64(493)),\n",
       "  (np.int64(187), 238),\n",
       "  (np.int64(189), 501),\n",
       "  (191, np.int64(192)),\n",
       "  (np.int64(191), 193),\n",
       "  (np.int64(200), 510),\n",
       "  (np.int64(202), 203),\n",
       "  (220, np.int64(289)),\n",
       "  (223, np.int64(390)),\n",
       "  (236, np.int64(240)),\n",
       "  (241, np.int64(243)),\n",
       "  (244, np.int64(245)),\n",
       "  (246, np.int64(247)),\n",
       "  (251, np.int64(252)),\n",
       "  (np.int64(251), 254),\n",
       "  (256, np.int64(346)),\n",
       "  (257, np.int64(347)),\n",
       "  (259, np.int64(341)),\n",
       "  (260, np.int64(341)),\n",
       "  (269, np.int64(338)),\n",
       "  (271, np.int64(272)),\n",
       "  (np.int64(280), 292),\n",
       "  (287, np.int64(288)),\n",
       "  (np.int64(293), 436),\n",
       "  (300, np.int64(435)),\n",
       "  (302, np.int64(442)),\n",
       "  (304, np.int64(428)),\n",
       "  (305, np.int64(433)),\n",
       "  (306, np.int64(447)),\n",
       "  (307, np.int64(429)),\n",
       "  (308, np.int64(438)),\n",
       "  (320, np.int64(377)),\n",
       "  (327, np.int64(330)),\n",
       "  (332, np.int64(368)),\n",
       "  (334, np.int64(335)),\n",
       "  (337, np.int64(363)),\n",
       "  (343, np.int64(344)),\n",
       "  (349, np.int64(350)),\n",
       "  (np.int64(350), 376),\n",
       "  (358, np.int64(511)),\n",
       "  (360, np.int64(361)),\n",
       "  (np.int64(363), 364),\n",
       "  (365, np.int64(367)),\n",
       "  (370, np.int64(371)),\n",
       "  (372, np.int64(373)),\n",
       "  (378, np.int64(379)),\n",
       "  (388, np.int64(395)),\n",
       "  (394, np.int64(395)),\n",
       "  (np.int64(394), 423),\n",
       "  (396, np.int64(398)),\n",
       "  (401, np.int64(402)),\n",
       "  (406, np.int64(407)),\n",
       "  (408, np.int64(411)),\n",
       "  (np.int64(411), 412),\n",
       "  (418, np.int64(419)),\n",
       "  (421, np.int64(422)),\n",
       "  (450, np.int64(451)),\n",
       "  (np.int64(450), 453),\n",
       "  (461, np.int64(462)),\n",
       "  (np.int64(462), 463),\n",
       "  (471, np.int64(495)),\n",
       "  (472, np.int64(496)),\n",
       "  (480, np.int64(509)),\n",
       "  (488, np.int64(489)),\n",
       "  (490, np.int64(491)),\n",
       "  (504, np.int64(505)),\n",
       "  (506, np.int64(507)),\n",
       "  (np.int64(516), 517)},\n",
       " 'optimal_taus': {'embedding_vector': np.float64(17.040000000000003),\n",
       "  'retrieval_embedding_vector': np.float64(17.080000000000002)},\n",
       " 'N_target': 104.0}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consensus_ACGC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c819c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_jac = consensus_JAC[\"N_target\"]\n",
    "target_acgc = consensus_ACGC[\"N_target\"]\n",
    "average_jac = consensus_JAC[\"AVG_groups\"]\n",
    "average_acgc = consensus_ACGC[\"AVG_groups\"]\n",
    "\n",
    "p_duplicates = (target_jac / (target_acgc)) * (average_acgc / target_acgc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ae80e557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3365384615384615"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_acgc / target_acgc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "db9ff135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06425665680473373"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a1070f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Calculating Empirical Probabilities (N-Model Logic) ---\n",
      "[embedding_vector] Processed. (Example: Dist=14.5735 -> P=0.59501)\n",
      "[retrieval_embedding_vector] Processed. (Example: Dist=14.6406 -> P=0.47601)\n",
      "\n",
      "================================================================================\n",
      "STAGE 2 OUTPUT: Probabilistic Outlier Model (P < 0.064257)\n",
      "================================================================================\n",
      "Found 21 significant groups.\n",
      "\n",
      "GROUP 1 (Size: 2) [Likelihood Diameter: 0.001919]\n",
      " - Does the privacy policy affirm that the company collects the dates and times of access?\n",
      " - Does the privacy policy affirm that the company collects the dates and times of access to the services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 2 (Size: 2) [Likelihood Diameter: 0.001919]\n",
      " - Does the privacy policy affirm that users have the right to request the correction of inaccurate personal data?\n",
      " - Does the privacy policy affirm that users have the right to request the correction of inaccurate data?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 3 (Size: 2) [Likelihood Diameter: 0.005758]\n",
      " - Does the privacy policy affirm that providing personal data constitutes agreement to the transfer of data outside of Canada?\n",
      " - Does the privacy policy affirm that providing personal data constitutes agreement to the disclosure of data outside of Canada?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 4 (Size: 2) [Likelihood Diameter: 0.009597]\n",
      " - Does the privacy policy affirm that the company relies on user consent to process contact information for specific marketing communications?\n",
      " - Does the privacy policy affirm that the company processes contact information for marketing communications based on user consent?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 5 (Size: 2) [Likelihood Diameter: 0.013436]\n",
      " - Does the privacy policy affirm that the company relies on Standard Contractual Clauses (SCCs) for transfers to countries without an adequacy decision?\n",
      " - Does the privacy policy affirm that the company relies on Standard Contractual Clauses (SCCs) for transfers to jurisdictions without adequacy decisions?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 6 (Size: 2) [Likelihood Diameter: 0.021113]\n",
      " - Does the privacy policy affirm that Personal Data is used to protect the rights, privacy, safety, or property of users?\n",
      " - Does the privacy policy affirm that Personal Data is used to protect the rights, privacy, safety, or property of the company?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 7 (Size: 2) [Likelihood Diameter: 0.024952]\n",
      " - Does the privacy policy affirm that Personal Data is used to improve the company's services?\n",
      " - Does the privacy policy affirm that Personal Data is used to develop the company's services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 8 (Size: 2) [Likelihood Diameter: 0.028791]\n",
      " - Does the privacy policy affirm that Personal Data is retained to comply with legal obligations?\n",
      " - Does the privacy policy affirm that Personal Data is used to comply with legal obligations?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 9 (Size: 2) [Likelihood Diameter: 0.028791]\n",
      " - Does the privacy policy affirm that personal data may be shared with service providers for data processing purposes?\n",
      " - Does the privacy policy affirm that personal data may be shared with service providers for the purpose of providing services to the user?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 10 (Size: 2) [Likelihood Diameter: 0.032630]\n",
      " - Does the privacy policy affirm that processing contact information to send technical announcements is based on the necessity to perform a contract?\n",
      " - Does the privacy policy affirm that the company processes contact information to send technical announcements based on the necessity to perform a contract?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 11 (Size: 2) [Likelihood Diameter: 0.036468]\n",
      " - Does the privacy policy affirm that users have the statutory right to access their Personal Data?\n",
      " - Does the privacy policy affirm that users have the statutory right to access information relating to how their Personal Data is processed?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 12 (Size: 2) [Likelihood Diameter: 0.036468]\n",
      " - Does the privacy policy affirm that the company collects the user's time zone?\n",
      " - Does the privacy policy affirm that the company collects time zone settings?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 13 (Size: 2) [Likelihood Diameter: 0.044146]\n",
      " - Does the privacy policy affirm that Inputs and Outputs disassociated via Feedback are used for training models?\n",
      " - Does the privacy policy affirm that Inputs and Outputs disassociated via Feedback are used for improving models?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 14 (Size: 2) [Likelihood Diameter: 0.047985]\n",
      " - Does the privacy policy affirm that the company does not knowingly disclose information from children under the age of 18?\n",
      " - Does the privacy policy affirm that the company does not knowingly share information from children under the age of 18?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 15 (Size: 2) [Likelihood Diameter: 0.051823]\n",
      " - Does the privacy policy affirm that the company trains its models using data provided by users?\n",
      " - Does the privacy policy affirm that personal data is used to train the company's models?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 16 (Size: 2) [Likelihood Diameter: 0.053743]\n",
      " - Does the privacy policy affirm that the company implements commercially reasonable technical measures to protect Personal Data?\n",
      " - Does the privacy policy affirm that the company implements commercially reasonable organizational measures to protect Personal Data?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 17 (Size: 2) [Likelihood Diameter: 0.055662]\n",
      " - Does the privacy policy affirm that users have the right to withdraw consent where processing is based on consent?\n",
      " - Does the privacy policy affirm that users have the right to withdraw their consent?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 18 (Size: 2) [Likelihood Diameter: 0.057582]\n",
      " - Does the privacy policy affirm that user email addresses are entrusted to the domestic representative?\n",
      " - Does the privacy policy affirm that user addresses are entrusted to the domestic representative?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 19 (Size: 3) [Likelihood Diameter: 0.057582]\n",
      " - Does the privacy policy affirm that the company may disclose personal data to governmental regulatory authorities as required by law?\n",
      " - Does the privacy policy affirm that the company may disclose personal data in response to requests from governmental regulatory authorities?\n",
      " - Does the privacy policy affirm that the company may disclose personal data to assist in investigations by governmental regulatory authorities?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 20 (Size: 2) [Likelihood Diameter: 0.061420]\n",
      " - Does the privacy policy affirm that security measures are designed to protect personal data from unauthorized access?\n",
      " - Does the privacy policy affirm that security measures are designed to protect personal data from unauthorized disclosure?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 21 (Size: 2) [Likelihood Diameter: 0.061420]\n",
      " - Does the privacy policy affirm that the Data Protection Officer can be contacted via email regarding matters related to Personal Data processing?\n",
      " - Does the privacy policy affirm that users can contact the company's Data Protection Officer via email?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "def getNearestNeighborDistances(dist_matrix):\n",
    "\td = dist_matrix.copy()\n",
    "\tnp.fill_diagonal(d, float(\"inf\"))\n",
    "\tmin_dists = np.min(d, axis=1)\n",
    "\treturn min_dists\n",
    "\n",
    "\n",
    "def convertDistToProb(dist_matrix, reference_dist_array):\n",
    "\tsorted_refs = np.sort(reference_dist_array)\n",
    "\tn = len(sorted_refs)\n",
    "\tranks = np.searchsorted(sorted_refs, dist_matrix)\n",
    "\tprobs = (ranks + 1) / (n + 1)\n",
    "\treturn probs\n",
    "\n",
    "\n",
    "def getMaxPairwiseScore(matrix, indices):\n",
    "\tif len(indices) < 2:\n",
    "\t\treturn 0.0\n",
    "\tsub = matrix[np.ix_(indices, indices)]\n",
    "\treturn np.max(sub)\n",
    "\n",
    "\n",
    "print(\"--- Calculating Empirical Probabilities (N-Model Logic) ---\")\n",
    "\n",
    "prob_matrices = []\n",
    "\n",
    "for key in model_keys:\n",
    "\tdist_mat = artifacts[key][\"dist_matrix\"]\n",
    "\n",
    "\tnn_dists = getNearestNeighborDistances(dist_mat)\n",
    "\n",
    "\tprob_mat = convertDistToProb(dist_mat, nn_dists)\n",
    "\tprob_matrices.append(prob_mat)\n",
    "\n",
    "\tprint(f\"[{key}] Processed. (Example: Dist={nn_dists[0]:.4f} -> P={prob_mat[0,1]:.5f})\")\n",
    "\n",
    "Prob_Fused = np.minimum.reduce(prob_matrices)\n",
    "\n",
    "PROB_THRESHOLD = p_duplicates\t# ratio used here\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STAGE 2 OUTPUT: Probabilistic Outlier Model (P < {PROB_THRESHOLD:.6f})\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "cluster_model = AgglomerativeClustering(\n",
    "\tn_clusters=None,\n",
    "\tdistance_threshold=PROB_THRESHOLD,\n",
    "\tmetric=\"precomputed\",\n",
    "\tlinkage=\"complete\",\n",
    ")\n",
    "\n",
    "labels = cluster_model.fit_predict(Prob_Fused)\n",
    "\n",
    "groups = {}\n",
    "for idx, label in enumerate(labels):\n",
    "\tgroups.setdefault(label, []).append(idx)\n",
    "\n",
    "raw_groups = [g for g in groups.values() if len(g) > 1]\n",
    "\n",
    "# --- Sorting Logic ---\n",
    "sorted_groups = []\n",
    "for indices in raw_groups:\n",
    "\t# Calculate score first to allow sorting\n",
    "\tscore = getMaxPairwiseScore(Prob_Fused, indices)\n",
    "\tsorted_groups.append({\"indices\": indices, \"score\": score})\n",
    "\n",
    "# Sort Ascending: Lower score = Lower Rank = Tighter Probability\n",
    "sorted_groups.sort(key=lambda x: x[\"score\"])\n",
    "\n",
    "print(f\"Found {len(sorted_groups)} significant groups.\\n\")\n",
    "\n",
    "semantic_data = artifacts[model_keys[0]][\"semantic_data\"]\n",
    "\n",
    "for i, g in enumerate(sorted_groups):\n",
    "\tindices = g[\"indices\"]\n",
    "\tlikelihood_diameter = g[\"score\"]\n",
    "\n",
    "\tprint(\n",
    "\t\tf\"GROUP {i+1} (Size: {len(indices)}) [Likelihood Diameter: {likelihood_diameter:.6f}]\"\n",
    "\t)\n",
    "\n",
    "\tcluster_strs = [semantic_data[idx] for idx in indices]\n",
    "\tfor s in cluster_strs:\n",
    "\t\tprint(f\" - {s}\")\n",
    "\tprint(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
