{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3498411",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20cb54ad",
   "metadata": {},
   "source": [
    "# Background\n",
    "I am writing some model to predict likely \"semantic\" duplicate groups amongs some very grammatically similar strings, about the same topic.\n",
    "We have a collection of different embedding vectors in some data structure `qdata` which is organised in the following form:\n",
    "```\n",
    "{\n",
    "[semantic_data:string]:{[embedding_name:list[float]]}\n",
    "}\n",
    "```\n",
    "\n",
    "We want to quantify where we think we have semantic duplicates, however trivially we do not know any distributions about the embedding space(s). i.e we do not know an expected distance for a duplicate a priori. (also i will want to generalise it to test different distance metrics)\n",
    "\n",
    "We want to quantify the some probability of seeing a duplicate, given a distance. We can refine this model by inferring some distributions about our data.\n",
    "\n",
    "We want to end up with some statistic p(duplicate|distance_metric|neighbours|embedding_model|all_embedding_models).\n",
    "The reason we can make an inference of `embedding_model|all_embedding_models` is because we are using the same fundamental model to produce our embeddings, \"gemini-embedding-001\", however we produce the embeddings from some master string $S$, by doing different preprocessing steps $f_i(S)$ along with specifying different particular task types $g_i(S)$. Which means our different embeddings are really all different perspectives on the same underlying data ($\\text{embedding}_i = \\text{EmbeddingModel}(g_i(f_i(S)))$).\n",
    "\n",
    "# Ground Truth Estimation\n",
    "We first need to quantify some ground truth about our data. We will produce some \"extremely\" likely pairings that are agreed amongst all embeddings. We treat this as some estimate of our number of pairings/duplicate groups [1].\n",
    "\n",
    "# Ground Probability Estimation\n",
    "We then want to produce some estimate about the number of these groups, given our nearest neighbours. Tentitively we will use the average over the number of groupings vs the largest distance that keeps these grous coherent [2]. We will treat this as the expected probability of a duplicate group given distances given all_embedding_models.\n",
    "\n",
    "# Empirical Distribution\n",
    "Next we want to compute some distribution about the expected distances of nearest neighbours for a particular model. Then use some measure of agreement to infer what it says about all_embedding_models given a particular distance.\n",
    "\n",
    "Now we can compare our estimate of where we expect duplicates using our \"Ground Probability Estimation\", vs the \"Empirical Distribution\", and we will just for now say that we will treat all distances that give some probability less than our \"Ground Probability Estimation\" as our duplicate groups.\n",
    "\n",
    "\n",
    "- 1. This may need refining since this is almost certainly correlated to the number of semantic groups but isnt exact. I also don't know if it is an over or under estimate. The group sizing may be smaller, but perhaps we would produce more distinct groups. I have a primitive normalisation of Jaccard Metric = Intersection(pairings)/Union(pairings) to try to rectify this.\n",
    "\n",
    "- 2. No clue how to justify this, ideally we could use the law of large numbers, however we only have a small number of distinct embedding types, however a large number of individual embeddings, at a high dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624ad9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "QUESTIONS_FILE = \"./data/questions_filter_after.json\"\n",
    "POLICIES_FILE = \"./data/policies_testing.json\"\n",
    "OUTPUT_Q_FILE = \"./output_q.json\"\n",
    "OUTPUT_P_FILE = \"./output_p.json\"\n",
    "\n",
    "\n",
    "def _loadJson(filepath):\n",
    "\tif not os.path.exists(filepath):\n",
    "\t\tprint(f\"Warning: File not found: {filepath}\")\n",
    "\t\treturn {}\n",
    "\ttry:\n",
    "\t\twith open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "\t\t\treturn json.load(f)\n",
    "\texcept json.JSONDecodeError:\n",
    "\t\tprint(f\"Error decoding JSON: {filepath}\")\n",
    "\t\treturn {}\n",
    "\n",
    "\n",
    "qdata = _loadJson(QUESTIONS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e1fc3",
   "metadata": {},
   "source": [
    "I understand that ACGC produces non monotonically decreasing scores, but that is independent of pairing. The intersections area already \"defined\" and same with the unions etc and groups membership.\n",
    "\n",
    "What if we start at the smallest tau, and record when we  have group merges, and with who given tau. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c21e448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Model A (Statement)...\n",
      "Processing Model B (Question)...\n",
      "\n",
      "--- Phase 1: Grid Search for Consensus (Tau) ---\n",
      "Search Range Defined: [5.77 to 25.35] (Step: 0.05)\n",
      "Pre-computing clusters for 392 thresholds...\n",
      "min dist\n",
      "1.0012390916358527e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from scipy.spatial.distance import cdist, mahalanobis\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def prepare_model_artifacts(raw_vector_list, name=\"Model\"):\n",
    "\t\"\"\"\n",
    "\tReturns dictionary containing:\n",
    "\t- 'dist_matrix': N x N pairwise distances\n",
    "\t- 'vectors': N x 256 normalized vectors\n",
    "\t- 'precision': 256 x 256 inverse covariance matrix\n",
    "\t\"\"\"\n",
    "\tprint(f\"Processing {name}...\")\n",
    "\n",
    "\tdata = np.array(raw_vector_list)\n",
    "\tdata_trunc = data[:, :256]\n",
    "\tnorms = np.linalg.norm(data_trunc, axis=1, keepdims=True)\n",
    "\tcleaned_vectors = data_trunc / (norms + 1e-10)\n",
    "\n",
    "\tlw = LedoitWolf()\n",
    "\tlw.fit(cleaned_vectors)\n",
    "\tprecision_matrix = lw.precision_\n",
    "\n",
    "\tdist_matrix = cdist(\n",
    "\t\tcleaned_vectors, cleaned_vectors, metric=\"mahalanobis\", VI=precision_matrix\n",
    "\t)\n",
    "\n",
    "\treturn {\n",
    "\t\t\"dist_matrix\": dist_matrix,\n",
    "\t\t\"vectors\": cleaned_vectors,\n",
    "\t\t\"precision\": precision_matrix,\n",
    "\t}\n",
    "\n",
    "\n",
    "data_set = qdata\n",
    "strings = list(data_set.keys())\n",
    "\n",
    "raw_vectors_A = [data_set[s][\"embedding_vector\"] for s in strings]\n",
    "raw_vectors_B = [data_set[s][\"retrieval_embedding_vector\"] for s in strings]\n",
    "\n",
    "\n",
    "Model_A = prepare_model_artifacts(raw_vectors_A, \"Model A (Statement)\")\n",
    "Model_B = prepare_model_artifacts(raw_vectors_B, \"Model B (Question)\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from scipy.spatial.distance import cdist, mahalanobis\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# We dont need to do this, surely we can assume if we have some cluster and a increase some radius and only 1 goes in it is invalid as a pair\n",
    "# therefore only merges matter\n",
    "# It assumes that then all pairs must be the nearest neighbour\n",
    "# This is a valid assumption since we are varying radius, so we end up pruning non-near neighbours out at tau_i - epsilon\n",
    "# So we arent considering the merges with trivial groups of length 1 because the models treat the smaller cluster as more related\n",
    "\n",
    "\n",
    "# def get_clustering_pairs_and_labels(dist_matrix, threshold):\n",
    "# \t\"\"\"Runs clustering and returns both the pair set and the labels array.\"\"\"\n",
    "# \tmodel = AgglomerativeClustering(\n",
    "# \t\tn_clusters=None,\n",
    "# \t\tdistance_threshold=threshold,\n",
    "# \t\tmetric=\"precomputed\",\n",
    "# \t\tlinkage=\"complete\",\n",
    "# \t)\n",
    "# \tlabels = model.fit_predict(dist_matrix)\n",
    "# \treturn get_pairs_from_labels(labels), labels\n",
    "\n",
    "\n",
    "# def get_pairs_from_labels(labels):\n",
    "# \t\"\"\"Converts cluster labels into a Set of unique pairs (indices).\"\"\"\n",
    "# \tdf = pd.DataFrame({\"label\": labels, \"id\": range(len(labels))})\n",
    "# \tpairs = set()\n",
    "# \tfor label, group in df.groupby(\"label\"):\n",
    "# \t\tindices = group[\"id\"].tolist()\n",
    "# \t\tif len(indices) > 1:\n",
    "# \t\t\tfor p in combinations(sorted(indices), 2):\n",
    "# \t\t\t\tpairs.add(p)\n",
    "# \treturn pairs\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "# def get_nn_pairs_from_labels_and_dist(labels, dist_matrix):\n",
    "# \tnp.fill_diagonal(dist_matrix, float(\"inf\"))\n",
    "# \tnearest_neighbors = np.argmin(dist_matrix, axis=1)\n",
    "\n",
    "# \tpairs = set()\n",
    "\n",
    "# \tgroups = {}\n",
    "# \tfor idx, label in enumerate(labels):\n",
    "# \t\tgroups.setdefault(label, []).append(idx)\n",
    "\n",
    "# \tfor indices in groups.values():\n",
    "# \t\tif len(indices) > 1:\n",
    "# \t\t\tindices_set = set(indices)\n",
    "# \t\t\tfor idx in indices:\n",
    "# \t\t\t\tnn_idx = nearest_neighbors[idx]\n",
    "# \t\t\t\tif nn_idx in indices_set:\n",
    "# \t\t\t\t\tpairs.add(tuple(sorted((idx, nn_idx))))\n",
    "\n",
    "\n",
    "# \treturn pairs\n",
    "def get_nn_pairs_from_labels_and_dist(labels, dist_matrix):\n",
    "\tnp.fill_diagonal(dist_matrix, float(\"inf\"))\n",
    "\n",
    "\t# 2. Find Nearest Neighbors\n",
    "\tnearest_neighbors = np.argmin(dist_matrix, axis=1)\n",
    "\n",
    "\tnp.fill_diagonal(dist_matrix, 0.0)\n",
    "\n",
    "\tpairs = set()\n",
    "\n",
    "\tgroups = {}\n",
    "\tfor idx, label in enumerate(labels):\n",
    "\t\tgroups.setdefault(label, []).append(idx)\n",
    "\n",
    "\tfor indices in groups.values():\n",
    "\t\tif len(indices) > 1:\n",
    "\t\t\tindices_set = set(indices)\n",
    "\t\t\tfor idx in indices:\n",
    "\t\t\t\tnn_idx = nearest_neighbors[idx]\n",
    "\t\t\t\tif nn_idx in indices_set:\n",
    "\t\t\t\t\tpairs.add(tuple(sorted((idx, nn_idx))))\n",
    "\n",
    "\treturn pairs\n",
    "\n",
    "\n",
    "def get_clustering_pairs_and_labels(dist_matrix, threshold):\n",
    "\tmodel = AgglomerativeClustering(\n",
    "\t\tn_clusters=None,\n",
    "\t\tdistance_threshold=threshold,\n",
    "\t\tmetric=\"precomputed\",\n",
    "\t\tlinkage=\"complete\",\n",
    "\t)\n",
    "\tlabels = model.fit_predict(dist_matrix)\n",
    "\tfiltered_pairs = get_nn_pairs_from_labels_and_dist(labels, dist_matrix)\n",
    "\treturn filtered_pairs, labels\n",
    "\n",
    "\n",
    "def calculate_n_true(labels_array, target_pairs):\n",
    "\t\"\"\"Calculates the number of unique clusters containing the elements of target_pairs.\"\"\"\n",
    "\tif not target_pairs or labels_array is None:\n",
    "\t\treturn 0\n",
    "\n",
    "\tinvolved_indices = set(idx for pair in target_pairs for idx in pair)\n",
    "\n",
    "\tlabels_of_interest = set(\n",
    "\t\tlabels_array[idx] for idx in involved_indices if idx < len(labels_array)\n",
    "\t)\n",
    "\n",
    "\tif not involved_indices:\n",
    "\t\treturn 0\n",
    "\n",
    "\treturn len(labels_of_interest)\n",
    "\tunion = pairs_A.union(pairs_B)\n",
    "\n",
    "\n",
    "print(\"\\n--- Phase 1: Grid Search for Consensus (Tau) ---\")\n",
    "\n",
    "\n",
    "dist_A_off = Model_A[\"dist_matrix\"][np.triu_indices_from(Model_A[\"dist_matrix\"], k=1)]\n",
    "dist_B_off = Model_B[\"dist_matrix\"][np.triu_indices_from(Model_B[\"dist_matrix\"], k=1)]\n",
    "all_dists = np.concatenate([dist_A_off, dist_B_off])\n",
    "\n",
    "\n",
    "D_min_min = np.min(all_dists)\n",
    "D_max_max = np.max(all_dists)\n",
    "\n",
    "\n",
    "SEARCH_STEP = 0.05\t# converges to same groups here 0.1 was too high\n",
    "\n",
    "\n",
    "TAU_SEARCH_START = max(D_min_min, 0.0)\n",
    "TAU_SEARCH_END = D_max_max\n",
    "\n",
    "tau_range = np.arange(TAU_SEARCH_START, TAU_SEARCH_END, SEARCH_STEP)\n",
    "t_range = np.arange(TAU_SEARCH_START, TAU_SEARCH_END, SEARCH_STEP)\n",
    "\n",
    "print(\n",
    "\tf\"Search Range Defined: [{TAU_SEARCH_START:.2f} to {TAU_SEARCH_END:.2f}] (Step: {SEARCH_STEP})\"\n",
    ")\n",
    "best_jaccard = -1\n",
    "P_true = set()\n",
    "labels_A_star = None\n",
    "labels_B_star = None\n",
    "\n",
    "\n",
    "cache_A = {}\n",
    "cache_B = {}\n",
    "\n",
    "print(f\"Pre-computing clusters for {len(tau_range)} thresholds...\")\n",
    "\n",
    "# for t in tau_range:\n",
    "# \tcache_A[t] = get_clustering_pairs_and_labels(Model_A[\"dist_matrix\"], t)\n",
    "# \tcache_B[t] = get_clustering_pairs_and_labels(Model_B[\"dist_matrix\"], t)\n",
    "TAU_A = 0\n",
    "TAU_B = 0\n",
    "\n",
    "best_metric_score = -1\n",
    "\n",
    "# for t_A in tau_range:\n",
    "# \tpairs_A, labels_A = cache_A[t_A]\n",
    "# \tfor t_B in tau_range:\n",
    "# \t\tpairs_B, labels_B = cache_B[t_B]\n",
    "\n",
    "# \t\tintersection = pairs_A.intersection(pairs_B)\n",
    "\n",
    "# \t\tif len(intersection) > 0:\n",
    "\n",
    "# \t\t\tN_A = calculate_n_true(labels_A, intersection)\n",
    "\n",
    "# \t\t\tN_B = calculate_n_true(labels_B, intersection)\n",
    "\n",
    "# \t\t\t# N_T = #calculate_n_true(labels_B, labels_B)\n",
    "\n",
    "# \t\t\tavg_consensus_groups = (N_A + N_B) / 2\n",
    "\n",
    "# \t\t\tif avg_consensus_groups > best_metric_score:\n",
    "# \t\t\t\tbest_metric_score = avg_consensus_groups\n",
    "\n",
    "# \t\t\t\t# Update Best State\n",
    "# \t\t\t\tP_true = intersection\n",
    "# \t\t\t\tlabels_A_star = labels_A\n",
    "# \t\t\t\tlabels_B_star = labels_B\n",
    "# \t\t\t\tTAU_A = t_A\n",
    "# \t\t\t\tTAU_B = t_B\n",
    "\n",
    "\n",
    "def get_minimum_distance_delta(dist_matrix):\n",
    "\t# 1. Extract upper triangle (unique pairwise distances), exclude diagonal 0s\n",
    "\t# k=1 drops the main diagonal\n",
    "\tdists = dist_matrix[np.triu_indices_from(dist_matrix, k=1)]\n",
    "\n",
    "\t# 2. Sort and find unique values\n",
    "\t# This automatically sorts them\n",
    "\tunique_dists = np.unique(dists)\n",
    "\n",
    "\t# 3. Calculate difference between consecutive elements\n",
    "\tdeltas = np.diff(unique_dists)\n",
    "\n",
    "\t# 4. Filter out floating point noise (e.g. differences like 1e-15)\n",
    "\t# We only care about meaningful structural changes.\n",
    "\tvalid_deltas = deltas[deltas > 1e-7]\n",
    "\n",
    "\tif len(valid_deltas) == 0:\n",
    "\t\treturn 0.0\n",
    "\n",
    "\treturn np.min(valid_deltas)\n",
    "\n",
    "\n",
    "print(\"min dist\")\n",
    "\n",
    "print(get_minimum_distance_delta(Model_A[\"dist_matrix\"]))\n",
    "\n",
    "\n",
    "_pairs_a = 0\n",
    "_pairs_b = 0\n",
    "_groups_a = 0\n",
    "_groups_b = 0\n",
    "t_range_a = []\n",
    "t_range_b = []\n",
    "MODE = \"Jaccard\"\n",
    "for t in tau_range:\n",
    "\n",
    "\t# for t in tau_range:\n",
    "\tp_a, l_a = get_clustering_pairs_and_labels(Model_A[\"dist_matrix\"], t)\n",
    "\tcache_A[t] = p_a, l_a\n",
    "\tp_b, l_b = get_clustering_pairs_and_labels(Model_B[\"dist_matrix\"], t)\n",
    "\tcache_B[t] = p_b, l_b\n",
    "\n",
    "\tif len(p_a) > _pairs_a:\n",
    "\t\t_pairs_a = len(p_a)\n",
    "\t\tt_range_a.append(t)\n",
    "\n",
    "\tif len(p_b) > _pairs_b:\n",
    "\t\t_pairs_b = len(p_b)\n",
    "\t\tt_range_b.append(t)\n",
    "\tif MODE == \"ACGC\":\n",
    "\t\tif len(set(l_a)) > groups_a:\n",
    "\t\t\t_groups_a.append(len(set(l_a)))\n",
    "\t\t\tt_range_a.append(t)\n",
    "\t\tif len(set(l_b)) > groups_b:\n",
    "\t\t\t_groups_b.append(len(set(l_b)))\n",
    "\t\t\tt_range_b.append(t)\n",
    "\n",
    "# print(len(t_range_a))\n",
    "# print(tau_range.shape)\n",
    "\n",
    "for t_A in t_range_a:\n",
    "\n",
    "\tpairs_A, labels_A = cache_A[t_A]\n",
    "\t# for t_B in tau_range:\n",
    "\tfor t_B in t_range_b:\n",
    "\n",
    "\t\tpairs_B, labels_B = cache_B[t_B]\n",
    "\n",
    "\t\tintersection = pairs_A.intersection(pairs_B)\n",
    "\t\tunion = pairs_A.union(pairs_B)\n",
    "\t\tif len(union) > 0:\n",
    "\t\t\tjaccard = len(intersection) / len(union)\n",
    "\n",
    "\t\t\tif jaccard > best_jaccard:\n",
    "\n",
    "\t\t\t\tbest_jaccard = jaccard\n",
    "\t\t\t\tbest_metric_score = jaccard\n",
    "\t\t\t\tP_true = intersection\n",
    "\t\t\t\tlabels_A_star = labels_A\n",
    "\t\t\t\tlabels_B_star = labels_B\n",
    "\t\t\t\tTAU_A = t_A\n",
    "\t\t\t\tTAU_B = t_B\n",
    "\n",
    "# # for t_A in tau_range:\n",
    "# # \tpairs_A, labels_A = cache_A[t_A]\n",
    "# # \tfor t_B in tau_range:\n",
    "# # \t\tpairs_B, labels_B = cache_B[t_B]\n",
    "\n",
    "# # \t\tintersection = pairs_A.intersection(pairs_B)\n",
    "# # \t\tunion = pairs_A.union(pairs_B)\n",
    "# # \t\tif len(union) > 0:\n",
    "# # \t\t\tjaccard = len(intersection) / len(union)\n",
    "\n",
    "# # \t\t\tif jaccard > best_jaccard:\n",
    "\n",
    "# # \t\t\t\tbest_jaccard = jaccard\n",
    "# # \t\t\t\tbest_metric_score = jaccard\n",
    "# # \t\t\t\tP_true = intersection\n",
    "# # \t\t\t\tlabels_A_star = labels_A\n",
    "# # \t\t\t\tlabels_B_star = labels_B\n",
    "# # \t\t\t\tTAU_A = t_A\n",
    "# # \t\t\t\tTAU_B = t_B\n",
    "\n",
    "\n",
    "# print(f\"\\n{'='*40}\")\n",
    "# print(f\"OPTIMIZATION RESULTS (Metric: ACGC)\")\n",
    "# print(f\"{'='*40}\")\n",
    "# print(f\"Best Metric Score: {best_metric_score}\")\n",
    "# print(f\"Selected Thresholds -> A: {TAU_A:.4f} | B: {TAU_B:.4f}\")\n",
    "\n",
    "# print(f\"\\nOptimization Results:\")\n",
    "# print(f\"Best Metric Score: {best_metric_score}\")\n",
    "# print(f\"TAU_A: {TAU_A}\")\n",
    "# print(f\"TAU_B: {TAU_B}\")\n",
    "\n",
    "\n",
    "# def print_ordered_groupings(labels, model_data, title):\n",
    "# \tprint(f\"\\n{'='*80}\")\n",
    "# \tprint(f\"OPTIMIZED GROUPINGS: {title}\")\n",
    "# \tprint(f\"{'='*80}\")\n",
    "\n",
    "# \tif labels is None:\n",
    "# \t\tprint(\"No groupings found.\")\n",
    "# \t\treturn\n",
    "\n",
    "# \tdf = pd.DataFrame({\"string\": strings, \"label\": labels, \"idx\": range(len(strings))})\n",
    "\n",
    "# \traw_groups = [g for _, g in df.groupby(\"label\") if len(g) > 1]\n",
    "# \tprocessed_groups = []\n",
    "\n",
    "# \tfor group in raw_groups:\n",
    "# \t\tindices = group[\"idx\"].tolist()\n",
    "# \t\tcluster_strs = group[\"string\"].tolist()\n",
    "\n",
    "# \t\tvecs = model_data[\"vectors\"][indices]\n",
    "# \t\tprec = model_data[\"precision\"]\n",
    "# \t\tlocal_mean = np.mean(vecs, axis=0)\n",
    "\n",
    "# \t\tdistances = []\n",
    "# \t\tmin_dist = float(\"inf\")\n",
    "# \t\trep_idx = -1\n",
    "\n",
    "# \t\tfor local_i, global_i in enumerate(indices):\n",
    "# \t\t\td = mahalanobis(model_data[\"vectors\"][global_i], local_mean, prec)\n",
    "# \t\t\tdistances.append(d)\n",
    "# \t\t\tif d < min_dist:\n",
    "# \t\t\t\tmin_dist = d\n",
    "# \t\t\t\trep_idx = local_i\n",
    "\n",
    "# \t\tradius = max(distances)\n",
    "\n",
    "# \t\tprocessed_groups.append(\n",
    "# \t\t\t{\"radius\": radius, \"members\": cluster_strs, \"rep_idx\": rep_idx, \"size\": len(indices)}\n",
    "# \t\t)\n",
    "\n",
    "# \t# Sort by Radius Ascending (Smallest/Tightest at top)\n",
    "# \tprocessed_groups.sort(key=lambda x: x[\"radius\"])\n",
    "\n",
    "# \tprint(f\"Found {len(processed_groups)} significant groups.\\n\")\n",
    "\n",
    "# \tfor i, g in enumerate(processed_groups):\n",
    "# \t\tprint(f\"GROUP {i+1} (Size: {g['size']}) [Radius: {g['radius']:.4f}]\")\n",
    "\n",
    "# \t\tfor idx, s in enumerate(g[\"members\"]):\n",
    "# \t\t\tprefix = \" [CENTROID] \" if idx == g[\"rep_idx\"] else \"            \"\n",
    "# \t\t\tprint(f\"{prefix} {s}\")\n",
    "# \t\tprint(\"-\" * 80)\n",
    "\n",
    "\n",
    "# print_ordered_groupings(labels_A_star, Model_A, \"Model A (Statement)\")\n",
    "# print_ordered_groupings(labels_B_star, Model_B, \"Model B (Question)\")\n",
    "\n",
    "# # print(f\"{'='*40}\\n\")\n",
    "# # for t_A in tau_range:\n",
    "# # \tpairs_A, labels_A = cache_A[t_A]\n",
    "# # \tfor t_B in tau_range:\n",
    "# # \t\tpairs_B, labels_B = cache_B[t_B]\n",
    "\n",
    "# # \t\tintersection = pairs_A.intersection(pairs_B)\n",
    "# # \t\tunion = pairs_A.union(pairs_B)\n",
    "# # \t\tif len(union) > 0:\n",
    "# # \t\t\tjaccard = len(intersection) / len(union)\n",
    "\n",
    "# # \t\t\tif jaccard > best_jaccard:\n",
    "\n",
    "# # \t\t\t\tbest_jaccard = jaccard\n",
    "# # \t\t\t\tP_true = intersection\n",
    "# # \t\t\t\tlabels_A_star = labels_A\n",
    "# # \t\t\t\tlabels_B_star = labels_B\n",
    "# # \t\t\t\tTAU_A = t_A\n",
    "# # \t\t\t\tTAU_B = t_B\n",
    "\n",
    "# # print(f\"tau a:{TAU_A}, tau b:{TAU_B}\")\n",
    "\n",
    "\n",
    "# # N_A_true = calculate_n_true(labels_A_star, P_true)\n",
    "# # N_B_true = calculate_n_true(labels_B_star, P_true)\n",
    "# # N_target = (N_A_true + N_B_true) / 2\n",
    "\n",
    "\n",
    "# # def getMaxStable(start_tau, model_data):\n",
    "# # \t\"\"\"\n",
    "# # \tIncreases threshold until baseline clusters are no longer a subset of current clusters.\n",
    "# # \tReturns the number of significant clusters at the maximum stable threshold.\n",
    "# # \t\"\"\"\n",
    "# # \tdist_matrix = model_data[\"dist_matrix\"]\n",
    "\n",
    "# # \tbase_model = AgglomerativeClustering(\n",
    "# # \t\tn_clusters=None,\n",
    "# # \t\tdistance_threshold=start_tau,\n",
    "# # \t\tmetric=\"precomputed\",\n",
    "# # \t\tlinkage=\"complete\",\n",
    "# # \t)\n",
    "# # \tbase_labels = base_model.fit_predict(dist_matrix)\n",
    "\n",
    "# # \tdef get_cluster_set_and_count(labels, n_items):\n",
    "# # \t\tdf = pd.DataFrame({\"label\": labels, \"idx\": range(n_items)})\n",
    "# # \t\tgroups = [\n",
    "# # \t\t\tg[\"idx\"].sort_values().tolist() for _, g in df.groupby(\"label\") if len(g) > 1\n",
    "# # \t\t]\n",
    "\n",
    "# # \t\treturn set(tuple(g) for g in groups), len(groups)\n",
    "\n",
    "# # \tbaseline_indices, baseline_count = get_cluster_set_and_count(\n",
    "# # \t\tbase_labels, len(dist_matrix)\n",
    "# # \t)\n",
    "\n",
    "# # \tif not baseline_indices:\n",
    "# # \t\treturn 0\n",
    "\n",
    "# # \tSTEP_SIZE = SEARCH_STEP\n",
    "# # \tMAX_ITERATIONS = 500\n",
    "# # \tt = start_tau\n",
    "# # \tlast_stable_count = baseline_count\n",
    "\n",
    "# # \tfor _ in range(MAX_ITERATIONS):\n",
    "# # \t\tt += STEP_SIZE\n",
    "\n",
    "# # \t\tmodel_t = AgglomerativeClustering(\n",
    "# # \t\t\tn_clusters=None,\n",
    "# # \t\t\tdistance_threshold=t,\n",
    "# # \t\t\tmetric=\"precomputed\",\n",
    "# # \t\t\tlinkage=\"complete\",\n",
    "# # \t\t)\n",
    "# # \t\tlabels_t = model_t.fit_predict(dist_matrix)\n",
    "\n",
    "# # \t\tcurrent_indices, current_count = get_cluster_set_and_count(labels_t, len(dist_matrix))\n",
    "\n",
    "# # \t\tif not baseline_indices.issubset(current_indices):\n",
    "# # \t\t\treturn last_stable_count\n",
    "\n",
    "# # \t\tlast_stable_count = current_count\n",
    "\n",
    "# # \treturn last_stable_count\n",
    "\n",
    "\n",
    "# # print(f\"\\nConsensus Structure Found:\")\n",
    "# # print(f\"Platinum Pairs Identified: {len(P_true)}\")\n",
    "# # print(f\"N_target: {N_target:.1f} (Avg of A={N_A_true}, B={N_B_true})\")\n",
    "\n",
    "\n",
    "# # # This bit is an artifact and the found t's are actually necesarily the corresponding tau's im very sure\n",
    "# # def tune_threshold_by_group_count(model_data, N_target, model_name):\n",
    "# # \tbest_t = 0\n",
    "# # \tmin_error = float(\"inf\")\n",
    "# # \tbest_f1 = -1\n",
    "\n",
    "# # \tfor t in t_range:\n",
    "# # \t\tcurrent_pairs, current_labels = get_clustering_pairs_and_labels(\n",
    "# # \t\t\tmodel_data[\"dist_matrix\"], t\n",
    "# # \t\t)\n",
    "\n",
    "# # \t\tN_predicted = calculate_n_true(current_labels, P_true)\n",
    "# # \t\tgroup_error = abs(N_predicted - N_target)\n",
    "\n",
    "# # \t\ttp = len(current_pairs.intersection(P_true))\n",
    "# # \t\tfp = len(current_pairs - P_true)\n",
    "# # \t\tfn = len(P_true - current_pairs)\n",
    "\n",
    "# # \t\tif tp > 0:\n",
    "# # \t\t\tprecision = tp / (tp + fp)\n",
    "# # \t\t\trecall = tp / (tp + fn)\n",
    "# # \t\t\tf1 = 2 * (precision * recall) / (precision + recall)\n",
    "# # \t\telse:\n",
    "# # \t\t\tf1 = 0\n",
    "\n",
    "# # \t\tif group_error < min_error:\n",
    "# # \t\t\tmin_error = group_error\n",
    "# # \t\t\tbest_f1 = f1\n",
    "# # \t\t\tbest_t = t\n",
    "# # \t\telif group_error == min_error and f1 > best_f1:\n",
    "# # \t\t\tbest_f1 = f1\n",
    "# # \t\t\tbest_t = t\n",
    "\n",
    "# # \tprint(\n",
    "# # \t\tf\"[{model_name}] Optimal t: {best_t:.1f} | Group Error: {min_error:.1f} | F1: {best_f1:.4f}\"\n",
    "# # \t)\n",
    "# # \treturn best_t\n",
    "\n",
    "\n",
    "# # optimal_t_A = tune_threshold_by_group_count(Model_A, N_target, \"Model A\")\n",
    "# # optimal_t_B = tune_threshold_by_group_count(Model_B, N_target, \"Model B\")\n",
    "\n",
    "# # print(f\"\\nFinal Calibrated Thresholds:\")\n",
    "# # print(f\"Model A (Statement): {optimal_t_A:.2f}\")\n",
    "# # print(f\"Model B (Question):  {optimal_t_B:.2f}\")\n",
    "\n",
    "\n",
    "# # def print_final_clusters(model_data, threshold, string_list, title):\n",
    "# # \tprint(f\"\\n{'='*80}\")\n",
    "# # \tprint(f\"FINAL OUTPUT: {title} (Threshold: {threshold:.2f})\")\n",
    "# # \tprint(f\"{'='*80}\")\n",
    "\n",
    "# # \tcluster_model = AgglomerativeClustering(\n",
    "# # \t\tn_clusters=None,\n",
    "# # \t\tdistance_threshold=threshold,\n",
    "# # \t\tmetric=\"precomputed\",\n",
    "# # \t\tlinkage=\"complete\",\n",
    "# # \t)\n",
    "# # \tlabels = cluster_model.fit_predict(model_data[\"dist_matrix\"])\n",
    "\n",
    "# # \tdf = pd.DataFrame(\n",
    "# # \t\t{\"string\": string_list, \"label\": labels, \"idx\": range(len(string_list))}\n",
    "# # \t)\n",
    "# # \tgroups = [g for _, g in df.groupby(\"label\") if len(g) > 1]\n",
    "# # \tgroups.sort(key=lambda x: len(x), reverse=True)\n",
    "\n",
    "# # \tfinal_pairs, _ = get_clustering_pairs_and_labels(model_data[\"dist_matrix\"], threshold)\n",
    "# # \ttp = len(final_pairs.intersection(P_true))\n",
    "# # \tfp = len(final_pairs - P_true)\n",
    "\n",
    "# # \tprint(f\"Found {len(groups)} total significant groups.\")\n",
    "# # \tprint(f\"P_true pairs captured: {tp} (False Positives: {fp})\\n\")\n",
    "\n",
    "# # \tfor i, group in enumerate(groups):\n",
    "# # \t\tindices = group[\"idx\"].tolist()\n",
    "# # \t\tcluster_strs = group[\"string\"].tolist()\n",
    "\n",
    "# # \t\tvecs = model_data[\"vectors\"][indices]\n",
    "# # \t\tprec = model_data[\"precision\"]\n",
    "# # \t\tlocal_mean = np.mean(vecs, axis=0)\n",
    "\n",
    "# # \t\tdistances = []\n",
    "# # \t\tmin_dist = float(\"inf\")\n",
    "# # \t\trep_idx = -1\n",
    "\n",
    "# # \t\tfor local_i, global_i in enumerate(indices):\n",
    "# # \t\t\td = mahalanobis(model_data[\"vectors\"][global_i], local_mean, prec)\n",
    "# # \t\t\tdistances.append(d)\n",
    "# # \t\t\tif d < min_dist:\n",
    "# # \t\t\t\tmin_dist = d\n",
    "# # \t\t\t\trep_idx = local_i\n",
    "\n",
    "# # \t\tgroup_radius = max(distances)\n",
    "\n",
    "# # \t\tprint(f\"GROUP {i+1} (Size: {len(indices)}) [Radius: {group_radius:.4f}]\")\n",
    "\n",
    "# # \t\tfor idx, s in enumerate(cluster_strs):\n",
    "# # \t\t\tprefix = \" [CENTROID] \" if idx == rep_idx else \"            \"\n",
    "# # \t\t\tprint(f\"{prefix} {s}\")\n",
    "# # \t\tprint(\"-\" * 80)\n",
    "\n",
    "\n",
    "# # print_final_clusters(Model_A, optimal_t_A, strings, \"Model A (Statement Embeddings)\")\n",
    "# # print_final_clusters(Model_B, optimal_t_B, strings, \"Model B (Question Embeddings)\")\n",
    "\n",
    "# # import numpy as np\n",
    "# # import pandas as pd\n",
    "# # from scipy.spatial.distance import squareform\n",
    "# # from scipy.stats import rankdata\n",
    "# # from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "# # def get_nearest_neighbor_distances(dist_matrix):\n",
    "# # \t\"\"\"\n",
    "# # \tExtracts the distance to the nearest neighbor for every point.\n",
    "# # \tIgnores the diagonal (0).\n",
    "# # \t\"\"\"\n",
    "# # \tnp.fill_diagonal(dist_matrix, float(\"inf\"))\n",
    "# # \tmin_dists = np.min(dist_matrix, axis=1)\n",
    "# # \tnp.fill_diagonal(dist_matrix, 0.0)\n",
    "# # \treturn min_dists\n",
    "\n",
    "\n",
    "# # def convert_dist_to_prob(dist_matrix, reference_dist_array):\n",
    "# # \t\"\"\"\n",
    "# # \tConverts raw distances into Probabilities (P-values) based on the\n",
    "# # \tEmpirical CDF of the provided reference distribution (Nearest Neighbors).\n",
    "\n",
    "# # \tP(d) = (Rank of d) / (Total Count + 1)\n",
    "# # \t\"\"\"\n",
    "\n",
    "# # \tsorted_refs = np.sort(reference_dist_array)\n",
    "# # \tn = len(sorted_refs)\n",
    "\n",
    "# # \tranks = np.searchsorted(sorted_refs, dist_matrix)\n",
    "\n",
    "# # \tprobs = (ranks + 1) / (n + 1)\n",
    "\n",
    "# # \treturn probs\n",
    "\n",
    "\n",
    "# # print(\"--- Calculating Empirical Probabilities (EVT Logic) ---\")\n",
    "\n",
    "\n",
    "# # nn_dists_A = get_nearest_neighbor_distances(Model_A[\"dist_matrix\"])\n",
    "# # nn_dists_B = get_nearest_neighbor_distances(Model_B[\"dist_matrix\"])\n",
    "\n",
    "\n",
    "# # Prob_A = convert_dist_to_prob(Model_A[\"dist_matrix\"], nn_dists_A)\n",
    "# # Prob_B = convert_dist_to_prob(Model_B[\"dist_matrix\"], nn_dists_B)\n",
    "\n",
    "# # print(f\"Probabilities Calculated.\")\n",
    "# # print(\n",
    "# # \tf\"Example (Model A): Dist=8.8 -> P={np.interp(8.8, np.sort(nn_dists_A), np.linspace(0,1,len(nn_dists_A))):.5f}\"\n",
    "# # )\n",
    "# # print(\n",
    "# # \tf\"Example (Model B): Dist=8.8 -> P={np.interp(8.8, np.sort(nn_dists_B), np.linspace(0,1,len(nn_dists_B))):.5f}\"\n",
    "# # )\n",
    "\n",
    "\n",
    "# # # print(f\"Inferred Probability Threshold: {PROB_THRESHOLD:.6f}\")\n",
    "# # Prob_Fused = np.minimum(Prob_A, Prob_B)\n",
    "\n",
    "# # PROB_THRESHOLD =  5 / 105\n",
    "\n",
    "# # print(f\"\\n{'='*80}\")\n",
    "# # print(f\"STAGE 2 OUTPUT: Probabilistic Outlier Model (P < {PROB_THRESHOLD:.6f})\")\n",
    "# # print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "# # cluster_model = AgglomerativeClustering(\n",
    "# # \tn_clusters=None,\n",
    "# # \tdistance_threshold=PROB_THRESHOLD,\n",
    "# # \tmetric=\"precomputed\",\n",
    "# # \tlinkage=\"complete\",\n",
    "# # )\n",
    "\n",
    "# # labels = cluster_model.fit_predict(Prob_Fused)\n",
    "\n",
    "\n",
    "# # df = pd.DataFrame({\"string\": strings, \"label\": labels, \"idx\": range(len(strings))})\n",
    "# # groups = [g for _, g in df.groupby(\"label\") if len(g) > 1]\n",
    "# # groups.sort(key=lambda x: len(x), reverse=True)\n",
    "\n",
    "# # print(f\"Found {len(groups)} significant groups.\\n\")\n",
    "\n",
    "# # for i, group in enumerate(groups):\n",
    "# # \tindices = group[\"idx\"].tolist()\n",
    "# # \tcluster_strs = group[\"string\"].tolist()\n",
    "\n",
    "# # \tref_vectors = Model_B[\"vectors\"][indices]\n",
    "# # \tref_prec = Model_B[\"precision\"]\n",
    "# # \tlocal_mean = np.mean(ref_vectors, axis=0)\n",
    "\n",
    "# # \tdists = []\n",
    "# # \tmin_d = float(\"inf\")\n",
    "# # \trep_i = -1\n",
    "\n",
    "# # \tfor loc_i, glob_i in enumerate(indices):\n",
    "# # \t\td = mahalanobis(Model_B[\"vectors\"][glob_i], local_mean, ref_prec)\n",
    "# # \t\tdists.append(d)\n",
    "# # \t\tif d < min_d:\n",
    "# # \t\t\tmin_d = d\n",
    "# # \t\t\trep_i = loc_i\n",
    "\n",
    "# # \tprint(f\"GROUP {i+1} (Size: {len(indices)}) [Radius: {max(dists):.4f}]\")\n",
    "\n",
    "# # \tfor idx, s in enumerate(cluster_strs):\n",
    "# # \t\tprefix = \" [CENTROID] \" if idx == rep_i else \"            \"\n",
    "# # \t\tprint(f\"{prefix} {s}\")\n",
    "# # \tprint(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990c76c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(392,)\n",
      "(134941,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(520, 520)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(t_range_a))\n",
    "print(tau_range.shape)\n",
    "\n",
    "dists_A_x = np.unique(Model_A[\"dist_matrix\"])\n",
    "tau_range_x = np.sort(np.unique(dists_A_x))\n",
    "print(tau_range_x.shape)\n",
    "Model_A[\"dist_matrix\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a683b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Calculating Empirical Probabilities (EVT Logic) ---\n",
      "Probabilities Calculated.\n",
      "Example (Model A): Dist=8.8 -> P=0.01354\n",
      "Example (Model B): Dist=8.8 -> P=0.01744\n",
      "\n",
      "================================================================================\n",
      "STAGE 2 OUTPUT: Probabilistic Outlier Model (P < 0.047619)\n",
      "================================================================================\n",
      "Found 14 significant groups.\n",
      "\n",
      "GROUP 1 (Size: 2) [Radius: 4.8566]\n",
      " [CENTROID]  Does the privacy policy affirm that Inputs and Outputs disassociated via Feedback are used for training models?\n",
      "             Does the privacy policy affirm that Inputs and Outputs disassociated via Feedback are used for improving models?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 2 (Size: 2) [Radius: 4.7142]\n",
      "             Does the privacy policy affirm that processing contact information to send technical announcements is based on the necessity to perform a contract?\n",
      " [CENTROID]  Does the privacy policy affirm that the company processes contact information to send technical announcements based on the necessity to perform a contract?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 3 (Size: 2) [Radius: 4.7381]\n",
      " [CENTROID]  Does the privacy policy affirm that the company collects the user's time zone?\n",
      "             Does the privacy policy affirm that the company collects time zone settings?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 4 (Size: 2) [Radius: 4.6968]\n",
      " [CENTROID]  Does the privacy policy affirm that Personal Data is retained to comply with legal obligations?\n",
      "             Does the privacy policy affirm that Personal Data is used to comply with legal obligations?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 5 (Size: 2) [Radius: 4.5656]\n",
      " [CENTROID]  Does the privacy policy affirm that Personal Data is used to protect the rights, privacy, safety, or property of users?\n",
      "             Does the privacy policy affirm that Personal Data is used to protect the rights, privacy, safety, or property of the company?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 6 (Size: 2) [Radius: 5.2123]\n",
      " [CENTROID]  Does the privacy policy affirm that Personal Data is used to improve the company's services?\n",
      "             Does the privacy policy affirm that Personal Data is used to develop the company's services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 7 (Size: 2) [Radius: 4.8486]\n",
      " [CENTROID]  Does the privacy policy affirm that users have the statutory right to access their Personal Data?\n",
      "             Does the privacy policy affirm that users have the statutory right to access information relating to how their Personal Data is processed?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 8 (Size: 2) [Radius: 4.3913]\n",
      " [CENTROID]  Does the privacy policy affirm that providing personal data constitutes agreement to the transfer of data outside of Canada?\n",
      "             Does the privacy policy affirm that providing personal data constitutes agreement to the disclosure of data outside of Canada?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 9 (Size: 2) [Radius: 4.6063]\n",
      " [CENTROID]  Does the privacy policy affirm that the company may disclose personal data to governmental regulatory authorities as required by law?\n",
      "             Does the privacy policy affirm that the company may disclose personal data in response to requests from governmental regulatory authorities?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 10 (Size: 2) [Radius: 4.3318]\n",
      " [CENTROID]  Does the privacy policy affirm that the company relies on Standard Contractual Clauses (SCCs) for transfers to countries without an adequacy decision?\n",
      "             Does the privacy policy affirm that the company relies on Standard Contractual Clauses (SCCs) for transfers to jurisdictions without adequacy decisions?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 11 (Size: 2) [Radius: 5.1673]\n",
      " [CENTROID]  Does the privacy policy affirm that personal data may be shared with service providers for data processing purposes?\n",
      "             Does the privacy policy affirm that personal data may be shared with service providers for the purpose of providing services to the user?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 12 (Size: 2) [Radius: 4.2495]\n",
      " [CENTROID]  Does the privacy policy affirm that the company relies on user consent to process contact information for specific marketing communications?\n",
      "             Does the privacy policy affirm that the company processes contact information for marketing communications based on user consent?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 13 (Size: 2) [Radius: 2.8865]\n",
      " [CENTROID]  Does the privacy policy affirm that users have the right to request the correction of inaccurate personal data?\n",
      "             Does the privacy policy affirm that users have the right to request the correction of inaccurate data?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 14 (Size: 2) [Radius: 4.0055]\n",
      " [CENTROID]  Does the privacy policy affirm that the company collects the dates and times of access?\n",
      "             Does the privacy policy affirm that the company collects the dates and times of access to the services?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "def get_nearest_neighbor_distances(dist_matrix):\n",
    "\t\"\"\"\n",
    "\tExtracts the distance to the nearest neighbor for every point.\n",
    "\tIgnores the diagonal (0).\n",
    "\t\"\"\"\n",
    "\tnp.fill_diagonal(dist_matrix, float(\"inf\"))\n",
    "\tmin_dists = np.min(dist_matrix, axis=1)\n",
    "\tnp.fill_diagonal(dist_matrix, 0.0)\n",
    "\treturn min_dists\n",
    "\n",
    "\n",
    "def convert_dist_to_prob(dist_matrix, reference_dist_array):\n",
    "\t\"\"\"\n",
    "\tConverts raw distances into Probabilities (P-values) based on the\n",
    "\tEmpirical CDF of the provided reference distribution (Nearest Neighbors).\n",
    "\n",
    "\tP(d) = (Rank of d) / (Total Count + 1)\n",
    "\t\"\"\"\n",
    "\n",
    "\tsorted_refs = np.sort(reference_dist_array)\n",
    "\tn = len(sorted_refs)\n",
    "\n",
    "\tranks = np.searchsorted(sorted_refs, dist_matrix)\n",
    "\n",
    "\tprobs = (ranks + 1) / (n + 1)\n",
    "\n",
    "\treturn probs\n",
    "\n",
    "\n",
    "print(\"--- Calculating Empirical Probabilities (EVT Logic) ---\")\n",
    "\n",
    "\n",
    "nn_dists_A = get_nearest_neighbor_distances(Model_A[\"dist_matrix\"])\n",
    "nn_dists_B = get_nearest_neighbor_distances(Model_B[\"dist_matrix\"])\n",
    "\n",
    "\n",
    "Prob_A = convert_dist_to_prob(Model_A[\"dist_matrix\"], nn_dists_A)\n",
    "Prob_B = convert_dist_to_prob(Model_B[\"dist_matrix\"], nn_dists_B)\n",
    "\n",
    "print(f\"Probabilities Calculated.\")\n",
    "print(\n",
    "\tf\"Example (Model A): Dist=8.8 -> P={np.interp(8.8, np.sort(nn_dists_A), np.linspace(0,1,len(nn_dists_A))):.5f}\"\n",
    ")\n",
    "print(\n",
    "\tf\"Example (Model B): Dist=8.8 -> P={np.interp(8.8, np.sort(nn_dists_B), np.linspace(0,1,len(nn_dists_B))):.5f}\"\n",
    ")\n",
    "\n",
    "\n",
    "# print(f\"Inferred Probability Threshold: {PROB_THRESHOLD:.6f}\")\n",
    "Prob_Fused = np.minimum(Prob_A, Prob_B)\n",
    "\n",
    "PROB_THRESHOLD = 5 / 105\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STAGE 2 OUTPUT: Probabilistic Outlier Model (P < {PROB_THRESHOLD:.6f})\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "cluster_model = AgglomerativeClustering(\n",
    "\tn_clusters=None,\n",
    "\tdistance_threshold=PROB_THRESHOLD,\n",
    "\tmetric=\"precomputed\",\n",
    "\tlinkage=\"complete\",\n",
    ")\n",
    "\n",
    "labels = cluster_model.fit_predict(Prob_Fused)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"string\": strings, \"label\": labels, \"idx\": range(len(strings))})\n",
    "groups = [g for _, g in df.groupby(\"label\") if len(g) > 1]\n",
    "groups.sort(key=lambda x: len(x), reverse=True)\n",
    "\n",
    "print(f\"Found {len(groups)} significant groups.\\n\")\n",
    "\n",
    "for i, group in enumerate(groups):\n",
    "\tindices = group[\"idx\"].tolist()\n",
    "\tcluster_strs = group[\"string\"].tolist()\n",
    "\n",
    "\tref_vectors = Model_B[\"vectors\"][indices]\n",
    "\tref_prec = Model_B[\"precision\"]\n",
    "\tlocal_mean = np.mean(ref_vectors, axis=0)\n",
    "\n",
    "\tdists = []\n",
    "\tmin_d = float(\"inf\")\n",
    "\trep_i = -1\n",
    "\n",
    "\tfor loc_i, glob_i in enumerate(indices):\n",
    "\t\td = mahalanobis(Model_B[\"vectors\"][glob_i], local_mean, ref_prec)\n",
    "\t\tdists.append(d)\n",
    "\t\tif d < min_d:\n",
    "\t\t\tmin_d = d\n",
    "\t\t\trep_i = loc_i\n",
    "\n",
    "\tprint(f\"GROUP {i+1} (Size: {len(indices)}) [Radius: {max(dists):.4f}]\")\n",
    "\n",
    "\tfor idx, s in enumerate(cluster_strs):\n",
    "\t\tprefix = \" [CENTROID] \" if idx == rep_i else \"            \"\n",
    "\t\tprint(f\"{prefix} {s}\")\n",
    "\tprint(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a44616a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 2: Probabilistic Intersection & Inference ---\n",
      "Probabilities Fused (Geometric Mean).\n",
      "Elbow Detected at Index: 96 / 520\n",
      "Inferred Probability Threshold: 0.208896\n",
      "\n",
      "================================================================================\n",
      "STAGE 2 OUTPUT: Probabilistic Intersection Model (P < 0.208896)\n",
      "================================================================================\n",
      "Found 41 significant groups.\n",
      "\n",
      "GROUP 1 (Size: 2) [Radius: 2.8865]\n",
      " [CENTROID]  Does the privacy policy affirm that users have the right to request the correction of inaccurate personal data?\n",
      "             Does the privacy policy affirm that users have the right to request the correction of inaccurate data?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 2 (Size: 2) [Radius: 4.0055]\n",
      " [CENTROID]  Does the privacy policy affirm that the company collects the dates and times of access?\n",
      "             Does the privacy policy affirm that the company collects the dates and times of access to the services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 3 (Size: 2) [Radius: 4.2495]\n",
      " [CENTROID]  Does the privacy policy affirm that the company relies on user consent to process contact information for specific marketing communications?\n",
      "             Does the privacy policy affirm that the company processes contact information for marketing communications based on user consent?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 4 (Size: 2) [Radius: 4.3318]\n",
      " [CENTROID]  Does the privacy policy affirm that the company relies on Standard Contractual Clauses (SCCs) for transfers to countries without an adequacy decision?\n",
      "             Does the privacy policy affirm that the company relies on Standard Contractual Clauses (SCCs) for transfers to jurisdictions without adequacy decisions?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 5 (Size: 2) [Radius: 4.3913]\n",
      " [CENTROID]  Does the privacy policy affirm that providing personal data constitutes agreement to the transfer of data outside of Canada?\n",
      "             Does the privacy policy affirm that providing personal data constitutes agreement to the disclosure of data outside of Canada?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 6 (Size: 2) [Radius: 4.6968]\n",
      " [CENTROID]  Does the privacy policy affirm that Personal Data is retained to comply with legal obligations?\n",
      "             Does the privacy policy affirm that Personal Data is used to comply with legal obligations?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 7 (Size: 2) [Radius: 4.7142]\n",
      "             Does the privacy policy affirm that processing contact information to send technical announcements is based on the necessity to perform a contract?\n",
      " [CENTROID]  Does the privacy policy affirm that the company processes contact information to send technical announcements based on the necessity to perform a contract?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 8 (Size: 2) [Radius: 4.7381]\n",
      " [CENTROID]  Does the privacy policy affirm that the company collects the user's time zone?\n",
      "             Does the privacy policy affirm that the company collects time zone settings?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 9 (Size: 2) [Radius: 4.8486]\n",
      " [CENTROID]  Does the privacy policy affirm that users have the statutory right to access their Personal Data?\n",
      "             Does the privacy policy affirm that users have the statutory right to access information relating to how their Personal Data is processed?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 10 (Size: 2) [Radius: 4.8566]\n",
      " [CENTROID]  Does the privacy policy affirm that Inputs and Outputs disassociated via Feedback are used for training models?\n",
      "             Does the privacy policy affirm that Inputs and Outputs disassociated via Feedback are used for improving models?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 11 (Size: 2) [Radius: 4.9003]\n",
      " [CENTROID]  Does the privacy policy affirm that users have the right to withdraw consent where processing is based on consent?\n",
      "             Does the privacy policy affirm that users have the right to withdraw their consent?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 12 (Size: 2) [Radius: 5.1103]\n",
      " [CENTROID]  Does the privacy policy affirm that the Data Protection Officer can be contacted via email regarding matters related to Personal Data processing?\n",
      "             Does the privacy policy affirm that users can contact the company's Data Protection Officer via email?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 13 (Size: 2) [Radius: 5.1473]\n",
      " [CENTROID]  Does the privacy policy affirm that the Services are not directed to children under 13?\n",
      "             Does the privacy policy affirm that the Services are not intended for children under 13?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 14 (Size: 2) [Radius: 5.1673]\n",
      " [CENTROID]  Does the privacy policy affirm that personal data may be shared with service providers for data processing purposes?\n",
      "             Does the privacy policy affirm that personal data may be shared with service providers for the purpose of providing services to the user?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 15 (Size: 2) [Radius: 5.2123]\n",
      " [CENTROID]  Does the privacy policy affirm that Personal Data is used to improve the company's services?\n",
      "             Does the privacy policy affirm that Personal Data is used to develop the company's services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 16 (Size: 2) [Radius: 5.2215]\n",
      " [CENTROID]  Does the privacy policy affirm that security measures are designed to protect personal data from unauthorized access?\n",
      "             Does the privacy policy affirm that security measures are designed to protect personal data from unauthorized disclosure?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 17 (Size: 2) [Radius: 5.3068]\n",
      " [CENTROID]  Does the privacy policy affirm that the company collects the user's name if they communicate via email or social media pages?\n",
      "             Does the privacy policy affirm that the company collects contact information if the user communicates via email or social media pages?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 18 (Size: 2) [Radius: 5.3921]\n",
      " [CENTROID]  Does the privacy policy affirm that administrators of enterprise or business accounts may access and control a user's account?\n",
      "             Does the privacy policy affirm that administrators of enterprise or business accounts may access a user's Content?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 19 (Size: 2) [Radius: 5.4281]\n",
      " [CENTROID]  Does the privacy policy affirm that user email addresses are entrusted to the domestic representative?\n",
      "             Does the privacy policy affirm that user addresses are entrusted to the domestic representative?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 20 (Size: 2) [Radius: 5.4932]\n",
      " [CENTROID]  Does the privacy policy affirm that personal data is used to investigate security issues?\n",
      "             Does the privacy policy affirm that personal data is used to resolve security issues?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 21 (Size: 2) [Radius: 5.5329]\n",
      " [CENTROID]  Does the privacy policy affirm that the company collects the operating system of the device used to access the Services?\n",
      "             Does the privacy policy affirm that the company collects operating system information?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 22 (Size: 2) [Radius: 5.6265]\n",
      " [CENTROID]  Does the privacy policy affirm that the company does not engage in decision-making based solely on automated processing that produces a legal effect?\n",
      "             Does the privacy policy affirm that the company does not engage in decision-making based solely on automated processing that significantly affects the user?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 23 (Size: 2) [Radius: 5.6686]\n",
      " [CENTROID]  Does the privacy policy affirm that users can contact the domestic representative via telephone?\n",
      "             Does the privacy policy affirm that users can contact the domestic representative via email?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 24 (Size: 2) [Radius: 5.7589]\n",
      " [CENTROID]  Does the privacy policy affirm that personal data is used to investigate disputes?\n",
      "             Does the privacy policy affirm that personal data is used to resolve disputes?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 25 (Size: 2) [Radius: 5.9236]\n",
      " [CENTROID]  Does the privacy policy affirm that the company processes aggregated or de-identified data to conduct research?\n",
      "             Does the privacy policy affirm that Personal Data is used to conduct research?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 26 (Size: 2) [Radius: 6.0266]\n",
      " [CENTROID]  Does the privacy policy affirm that the company performs necessary procedures for handling data when it is no longer required?\n",
      "             Does the privacy policy affirm that service providers perform necessary procedures for handling data when it is no longer required?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 27 (Size: 2) [Radius: 6.0660]\n",
      " [CENTROID]  Does the privacy policy affirm that if a user creates an account using an email address belonging to an employer, the company may share the fact that the user has an account with that employer?\n",
      "             Does the privacy policy affirm that if a user creates an account using an email address belonging to an employer, the company may share account information with the employer to enable the user to be added to a business account?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 28 (Size: 2) [Radius: 6.0733]\n",
      "             Does the privacy policy affirm that users have the right to object to the processing of their Personal Data when the processing is based on legitimate interests?\n",
      " [CENTROID]  Does the privacy policy affirm that users have the right to object to the processing of their personal data?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 29 (Size: 2) [Radius: 6.2328]\n",
      " [CENTROID]  Does the privacy policy affirm that the company uses Standard Contractual Clauses to transfer information to certain affiliates?\n",
      "             Does the privacy policy affirm that the company uses Standard Contractual Clauses to transfer information to third parties?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 30 (Size: 2) [Radius: 6.2346]\n",
      " [CENTROID]  Does the privacy policy affirm that the company trains its models using publicly available information from the Internet?\n",
      "             Does the privacy policy affirm that the company collects publicly available information from the internet to develop the models powering its Services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 31 (Size: 2) [Radius: 6.2482]\n",
      " [CENTROID]  Does the privacy policy affirm that personal data is used to provide products and services related to the user's account?\n",
      "             Does the privacy policy affirm that personal data is used to maintain products and services related to the user's account?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 32 (Size: 2) [Radius: 6.2830]\n",
      " [CENTROID]  Does the privacy policy affirm that users have the right to lodge a complaint with the supervisory authority in the place where they live?\n",
      "             Does the privacy policy affirm that users have the right to lodge a complaint with the supervisory authority in the place where they work?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 33 (Size: 2) [Radius: 6.2905]\n",
      "             Does the privacy policy affirm that Personal Data is used to send information about services and events?\n",
      " [CENTROID]  Does the privacy policy affirm that personal data is used to send information about events?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 34 (Size: 2) [Radius: 6.3103]\n",
      " [CENTROID]  Does the privacy policy affirm that user names are entrusted to the domestic representative?\n",
      "             Does the privacy policy affirm that user IDs are entrusted to the domestic representative?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 35 (Size: 3) [Radius: 6.3322]\n",
      " [CENTROID]  Does the privacy policy affirm that Personal Data is used to protect the rights, privacy, safety, or property of users?\n",
      "             Does the privacy policy affirm that Personal Data is used to protect the rights, privacy, safety, or property of the company?\n",
      "             Does the privacy policy affirm that Personal Data is used to protect the rights, privacy, safety, or property of third parties?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 36 (Size: 3) [Radius: 6.5820]\n",
      "             Does the privacy policy affirm that the company may disclose personal data to governmental regulatory authorities as required by law?\n",
      " [CENTROID]  Does the privacy policy affirm that the company may disclose personal data in response to requests from governmental regulatory authorities?\n",
      "             Does the privacy policy affirm that the company may disclose personal data to assist in investigations by governmental regulatory authorities?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 37 (Size: 2) [Radius: 6.6204]\n",
      " [CENTROID]  Does the privacy policy affirm that users should contact the company via email if they believe a child under 13 has provided Personal Data to the company?\n",
      "             Does the privacy policy affirm that users should contact the company via email if they become aware that a child under 18 has provided personal data?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 38 (Size: 2) [Radius: 6.6492]\n",
      " [CENTROID]  Does the privacy policy affirm that personal data may be disclosed to enforce the company's legal rights?\n",
      "             Does the privacy policy affirm that personal data may be disclosed to enforce the legal rights of others?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 39 (Size: 3) [Radius: 6.9058]\n",
      "             Does the privacy policy affirm that the company does not knowingly collect information from children under the age of 18?\n",
      " [CENTROID]  Does the privacy policy affirm that the company does not knowingly disclose information from children under the age of 18?\n",
      "             Does the privacy policy affirm that the company does not knowingly share information from children under the age of 18?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 40 (Size: 3) [Radius: 7.4186]\n",
      " [CENTROID]  Does the privacy policy affirm that the company implements commercially reasonable technical measures to protect Personal Data?\n",
      "             Does the privacy policy affirm that the company implements commercially reasonable administrative measures to protect Personal Data?\n",
      "             Does the privacy policy affirm that the company implements commercially reasonable organizational measures to protect Personal Data?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 41 (Size: 4) [Radius: 7.4877]\n",
      "             Does the privacy policy affirm that the company trains its models using data provided by users?\n",
      "             Does the privacy policy affirm that user-provided Content may be used to train the company's models?\n",
      " [CENTROID]  Does the privacy policy affirm that personal data is used to train the company's models?\n",
      "             Does the privacy policy affirm that user Inputs and Outputs may be used to train the company's models?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p9/7lnzfn4x6vl70kl41kn6k6l80000gn/T/ipykernel_10762/1929150721.py:69: DeprecationWarning: Arrays of 2-dimensional vectors are deprecated. Use arrays of 3-dimensional vectors instead. (deprecated in NumPy 2.0)\n",
      "  d = np.abs(np.cross(vec_line, p0 - p1)) / norm_line\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# --- 1. Probability Calculation Helpers ---\n",
    "\n",
    "\n",
    "def get_nearest_neighbor_distances(dist_matrix):\n",
    "\t\"\"\"\n",
    "\tExtracts the distance to the nearest neighbor for every point.\n",
    "\tIgnores the diagonal (0).\n",
    "\t\"\"\"\n",
    "\td = dist_matrix.copy()\n",
    "\tnp.fill_diagonal(d, float(\"inf\"))\n",
    "\treturn np.min(d, axis=1)\n",
    "\n",
    "\n",
    "def convert_dist_to_prob(dist_matrix, reference_dist_array):\n",
    "\t\"\"\"\n",
    "\tConverts raw distances into Probabilities (P-values) based on the\n",
    "\tEmpirical CDF of the provided reference distribution.\n",
    "\t\"\"\"\n",
    "\tsorted_refs = np.sort(reference_dist_array)\n",
    "\tn = len(sorted_refs)\n",
    "\tranks = np.searchsorted(sorted_refs, dist_matrix)\n",
    "\treturn (ranks + 1) / (n + 1)\n",
    "\n",
    "\n",
    "print(\"\\n--- Phase 2: Probabilistic Intersection & Inference ---\")\n",
    "\n",
    "# Calculate empirical probabilities for both models\n",
    "nn_dists_A = get_nearest_neighbor_distances(Model_A[\"dist_matrix\"])\n",
    "nn_dists_B = get_nearest_neighbor_distances(Model_B[\"dist_matrix\"])\n",
    "\n",
    "Prob_A = convert_dist_to_prob(Model_A[\"dist_matrix\"], nn_dists_A)\n",
    "Prob_B = convert_dist_to_prob(Model_B[\"dist_matrix\"], nn_dists_B)\n",
    "\n",
    "# --- 2. Geometric Mean Fusion (Strict Intersection) ---\n",
    "# We use Sqrt(A * B). This penalizes divergence.\n",
    "# If A=0.001 and B=0.1, Min=0.001 (Accept), Geometric=0.01 (Reject).\n",
    "Prob_Fused = np.sqrt(Prob_A * Prob_B)\n",
    "\n",
    "print(f\"Probabilities Fused (Geometric Mean).\")\n",
    "\n",
    "\n",
    "# --- 3. Topological Elbow Detection (Kneedle) ---\n",
    "# We find the natural \"Bend\" in the distribution of Best Matches.\n",
    "# Duplicates form the flat floor; Neighbors form the rising slope.\n",
    "\n",
    "temp_prob = Prob_Fused.copy()\n",
    "np.fill_diagonal(temp_prob, 1.0)\t# Ignore self-matches\n",
    "min_probs = np.min(temp_prob, axis=1)\t# Best match for each item\n",
    "\n",
    "sorted_probs = np.sort(min_probs)\n",
    "x = np.arange(len(sorted_probs))\n",
    "y = sorted_probs\n",
    "\n",
    "# Define the Secant Line\n",
    "p1 = np.array([x[0], y[0]])\n",
    "p2 = np.array([x[-1], y[-1]])\n",
    "vec_line = p2 - p1\n",
    "norm_line = np.linalg.norm(vec_line)\n",
    "\n",
    "# Calculate Perpendicular Distance\n",
    "distances = []\n",
    "for i in range(len(x)):\n",
    "\tp0 = np.array([x[i], y[i]])\n",
    "\td = np.abs(np.cross(vec_line, p0 - p1)) / norm_line\n",
    "\tdistances.append(d)\n",
    "\n",
    "elbow_idx = np.argmax(distances)\n",
    "elbow_value = sorted_probs[elbow_idx]\n",
    "\n",
    "PROB_THRESHOLD = elbow_value\n",
    "\n",
    "print(f\"Elbow Detected at Index: {elbow_idx} / {len(x)}\")\n",
    "print(f\"Inferred Probability Threshold: {PROB_THRESHOLD:.6f}\")\n",
    "\n",
    "\n",
    "# --- 4. Final Clustering & Ordered Output ---\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STAGE 2 OUTPUT: Probabilistic Intersection Model (P < {PROB_THRESHOLD:.6f})\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "cluster_model = AgglomerativeClustering(\n",
    "\tn_clusters=None,\n",
    "\tdistance_threshold=PROB_THRESHOLD,\n",
    "\tmetric=\"precomputed\",\n",
    "\tlinkage=\"complete\",\n",
    ")\n",
    "\n",
    "labels = cluster_model.fit_predict(Prob_Fused)\n",
    "\n",
    "df = pd.DataFrame({\"string\": strings, \"label\": labels, \"idx\": range(len(strings))})\n",
    "\n",
    "# Process groups to calculate radius BEFORE printing\n",
    "processed_groups = []\n",
    "raw_groups = [g for _, g in df.groupby(\"label\") if len(g) > 1]\n",
    "\n",
    "for group in raw_groups:\n",
    "\tindices = group[\"idx\"].tolist()\n",
    "\tcluster_strs = group[\"string\"].tolist()\n",
    "\n",
    "\t# Calculate Radius using Model B (as requested in previous snippets)\n",
    "\tref_vectors = Model_B[\"vectors\"][indices]\n",
    "\tref_prec = Model_B[\"precision\"]\n",
    "\tlocal_mean = np.mean(ref_vectors, axis=0)\n",
    "\n",
    "\tdists = []\n",
    "\tmin_d = float(\"inf\")\n",
    "\trep_i = -1\n",
    "\n",
    "\tfor loc_i, glob_i in enumerate(indices):\n",
    "\t\td = mahalanobis(Model_B[\"vectors\"][glob_i], local_mean, ref_prec)\n",
    "\t\tdists.append(d)\n",
    "\t\tif d < min_d:\n",
    "\t\t\tmin_d = d\n",
    "\t\t\trep_i = loc_i\n",
    "\n",
    "\tprocessed_groups.append(\n",
    "\t\t{\n",
    "\t\t\t\"radius\": max(dists),\n",
    "\t\t\t\"size\": len(indices),\n",
    "\t\t\t\"strings\": cluster_strs,\n",
    "\t\t\t\"centroid_idx\": rep_i,\n",
    "\t\t}\n",
    "\t)\n",
    "\n",
    "# Sort by Radius Ascending (Tightest first)\n",
    "processed_groups.sort(key=lambda x: x[\"radius\"])\n",
    "\n",
    "print(f\"Found {len(processed_groups)} significant groups.\\n\")\n",
    "\n",
    "for i, g in enumerate(processed_groups):\n",
    "\tprint(f\"GROUP {i+1} (Size: {g['size']}) [Radius: {g['radius']:.4f}]\")\n",
    "\n",
    "\tfor idx, s in enumerate(g[\"strings\"]):\n",
    "\t\tprefix = \" [CENTROID] \" if idx == g[\"centroid_idx\"] else \"            \"\n",
    "\t\tprint(f\"{prefix} {s}\")\n",
    "\tprint(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de04ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# --- 1. Probability Calculation Helpers ---\n",
    "\n",
    "\n",
    "def get_nearest_neighbor_distances(dist_matrix):\n",
    "\t\"\"\"\n",
    "\tExtracts the distance to the nearest neighbor for every point.\n",
    "\tIgnores the diagonal (0).\n",
    "\t\"\"\"\n",
    "\td = dist_matrix.copy()\n",
    "\tnp.fill_diagonal(d, float(\"inf\"))\n",
    "\treturn np.min(d, axis=1)\n",
    "\n",
    "\n",
    "def convert_dist_to_prob(dist_matrix, reference_dist_array):\n",
    "\t\"\"\"\n",
    "\tConverts raw distances into Probabilities (P-values) based on the\n",
    "\tEmpirical CDF of the provided reference distribution.\n",
    "\t\"\"\"\n",
    "\tsorted_refs = np.sort(reference_dist_array)\n",
    "\tn = len(sorted_refs)\n",
    "\tranks = np.searchsorted(sorted_refs, dist_matrix)\n",
    "\treturn (ranks + 1) / (n + 1)\n",
    "\n",
    "\n",
    "print(\"\\n--- Phase 2: Probabilistic Intersection & Inference ---\")\n",
    "\n",
    "# Calculate empirical probabilities for both models\n",
    "nn_dists_A = get_nearest_neighbor_distances(Model_A[\"dist_matrix\"])\n",
    "nn_dists_B = get_nearest_neighbor_distances(Model_B[\"dist_matrix\"])\n",
    "\n",
    "Prob_A = convert_dist_to_prob(Model_A[\"dist_matrix\"], nn_dists_A)\n",
    "Prob_B = convert_dist_to_prob(Model_B[\"dist_matrix\"], nn_dists_B)\n",
    "\n",
    "# --- 2. Geometric Mean Fusion (Strict Intersection) ---\n",
    "# We use Sqrt(A * B). This penalizes divergence.\n",
    "# If A=0.001 and B=0.1, Min=0.001 (Accept), Geometric=0.01 (Reject).\n",
    "Prob_Fused = np.sqrt(Prob_A * Prob_B)\n",
    "\n",
    "print(f\"Probabilities Fused (Geometric Mean).\")\n",
    "\n",
    "\n",
    "# --- 3. Topological Elbow Detection (Kneedle) ---\n",
    "# We find the natural \"Bend\" in the distribution of Best Matches.\n",
    "# Duplicates form the flat floor; Neighbors form the rising slope.\n",
    "\n",
    "temp_prob = Prob_Fused.copy()\n",
    "np.fill_diagonal(temp_prob, 1.0)\t# Ignore self-matches\n",
    "min_probs = np.min(temp_prob, axis=1)\t# Best match for each item\n",
    "\n",
    "sorted_probs = np.sort(min_probs)\n",
    "x = np.arange(len(sorted_probs))\n",
    "y = sorted_probs\n",
    "\n",
    "# Define the Secant Line\n",
    "p1 = np.array([x[0], y[0]])\n",
    "p2 = np.array([x[-1], y[-1]])\n",
    "vec_line = p2 - p1\n",
    "norm_line = np.linalg.norm(vec_line)\n",
    "\n",
    "# Calculate Perpendicular Distance\n",
    "distances = []\n",
    "for i in range(len(x)):\n",
    "\tp0 = np.array([x[i], y[i]])\n",
    "\td = np.abs(np.cross(vec_line, p0 - p1)) / norm_line\n",
    "\tdistances.append(d)\n",
    "\n",
    "elbow_idx = np.argmax(distances)\n",
    "elbow_value = sorted_probs[elbow_idx]\n",
    "\n",
    "PROB_THRESHOLD = elbow_value\n",
    "\n",
    "print(f\"Elbow Detected at Index: {elbow_idx} / {len(x)}\")\n",
    "print(f\"Inferred Probability Threshold: {PROB_THRESHOLD:.6f}\")\n",
    "\n",
    "\n",
    "# --- 4. Final Clustering & Ordered Output ---\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STAGE 2 OUTPUT: Probabilistic Intersection Model (P < {PROB_THRESHOLD:.6f})\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "cluster_model = AgglomerativeClustering(\n",
    "\tn_clusters=None,\n",
    "\tdistance_threshold=PROB_THRESHOLD,\n",
    "\tmetric=\"precomputed\",\n",
    "\tlinkage=\"complete\",\n",
    ")\n",
    "\n",
    "labels = cluster_model.fit_predict(Prob_Fused)\n",
    "\n",
    "df = pd.DataFrame({\"string\": strings, \"label\": labels, \"idx\": range(len(strings))})\n",
    "\n",
    "# Process groups to calculate radius BEFORE printing\n",
    "processed_groups = []\n",
    "raw_groups = [g for _, g in df.groupby(\"label\") if len(g) > 1]\n",
    "\n",
    "for group in raw_groups:\n",
    "\tindices = group[\"idx\"].tolist()\n",
    "\tcluster_strs = group[\"string\"].tolist()\n",
    "\n",
    "\t# Calculate Radius using Model B (as requested in previous snippets)\n",
    "\tref_vectors = Model_B[\"vectors\"][indices]\n",
    "\tref_prec = Model_B[\"precision\"]\n",
    "\tlocal_mean = np.mean(ref_vectors, axis=0)\n",
    "\n",
    "\tdists = []\n",
    "\tmin_d = float(\"inf\")\n",
    "\trep_i = -1\n",
    "\n",
    "\tfor loc_i, glob_i in enumerate(indices):\n",
    "\t\td = mahalanobis(Model_B[\"vectors\"][glob_i], local_mean, ref_prec)\n",
    "\t\tdists.append(d)\n",
    "\t\tif d < min_d:\n",
    "\t\t\tmin_d = d\n",
    "\t\t\trep_i = loc_i\n",
    "\n",
    "\tprocessed_groups.append(\n",
    "\t\t{\n",
    "\t\t\t\"radius\": max(dists),\n",
    "\t\t\t\"size\": len(indices),\n",
    "\t\t\t\"strings\": cluster_strs,\n",
    "\t\t\t\"centroid_idx\": rep_i,\n",
    "\t\t}\n",
    "\t)\n",
    "\n",
    "# Sort by Radius Ascending (Tightest first)\n",
    "processed_groups.sort(key=lambda x: x[\"radius\"])\n",
    "\n",
    "print(f\"Found {len(processed_groups)} significant groups.\\n\")\n",
    "\n",
    "for i, g in enumerate(processed_groups):\n",
    "\tprint(f\"GROUP {i+1} (Size: {g['size']}) [Radius: {g['radius']:.4f}]\")\n",
    "\n",
    "\tfor idx, s in enumerate(g[\"strings\"]):\n",
    "\t\tprefix = \" [CENTROID] \" if idx == g[\"centroid_idx\"] else \"            \"\n",
    "\t\tprint(f\"{prefix} {s}\")\n",
    "\tprint(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff70d47",
   "metadata": {},
   "source": [
    "Lets start from scratch then I will give you my old code:\n",
    "# Old code\n",
    "```\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from scipy.spatial.distance import cdist, mahalanobis\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from itertools import combinations\n",
    "\n",
    "\n",
    "def prepare_model_artifacts(raw_vector_list, name=\"Model\"):\n",
    "\t\"\"\"\n",
    "\tReturns dictionary containing:\n",
    "\t- 'dist_matrix': N x N pairwise distances\n",
    "\t- 'vectors': N x 256 normalized vectors\n",
    "\t- 'precision': 256 x 256 inverse covariance matrix\n",
    "\t\"\"\"\n",
    "\tprint(f\"Processing {name}...\")\n",
    "\n",
    "\tdata = np.array(raw_vector_list)\n",
    "\tdata_trunc = data[:, :256]\n",
    "\tnorms = np.linalg.norm(data_trunc, axis=1, keepdims=True)\n",
    "\tcleaned_vectors = data_trunc / (norms + 1e-10)\n",
    "\n",
    "\tlw = LedoitWolf()\n",
    "\tlw.fit(cleaned_vectors)\n",
    "\tprecision_matrix = lw.precision_\n",
    "\n",
    "\tdist_matrix = cdist(\n",
    "\t\tcleaned_vectors, cleaned_vectors, metric=\"mahalanobis\", VI=precision_matrix\n",
    "\t)\n",
    "\n",
    "\treturn {\n",
    "\t\t\"dist_matrix\": dist_matrix,\n",
    "\t\t\"vectors\": cleaned_vectors,\n",
    "\t\t\"precision\": precision_matrix,\n",
    "\t}\n",
    "\n",
    "\n",
    "data_set = qdata\n",
    "strings = list(data_set.keys())\n",
    "\n",
    "raw_vectors_A = [data_set[s][\"embedding_vector\"] for s in strings]\n",
    "raw_vectors_B = [data_set[s][\"retrieval_embedding_vector\"] for s in strings]\n",
    "\n",
    "\n",
    "Model_A = prepare_model_artifacts(raw_vectors_A, \"Model A (Statement)\")\n",
    "Model_B = prepare_model_artifacts(raw_vectors_B, \"Model B (Question)\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from scipy.spatial.distance import cdist, mahalanobis\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_pairs_from_labels(labels):\n",
    "\t\"\"\"Converts cluster labels into a Set of unique pairs (indices).\"\"\"\n",
    "\tdf = pd.DataFrame({\"label\": labels, \"id\": range(len(labels))})\n",
    "\tpairs = set()\n",
    "\tfor label, group in df.groupby(\"label\"):\n",
    "\t\tindices = group[\"id\"].tolist()\n",
    "\t\tif len(indices) > 1:\n",
    "\t\t\tfor p in combinations(sorted(indices), 2):\n",
    "\t\t\t\tpairs.add(p)\n",
    "\treturn pairs\n",
    "\n",
    "\n",
    "def get_clustering_pairs_and_labels(dist_matrix, threshold):\n",
    "\t\"\"\"Runs clustering and returns both the pair set and the labels array.\"\"\"\n",
    "\tmodel = AgglomerativeClustering(\n",
    "\t\tn_clusters=None,\n",
    "\t\tdistance_threshold=threshold,\n",
    "\t\tmetric=\"precomputed\",\n",
    "\t\tlinkage=\"complete\",\n",
    "\t)\n",
    "\tlabels = model.fit_predict(dist_matrix)\n",
    "\treturn get_pairs_from_labels(labels), labels\n",
    "\n",
    "\n",
    "def calculate_n_true(labels_array, target_pairs):\n",
    "\t\"\"\"Calculates the number of unique clusters containing the elements of target_pairs.\"\"\"\n",
    "\tif not target_pairs or labels_array is None:\n",
    "\t\treturn 0\n",
    "\n",
    "\tinvolved_indices = set(idx for pair in target_pairs for idx in pair)\n",
    "\n",
    "\tlabels_of_interest = set(\n",
    "\t\tlabels_array[idx] for idx in involved_indices if idx < len(labels_array)\n",
    "\t)\n",
    "\n",
    "\tif not involved_indices:\n",
    "\t\treturn 0\n",
    "\n",
    "\treturn len(labels_of_interest)\n",
    "\n",
    "\n",
    "print(\"\\n--- Phase 1: Grid Search for Consensus (Tau) ---\")\n",
    "\n",
    "\n",
    "dist_A_off = Model_A[\"dist_matrix\"][np.triu_indices_from(Model_A[\"dist_matrix\"], k=1)]\n",
    "dist_B_off = Model_B[\"dist_matrix\"][np.triu_indices_from(Model_B[\"dist_matrix\"], k=1)]\n",
    "all_dists = np.concatenate([dist_A_off, dist_B_off])\n",
    "\n",
    "\n",
    "D_min_min = np.min(all_dists)\n",
    "D_max_max = np.max(all_dists)\n",
    "\n",
    "\n",
    "SEARCH_STEP = 0.1\t# converges to same groups here 0.1 was too high\n",
    "\n",
    "\n",
    "TAU_SEARCH_START = max(D_min_min, 0.0)\n",
    "TAU_SEARCH_END = D_max_max\n",
    "\n",
    "tau_range = np.arange(TAU_SEARCH_START, TAU_SEARCH_END, SEARCH_STEP)\n",
    "t_range = np.arange(TAU_SEARCH_START, TAU_SEARCH_END, SEARCH_STEP)\n",
    "\n",
    "print(\n",
    "\tf\"Search Range Defined: [{TAU_SEARCH_START:.2f} to {TAU_SEARCH_END:.2f}] (Step: {SEARCH_STEP})\"\n",
    ")\n",
    "best_jaccard = -1\n",
    "P_true = set()\n",
    "labels_A_star = None\n",
    "labels_B_star = None\n",
    "\n",
    "\n",
    "cache_A = {}\n",
    "cache_B = {}\n",
    "\n",
    "print(f\"Pre-computing clusters for {len(tau_range)} thresholds...\")\n",
    "for t in tau_range:\n",
    "\tcache_A[t] = get_clustering_pairs_and_labels(Model_A[\"dist_matrix\"], t)\n",
    "\tcache_B[t] = get_clustering_pairs_and_labels(Model_B[\"dist_matrix\"], t)\n",
    "TAU_A = 0\n",
    "TAU_B = 0\n",
    "\n",
    "# best_metric_score = -1\n",
    "\n",
    "# for t_A in tau_range:\n",
    "# \tpairs_A, labels_A = cache_A[t_A]\n",
    "# \tfor t_B in tau_range:\n",
    "# \t\tpairs_B, labels_B = cache_B[t_B]\n",
    "\n",
    "# \t\tintersection = pairs_A.intersection(pairs_B)\n",
    "\n",
    "# \t\tif len(intersection) > 0:\n",
    "\n",
    "# \t\t\tN_A = calculate_n_true(labels_A, intersection)\n",
    "\n",
    "# \t\t\tN_B = calculate_n_true(labels_B, intersection)\n",
    "\n",
    "# \t\t\tavg_consensus_groups = (N_A + N_B) / 2\n",
    "\n",
    "# \t\t\tif avg_consensus_groups > best_metric_score:\n",
    "# \t\t\t\tbest_metric_score = avg_consensus_groups\n",
    "\n",
    "# \t\t\t\t# Update Best State\n",
    "# \t\t\t\tP_true = intersection\n",
    "# \t\t\t\tlabels_A_star = labels_A\n",
    "# \t\t\t\tlabels_B_star = labels_B\n",
    "# \t\t\t\tTAU_A = t_A\n",
    "# \t\t\t\tTAU_B = t_B\n",
    "for t_A in tau_range:\n",
    "\tpairs_A, labels_A = cache_A[t_A]\n",
    "\tfor t_B in tau_range:\n",
    "\t\tpairs_B, labels_B = cache_B[t_B]\n",
    "\n",
    "\t\tintersection = pairs_A.intersection(pairs_B)\n",
    "\t\tunion = pairs_A.union(pairs_B)\n",
    "\t\tif len(union) > 0:\n",
    "\t\t\tjaccard = len(intersection) / len(union)\n",
    "\n",
    "\t\t\tif jaccard > best_jaccard:\n",
    "\n",
    "\t\t\t\tbest_jaccard = jaccard\n",
    "\t\t\t\tP_true = intersection\n",
    "\t\t\t\tlabels_A_star = labels_A\n",
    "\t\t\t\tlabels_B_star = labels_B\n",
    "\t\t\t\tTAU_A = t_A\n",
    "\t\t\t\tTAU_B = t_B\n",
    "\n",
    "print(f\"tau a:{TAU_A}, tau b:{TAU_B}\")\n",
    "\n",
    "\n",
    "N_A_true = calculate_n_true(labels_A_star, P_true)\n",
    "N_B_true = calculate_n_true(labels_B_star, P_true)\n",
    "N_target = (N_A_true + N_B_true) / 2\n",
    "\n",
    "\n",
    "def getMaxStable(start_tau, model_data):\n",
    "\t\"\"\"\n",
    "\tIncreases threshold until baseline clusters are no longer a subset of current clusters.\n",
    "\tReturns the number of significant clusters at the maximum stable threshold.\n",
    "\t\"\"\"\n",
    "\tdist_matrix = model_data[\"dist_matrix\"]\n",
    "\n",
    "\tbase_model = AgglomerativeClustering(\n",
    "\t\tn_clusters=None,\n",
    "\t\tdistance_threshold=start_tau,\n",
    "\t\tmetric=\"precomputed\",\n",
    "\t\tlinkage=\"complete\",\n",
    "\t)\n",
    "\tbase_labels = base_model.fit_predict(dist_matrix)\n",
    "\n",
    "\tdef get_cluster_set_and_count(labels, n_items):\n",
    "\t\tdf = pd.DataFrame({\"label\": labels, \"idx\": range(n_items)})\n",
    "\t\tgroups = [\n",
    "\t\t\tg[\"idx\"].sort_values().tolist() for _, g in df.groupby(\"label\") if len(g) > 1\n",
    "\t\t]\n",
    "\n",
    "\t\treturn set(tuple(g) for g in groups), len(groups)\n",
    "\n",
    "\tbaseline_indices, baseline_count = get_cluster_set_and_count(\n",
    "\t\tbase_labels, len(dist_matrix)\n",
    "\t)\n",
    "\n",
    "\tif not baseline_indices:\n",
    "\t\treturn 0\n",
    "\n",
    "\tSTEP_SIZE = SEARCH_STEP\n",
    "\tMAX_ITERATIONS = 500\n",
    "\tt = start_tau\n",
    "\tlast_stable_count = baseline_count\n",
    "\n",
    "\tfor _ in range(MAX_ITERATIONS):\n",
    "\t\tt += STEP_SIZE\n",
    "\n",
    "\t\tmodel_t = AgglomerativeClustering(\n",
    "\t\t\tn_clusters=None,\n",
    "\t\t\tdistance_threshold=t,\n",
    "\t\t\tmetric=\"precomputed\",\n",
    "\t\t\tlinkage=\"complete\",\n",
    "\t\t)\n",
    "\t\tlabels_t = model_t.fit_predict(dist_matrix)\n",
    "\n",
    "\t\tcurrent_indices, current_count = get_cluster_set_and_count(labels_t, len(dist_matrix))\n",
    "\n",
    "\t\tif not baseline_indices.issubset(current_indices):\n",
    "\t\t\treturn last_stable_count\n",
    "\n",
    "\t\tlast_stable_count = current_count\n",
    "\n",
    "\treturn last_stable_count\n",
    "\n",
    "\n",
    "print(f\"\\nConsensus Structure Found:\")\n",
    "print(f\"Platinum Pairs Identified: {len(P_true)}\")\n",
    "print(f\"N_target: {N_target:.1f} (Avg of A={N_A_true}, B={N_B_true})\")\n",
    "\n",
    "\n",
    "# This bit is an artifact and the found t's are actually necesarily the corresponding tau's im very sure\n",
    "def tune_threshold_by_group_count(model_data, N_target, model_name):\n",
    "\tbest_t = 0\n",
    "\tmin_error = float(\"inf\")\n",
    "\tbest_f1 = -1\n",
    "\n",
    "\tfor t in t_range:\n",
    "\t\tcurrent_pairs, current_labels = get_clustering_pairs_and_labels(\n",
    "\t\t\tmodel_data[\"dist_matrix\"], t\n",
    "\t\t)\n",
    "\n",
    "\t\tN_predicted = calculate_n_true(current_labels, P_true)\n",
    "\t\tgroup_error = abs(N_predicted - N_target)\n",
    "\n",
    "\t\ttp = len(current_pairs.intersection(P_true))\n",
    "\t\tfp = len(current_pairs - P_true)\n",
    "\t\tfn = len(P_true - current_pairs)\n",
    "\n",
    "\t\tif tp > 0:\n",
    "\t\t\tprecision = tp / (tp + fp)\n",
    "\t\t\trecall = tp / (tp + fn)\n",
    "\t\t\tf1 = 2 * (precision * recall) / (precision + recall)\n",
    "\t\telse:\n",
    "\t\t\tf1 = 0\n",
    "\n",
    "\t\tif group_error < min_error:\n",
    "\t\t\tmin_error = group_error\n",
    "\t\t\tbest_f1 = f1\n",
    "\t\t\tbest_t = t\n",
    "\t\telif group_error == min_error and f1 > best_f1:\n",
    "\t\t\tbest_f1 = f1\n",
    "\t\t\tbest_t = t\n",
    "\n",
    "\tprint(\n",
    "\t\tf\"[{model_name}] Optimal t: {best_t:.1f} | Group Error: {min_error:.1f} | F1: {best_f1:.4f}\"\n",
    "\t)\n",
    "\treturn best_t\n",
    "\n",
    "\n",
    "optimal_t_A = tune_threshold_by_group_count(Model_A, N_target, \"Model A\")\n",
    "optimal_t_B = tune_threshold_by_group_count(Model_B, N_target, \"Model B\")\n",
    "\n",
    "print(f\"\\nFinal Calibrated Thresholds:\")\n",
    "print(f\"Model A (Statement): {optimal_t_A:.2f}\")\n",
    "print(f\"Model B (Question):  {optimal_t_B:.2f}\")\n",
    "\n",
    "\n",
    "def print_final_clusters(model_data, threshold, string_list, title):\n",
    "\tprint(f\"\\n{'='*80}\")\n",
    "\tprint(f\"FINAL OUTPUT: {title} (Threshold: {threshold:.2f})\")\n",
    "\tprint(f\"{'='*80}\")\n",
    "\n",
    "\tcluster_model = AgglomerativeClustering(\n",
    "\t\tn_clusters=None,\n",
    "\t\tdistance_threshold=threshold,\n",
    "\t\tmetric=\"precomputed\",\n",
    "\t\tlinkage=\"complete\",\n",
    "\t)\n",
    "\tlabels = cluster_model.fit_predict(model_data[\"dist_matrix\"])\n",
    "\n",
    "\tdf = pd.DataFrame(\n",
    "\t\t{\"string\": string_list, \"label\": labels, \"idx\": range(len(string_list))}\n",
    "\t)\n",
    "\tgroups = [g for _, g in df.groupby(\"label\") if len(g) > 1]\n",
    "\tgroups.sort(key=lambda x: len(x), reverse=True)\n",
    "\n",
    "\tfinal_pairs, _ = get_clustering_pairs_and_labels(model_data[\"dist_matrix\"], threshold)\n",
    "\ttp = len(final_pairs.intersection(P_true))\n",
    "\tfp = len(final_pairs - P_true)\n",
    "\n",
    "\tprint(f\"Found {len(groups)} total significant groups.\")\n",
    "\tprint(f\"P_true pairs captured: {tp} (False Positives: {fp})\\n\")\n",
    "\n",
    "\tfor i, group in enumerate(groups):\n",
    "\t\tindices = group[\"idx\"].tolist()\n",
    "\t\tcluster_strs = group[\"string\"].tolist()\n",
    "\n",
    "\t\tvecs = model_data[\"vectors\"][indices]\n",
    "\t\tprec = model_data[\"precision\"]\n",
    "\t\tlocal_mean = np.mean(vecs, axis=0)\n",
    "\n",
    "\t\tdistances = []\n",
    "\t\tmin_dist = float(\"inf\")\n",
    "\t\trep_idx = -1\n",
    "\n",
    "\t\tfor local_i, global_i in enumerate(indices):\n",
    "\t\t\td = mahalanobis(model_data[\"vectors\"][global_i], local_mean, prec)\n",
    "\t\t\tdistances.append(d)\n",
    "\t\t\tif d < min_dist:\n",
    "\t\t\t\tmin_dist = d\n",
    "\t\t\t\trep_idx = local_i\n",
    "\n",
    "\t\tgroup_radius = max(distances)\n",
    "\n",
    "\t\tprint(f\"GROUP {i+1} (Size: {len(indices)}) [Radius: {group_radius:.4f}]\")\n",
    "\n",
    "\t\tfor idx, s in enumerate(cluster_strs):\n",
    "\t\t\tprefix = \" [CENTROID] \" if idx == rep_idx else \"            \"\n",
    "\t\t\tprint(f\"{prefix} {s}\")\n",
    "\t\tprint(\"-\" * 80)\n",
    "\n",
    "\n",
    "print_final_clusters(Model_A, optimal_t_A, strings, \"Model A (Statement Embeddings)\")\n",
    "print_final_clusters(Model_B, optimal_t_B, strings, \"Model B (Question Embeddings)\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.stats import rankdata\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "\n",
    "def get_nearest_neighbor_distances(dist_matrix):\n",
    "\t\"\"\"\n",
    "\tExtracts the distance to the nearest neighbor for every point.\n",
    "\tIgnores the diagonal (0).\n",
    "\t\"\"\"\n",
    "\tnp.fill_diagonal(dist_matrix, float(\"inf\"))\n",
    "\tmin_dists = np.min(dist_matrix, axis=1)\n",
    "\tnp.fill_diagonal(dist_matrix, 0.0)\n",
    "\treturn min_dists\n",
    "\n",
    "\n",
    "def convert_dist_to_prob(dist_matrix, reference_dist_array):\n",
    "\t\"\"\"\n",
    "\tConverts raw distances into Probabilities (P-values) based on the\n",
    "\tEmpirical CDF of the provided reference distribution (Nearest Neighbors).\n",
    "\n",
    "\tP(d) = (Rank of d) / (Total Count + 1)\n",
    "\t\"\"\"\n",
    "\n",
    "\tsorted_refs = np.sort(reference_dist_array)\n",
    "\tn = len(sorted_refs)\n",
    "\n",
    "\tranks = np.searchsorted(sorted_refs, dist_matrix)\n",
    "\n",
    "\tprobs = (ranks + 1) / (n + 1)\n",
    "\n",
    "\treturn probs\n",
    "\n",
    "\n",
    "print(\"--- Calculating Empirical Probabilities (EVT Logic) ---\")\n",
    "\n",
    "\n",
    "nn_dists_A = get_nearest_neighbor_distances(Model_A[\"dist_matrix\"])\n",
    "nn_dists_B = get_nearest_neighbor_distances(Model_B[\"dist_matrix\"])\n",
    "\n",
    "\n",
    "Prob_A = convert_dist_to_prob(Model_A[\"dist_matrix\"], nn_dists_A)\n",
    "Prob_B = convert_dist_to_prob(Model_B[\"dist_matrix\"], nn_dists_B)\n",
    "\n",
    "print(f\"Probabilities Calculated.\")\n",
    "print(\n",
    "\tf\"Example (Model A): Dist=8.8 -> P={np.interp(8.8, np.sort(nn_dists_A), np.linspace(0,1,len(nn_dists_A))):.5f}\"\n",
    ")\n",
    "print(\n",
    "\tf\"Example (Model B): Dist=8.8 -> P={np.interp(8.8, np.sort(nn_dists_B), np.linspace(0,1,len(nn_dists_B))):.5f}\"\n",
    ")\n",
    "\n",
    "\n",
    "Prob_Fused = np.minimum(Prob_A, Prob_B)\n",
    "max_a = getMaxStable(TAU_A, Model_A)\n",
    "max_b = getMaxStable(TAU_B, Model_B)\n",
    "\n",
    "\n",
    "AV_STABLE = (max_a + max_b) / 2\n",
    "print(f\"Max Stable A:{max_a}\")\n",
    "print(f\"Max Stable B:{max_b}\")\n",
    "print(f\"Average Stable Stable A:{AV_STABLE}\")\n",
    "\n",
    "\n",
    "PROB_THRESHOLD = len(P_true) / AV_STABLE\n",
    "# PROB_THRESHOLD = 0.04\n",
    "print(f\"Inferred Probability Threshold: {PROB_THRESHOLD:.6f}\")\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STAGE 2 OUTPUT: Probabilistic Outlier Model (P < {PROB_THRESHOLD:.6f})\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "\n",
    "cluster_model = AgglomerativeClustering(\n",
    "\tn_clusters=None,\n",
    "\tdistance_threshold=PROB_THRESHOLD,\n",
    "\tmetric=\"precomputed\",\n",
    "\tlinkage=\"complete\",\n",
    ")\n",
    "\n",
    "labels = cluster_model.fit_predict(Prob_Fused)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"string\": strings, \"label\": labels, \"idx\": range(len(strings))})\n",
    "groups = [g for _, g in df.groupby(\"label\") if len(g) > 1]\n",
    "groups.sort(key=lambda x: len(x), reverse=True)\n",
    "\n",
    "print(f\"Found {len(groups)} significant groups.\\n\")\n",
    "\n",
    "for i, group in enumerate(groups):\n",
    "\tindices = group[\"idx\"].tolist()\n",
    "\tcluster_strs = group[\"string\"].tolist()\n",
    "\n",
    "\tref_vectors = Model_B[\"vectors\"][indices]\n",
    "\tref_prec = Model_B[\"precision\"]\n",
    "\tlocal_mean = np.mean(ref_vectors, axis=0)\n",
    "\n",
    "\tdists = []\n",
    "\tmin_d = float(\"inf\")\n",
    "\trep_i = -1\n",
    "\n",
    "\tfor loc_i, glob_i in enumerate(indices):\n",
    "\t\td = mahalanobis(Model_B[\"vectors\"][glob_i], local_mean, ref_prec)\n",
    "\t\tdists.append(d)\n",
    "\t\tif d < min_d:\n",
    "\t\t\tmin_d = d\n",
    "\t\t\trep_i = loc_i\n",
    "\n",
    "\tprint(f\"GROUP {i+1} (Size: {len(indices)}) [Radius: {max(dists):.4f}]\")\n",
    "\n",
    "\tfor idx, s in enumerate(cluster_strs):\n",
    "\t\tprefix = \" [CENTROID] \" if idx == rep_i else \"            \"\n",
    "\t\tprint(f\"{prefix} {s}\")\n",
    "\tprint(\"-\" * 80)\n",
    "````\n",
    "#ACGC Iteration\n",
    "```\n",
    "\n",
    "for t_A in tau_range:\n",
    "\tpairs_A, labels_A = cache_A[t_A]\n",
    "\tfor t_B in tau_range:\n",
    "\t\tpairs_B, labels_B = cache_B[t_B]\n",
    "\n",
    "\t\tintersection = pairs_A.intersection(pairs_B)\n",
    "\n",
    "\t\tif len(intersection) > 0:\n",
    "\n",
    "\t\t\tN_A = calculate_n_true(labels_A, intersection)\n",
    "\n",
    "\t\t\tN_B = calculate_n_true(labels_B, intersection)\n",
    "\n",
    "\t\t\tavg_consensus_groups = (N_A + N_B) / 2\n",
    "\n",
    "\t\t\tif avg_consensus_groups > best_metric_score:\n",
    "\t\t\t\tbest_metric_score = avg_consensus_groups\n",
    "\n",
    "\t\t\t\t# Update Best State\n",
    "\t\t\t\tP_true = intersection\n",
    "\t\t\t\tlabels_A_star = labels_A\n",
    "\t\t\t\tlabels_B_star = labels_B\n",
    "\t\t\t\tTAU_A = t_A\n",
    "\t\t\t\tTAU_B = t_B\n",
    "```\n",
    "# Elbow\n",
    "```\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# --- 1. Probability Calculation Helpers ---\n",
    "\n",
    "def get_nearest_neighbor_distances(dist_matrix):\n",
    "    \"\"\"\n",
    "    Extracts the distance to the nearest neighbor for every point.\n",
    "    Ignores the diagonal (0).\n",
    "    \"\"\"\n",
    "    d = dist_matrix.copy()\n",
    "    np.fill_diagonal(d, float(\"inf\"))\n",
    "    return np.min(d, axis=1)\n",
    "\n",
    "def convert_dist_to_prob(dist_matrix, reference_dist_array):\n",
    "    \"\"\"\n",
    "    Converts raw distances into Probabilities (P-values) based on the\n",
    "    Empirical CDF of the provided reference distribution.\n",
    "    \"\"\"\n",
    "    sorted_refs = np.sort(reference_dist_array)\n",
    "    n = len(sorted_refs)\n",
    "    ranks = np.searchsorted(sorted_refs, dist_matrix)\n",
    "    return (ranks + 1) / (n + 1)\n",
    "\n",
    "print(\"\\n--- Phase 2: Probabilistic Intersection & Inference ---\")\n",
    "\n",
    "# Calculate empirical probabilities for both models\n",
    "nn_dists_A = get_nearest_neighbor_distances(Model_A[\"dist_matrix\"])\n",
    "nn_dists_B = get_nearest_neighbor_distances(Model_B[\"dist_matrix\"])\n",
    "\n",
    "Prob_A = convert_dist_to_prob(Model_A[\"dist_matrix\"], nn_dists_A)\n",
    "Prob_B = convert_dist_to_prob(Model_B[\"dist_matrix\"], nn_dists_B)\n",
    "\n",
    "# --- 2. Geometric Mean Fusion (Strict Intersection) ---\n",
    "# We use Sqrt(A * B). This penalizes divergence.\n",
    "# If A=0.001 and B=0.1, Min=0.001 (Accept), Geometric=0.01 (Reject).\n",
    "Prob_Fused = np.sqrt(Prob_A * Prob_B)\n",
    "\n",
    "print(f\"Probabilities Fused (Geometric Mean).\")\n",
    "\n",
    "\n",
    "# --- 3. Topological Elbow Detection (Kneedle) ---\n",
    "# We find the natural \"Bend\" in the distribution of Best Matches.\n",
    "# Duplicates form the flat floor; Neighbors form the rising slope.\n",
    "\n",
    "temp_prob = Prob_Fused.copy()\n",
    "np.fill_diagonal(temp_prob, 1.0) # Ignore self-matches\n",
    "min_probs = np.min(temp_prob, axis=1) # Best match for each item\n",
    "\n",
    "sorted_probs = np.sort(min_probs)\n",
    "x = np.arange(len(sorted_probs))\n",
    "y = sorted_probs\n",
    "\n",
    "# Define the Secant Line\n",
    "p1 = np.array([x[0], y[0]])\n",
    "p2 = np.array([x[-1], y[-1]])\n",
    "vec_line = p2 - p1\n",
    "norm_line = np.linalg.norm(vec_line)\n",
    "\n",
    "# Calculate Perpendicular Distance\n",
    "distances = []\n",
    "for i in range(len(x)):\n",
    "    p0 = np.array([x[i], y[i]])\n",
    "    d = np.abs(np.cross(vec_line, p0 - p1)) / norm_line\n",
    "    distances.append(d)\n",
    "\n",
    "elbow_idx = np.argmax(distances)\n",
    "elbow_value = sorted_probs[elbow_idx]\n",
    "\n",
    "PROB_THRESHOLD = elbow_value\n",
    "\n",
    "print(f\"Elbow Detected at Index: {elbow_idx} / {len(x)}\")\n",
    "print(f\"Inferred Probability Threshold: {PROB_THRESHOLD:.6f}\")\n",
    "\n",
    "\n",
    "# --- 4. Final Clustering & Ordered Output ---\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STAGE 2 OUTPUT: Probabilistic Intersection Model (P < {PROB_THRESHOLD:.6f})\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "cluster_model = AgglomerativeClustering(\n",
    "    n_clusters=None,\n",
    "    distance_threshold=PROB_THRESHOLD,\n",
    "    metric=\"precomputed\",\n",
    "    linkage=\"complete\",\n",
    ")\n",
    "\n",
    "labels = cluster_model.fit_predict(Prob_Fused)\n",
    "\n",
    "df = pd.DataFrame({\"string\": strings, \"label\": labels, \"idx\": range(len(strings))})\n",
    "\n",
    "# Process groups to calculate radius BEFORE printing\n",
    "processed_groups = []\n",
    "raw_groups = [g for _, g in df.groupby(\"label\") if len(g) > 1]\n",
    "\n",
    "for group in raw_groups:\n",
    "    indices = group[\"idx\"].tolist()\n",
    "    cluster_strs = group[\"string\"].tolist()\n",
    "\n",
    "    # Calculate Radius using Model B (as requested in previous snippets)\n",
    "    ref_vectors = Model_B[\"vectors\"][indices]\n",
    "    ref_prec = Model_B[\"precision\"]\n",
    "    local_mean = np.mean(ref_vectors, axis=0)\n",
    "\n",
    "    dists = []\n",
    "    min_d = float(\"inf\")\n",
    "    rep_i = -1\n",
    "\n",
    "    for loc_i, glob_i in enumerate(indices):\n",
    "        d = mahalanobis(Model_B[\"vectors\"][glob_i], local_mean, ref_prec)\n",
    "        dists.append(d)\n",
    "        if d < min_d:\n",
    "            min_d = d\n",
    "            rep_i = loc_i\n",
    "\n",
    "    processed_groups.append({\n",
    "        \"radius\": max(dists),\n",
    "        \"size\": len(indices),\n",
    "        \"strings\": cluster_strs,\n",
    "        \"centroid_idx\": rep_i\n",
    "    })\n",
    "\n",
    "# Sort by Radius Ascending (Tightest first)\n",
    "processed_groups.sort(key=lambda x: x[\"radius\"])\n",
    "\n",
    "print(f\"Found {len(processed_groups)} significant groups.\\n\")\n",
    "\n",
    "for i, g in enumerate(processed_groups):\n",
    "    print(f\"GROUP {i+1} (Size: {g['size']}) [Radius: {g['radius']:.4f}]\")\n",
    "\n",
    "    for idx, s in enumerate(g[\"strings\"]):\n",
    "        prefix = \" [CENTROID] \" if idx == g[\"centroid_idx\"] else \"            \"\n",
    "        print(f\"{prefix} {s}\")\n",
    "    print(\"-\" * 80)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "932bfb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STAGE 3: INFERENTIAL DUPLICATE FILTERING (Topological Elbow Prior)\n",
      "================================================================================\n",
      "Analysis of 520 Probability Pairs:\n",
      "Elbow Index: 96\n",
      "Inferred Threshold: 0.208896\n",
      "Found 60 significant groups.\n",
      "\n",
      "GROUP 1 (Size: 2) [Radius: 2.8865]\n",
      " [CENTROID]  Does the privacy policy affirm that users have the right to request the correction of inaccurate personal data?\n",
      "             Does the privacy policy affirm that users have the right to request the correction of inaccurate data?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 2 (Size: 2) [Radius: 4.0055]\n",
      " [CENTROID]  Does the privacy policy affirm that the company collects the dates and times of access?\n",
      "             Does the privacy policy affirm that the company collects the dates and times of access to the services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 3 (Size: 2) [Radius: 4.2495]\n",
      " [CENTROID]  Does the privacy policy affirm that the company relies on user consent to process contact information for specific marketing communications?\n",
      "             Does the privacy policy affirm that the company processes contact information for marketing communications based on user consent?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 4 (Size: 2) [Radius: 4.3318]\n",
      " [CENTROID]  Does the privacy policy affirm that the company relies on Standard Contractual Clauses (SCCs) for transfers to countries without an adequacy decision?\n",
      "             Does the privacy policy affirm that the company relies on Standard Contractual Clauses (SCCs) for transfers to jurisdictions without adequacy decisions?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 5 (Size: 2) [Radius: 4.3913]\n",
      " [CENTROID]  Does the privacy policy affirm that providing personal data constitutes agreement to the transfer of data outside of Canada?\n",
      "             Does the privacy policy affirm that providing personal data constitutes agreement to the disclosure of data outside of Canada?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 6 (Size: 2) [Radius: 4.6968]\n",
      " [CENTROID]  Does the privacy policy affirm that Personal Data is retained to comply with legal obligations?\n",
      "             Does the privacy policy affirm that Personal Data is used to comply with legal obligations?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 7 (Size: 2) [Radius: 4.7142]\n",
      "             Does the privacy policy affirm that processing contact information to send technical announcements is based on the necessity to perform a contract?\n",
      " [CENTROID]  Does the privacy policy affirm that the company processes contact information to send technical announcements based on the necessity to perform a contract?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 8 (Size: 2) [Radius: 4.7381]\n",
      " [CENTROID]  Does the privacy policy affirm that the company collects the user's time zone?\n",
      "             Does the privacy policy affirm that the company collects time zone settings?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 9 (Size: 2) [Radius: 4.8486]\n",
      " [CENTROID]  Does the privacy policy affirm that users have the statutory right to access their Personal Data?\n",
      "             Does the privacy policy affirm that users have the statutory right to access information relating to how their Personal Data is processed?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 10 (Size: 2) [Radius: 4.8566]\n",
      " [CENTROID]  Does the privacy policy affirm that Inputs and Outputs disassociated via Feedback are used for training models?\n",
      "             Does the privacy policy affirm that Inputs and Outputs disassociated via Feedback are used for improving models?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 11 (Size: 2) [Radius: 5.1103]\n",
      " [CENTROID]  Does the privacy policy affirm that the Data Protection Officer can be contacted via email regarding matters related to Personal Data processing?\n",
      "             Does the privacy policy affirm that users can contact the company's Data Protection Officer via email?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 12 (Size: 2) [Radius: 5.1473]\n",
      " [CENTROID]  Does the privacy policy affirm that the Services are not directed to children under 13?\n",
      "             Does the privacy policy affirm that the Services are not intended for children under 13?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 13 (Size: 2) [Radius: 5.2215]\n",
      " [CENTROID]  Does the privacy policy affirm that security measures are designed to protect personal data from unauthorized access?\n",
      "             Does the privacy policy affirm that security measures are designed to protect personal data from unauthorized disclosure?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 14 (Size: 2) [Radius: 5.3068]\n",
      " [CENTROID]  Does the privacy policy affirm that the company collects the user's name if they communicate via email or social media pages?\n",
      "             Does the privacy policy affirm that the company collects contact information if the user communicates via email or social media pages?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 15 (Size: 2) [Radius: 5.3921]\n",
      " [CENTROID]  Does the privacy policy affirm that administrators of enterprise or business accounts may access and control a user's account?\n",
      "             Does the privacy policy affirm that administrators of enterprise or business accounts may access a user's Content?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 16 (Size: 2) [Radius: 5.4281]\n",
      " [CENTROID]  Does the privacy policy affirm that user email addresses are entrusted to the domestic representative?\n",
      "             Does the privacy policy affirm that user addresses are entrusted to the domestic representative?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 17 (Size: 2) [Radius: 5.5329]\n",
      " [CENTROID]  Does the privacy policy affirm that the company collects the operating system of the device used to access the Services?\n",
      "             Does the privacy policy affirm that the company collects operating system information?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 18 (Size: 2) [Radius: 5.6265]\n",
      " [CENTROID]  Does the privacy policy affirm that the company does not engage in decision-making based solely on automated processing that produces a legal effect?\n",
      "             Does the privacy policy affirm that the company does not engage in decision-making based solely on automated processing that significantly affects the user?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 19 (Size: 2) [Radius: 5.6452]\n",
      " [CENTROID]  Does the privacy policy affirm that user Inputs and Outputs may be used to train the company's models?\n",
      "             Does the privacy policy affirm that user Inputs and Outputs may be used to improve the company's services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 20 (Size: 2) [Radius: 5.6686]\n",
      " [CENTROID]  Does the privacy policy affirm that users can contact the domestic representative via telephone?\n",
      "             Does the privacy policy affirm that users can contact the domestic representative via email?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 21 (Size: 2) [Radius: 5.6878]\n",
      " [CENTROID]  Does the privacy policy affirm that requests regarding inaccurate model output are considered based on applicable law?\n",
      "             Does the privacy policy affirm that requests regarding inaccurate model output are considered based on the technical capabilities of the models?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 22 (Size: 2) [Radius: 5.7074]\n",
      " [CENTROID]  Does the privacy policy affirm that technical information is processed to debug and repair errors that impair existing functionality?\n",
      "             Does the privacy policy affirm that personal data is used to repair errors that impair existing functionality?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 23 (Size: 2) [Radius: 5.7589]\n",
      " [CENTROID]  Does the privacy policy affirm that personal data is used to investigate disputes?\n",
      "             Does the privacy policy affirm that personal data is used to resolve disputes?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 24 (Size: 2) [Radius: 5.7735]\n",
      " [CENTROID]  Does the privacy policy affirm that feedback is processed to improve services and conduct research including model training?\n",
      "             Does the privacy policy affirm that consent is the legal basis for processing user feedback for model training research?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 25 (Size: 2) [Radius: 5.8008]\n",
      "             Does the privacy policy affirm that personal data may be processed in an aggregated form?\n",
      " [CENTROID]  Does the privacy policy affirm that the company may aggregate Personal Data?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 26 (Size: 2) [Radius: 5.8838]\n",
      " [CENTROID]  Does the privacy policy affirm that the company processes Personal Data on servers located outside of the EEA?\n",
      "             Does the privacy policy affirm that the company processes Personal Data on servers located outside of the UK?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 27 (Size: 2) [Radius: 5.9043]\n",
      " [CENTROID]  Does the privacy policy affirm that payment information is processed to facilitate payments for products and services?\n",
      "             Does the privacy policy affirm that personal data is used to facilitate payments for products and services provided by the company?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 28 (Size: 2) [Radius: 5.9236]\n",
      " [CENTROID]  Does the privacy policy affirm that the company processes aggregated or de-identified data to conduct research?\n",
      "             Does the privacy policy affirm that Personal Data is used to conduct research?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 29 (Size: 2) [Radius: 6.0266]\n",
      " [CENTROID]  Does the privacy policy affirm that the company performs necessary procedures for handling data when it is no longer required?\n",
      "             Does the privacy policy affirm that service providers perform necessary procedures for handling data when it is no longer required?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 30 (Size: 2) [Radius: 6.0660]\n",
      " [CENTROID]  Does the privacy policy affirm that if a user creates an account using an email address belonging to an employer, the company may share the fact that the user has an account with that employer?\n",
      "             Does the privacy policy affirm that if a user creates an account using an email address belonging to an employer, the company may share account information with the employer to enable the user to be added to a business account?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 31 (Size: 2) [Radius: 6.0733]\n",
      "             Does the privacy policy affirm that users have the right to object to the processing of their Personal Data when the processing is based on legitimate interests?\n",
      " [CENTROID]  Does the privacy policy affirm that users have the right to object to the processing of their personal data?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 32 (Size: 3) [Radius: 6.1075]\n",
      "             Does the privacy policy affirm that the company trains its models using data provided by users?\n",
      "             Does the privacy policy affirm that user-provided Content may be used to train the company's models?\n",
      " [CENTROID]  Does the privacy policy affirm that personal data is used to train the company's models?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 33 (Size: 2) [Radius: 6.1303]\n",
      " [CENTROID]  Does the privacy policy affirm that the provided supplemental disclosures apply specifically to residents of Canada?\n",
      "             Does the privacy policy affirm that if a conflict arises between the main policy and the supplemental disclosures, the supplemental disclosures prevail for residents of Canada?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 34 (Size: 2) [Radius: 6.1620]\n",
      " [CENTROID]  Does the privacy policy affirm that Personal Data is used to provide the company's services?\n",
      "             Does the privacy policy affirm that Personal Data is used to maintain the company's services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 35 (Size: 2) [Radius: 6.1846]\n",
      " [CENTROID]  Does the privacy policy affirm that users can delete individual conversations?\n",
      "             Does the privacy policy affirm that deleted individual conversations are removed immediately from conversation history?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 36 (Size: 2) [Radius: 6.1901]\n",
      " [CENTROID]  Does the privacy policy affirm that personal data may be processed in a de-identified form?\n",
      "             Does the privacy policy affirm that the company will maintain de-identified information in de-identified form?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 37 (Size: 2) [Radius: 6.2328]\n",
      " [CENTROID]  Does the privacy policy affirm that the company uses Standard Contractual Clauses to transfer information to certain affiliates?\n",
      "             Does the privacy policy affirm that the company uses Standard Contractual Clauses to transfer information to third parties?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 38 (Size: 2) [Radius: 6.2346]\n",
      " [CENTROID]  Does the privacy policy affirm that the company trains its models using publicly available information from the Internet?\n",
      "             Does the privacy policy affirm that the company collects publicly available information from the internet to develop the models powering its Services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 39 (Size: 2) [Radius: 6.2395]\n",
      "             Does the privacy policy affirm that the company may continue to process data despite a request for deletion to comply with legal obligations?\n",
      " [CENTROID]  Does the privacy policy affirm that the company may continue to process data despite a request for deletion to safeguard and exercise rights?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 40 (Size: 2) [Radius: 6.2482]\n",
      " [CENTROID]  Does the privacy policy affirm that personal data is used to provide products and services related to the user's account?\n",
      "             Does the privacy policy affirm that personal data is used to maintain products and services related to the user's account?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 41 (Size: 2) [Radius: 6.2706]\n",
      " [CENTROID]  Does the privacy policy affirm that the company has designated a domestic representative for data protection purposes in the Republic of Korea?\n",
      "             Does the privacy policy affirm that the domestic representative is located in South Korea?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 42 (Size: 2) [Radius: 6.2830]\n",
      " [CENTROID]  Does the privacy policy affirm that users have the right to lodge a complaint with the supervisory authority in the place where they live?\n",
      "             Does the privacy policy affirm that users have the right to lodge a complaint with the supervisory authority in the place where they work?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 43 (Size: 2) [Radius: 6.2905]\n",
      "             Does the privacy policy affirm that Personal Data is used to send information about services and events?\n",
      " [CENTROID]  Does the privacy policy affirm that personal data is used to send information about events?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 44 (Size: 2) [Radius: 6.3103]\n",
      " [CENTROID]  Does the privacy policy affirm that user names are entrusted to the domestic representative?\n",
      "             Does the privacy policy affirm that user IDs are entrusted to the domestic representative?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 45 (Size: 3) [Radius: 6.3322]\n",
      " [CENTROID]  Does the privacy policy affirm that Personal Data is used to protect the rights, privacy, safety, or property of users?\n",
      "             Does the privacy policy affirm that Personal Data is used to protect the rights, privacy, safety, or property of the company?\n",
      "             Does the privacy policy affirm that Personal Data is used to protect the rights, privacy, safety, or property of third parties?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 46 (Size: 2) [Radius: 6.4450]\n",
      " [CENTROID]  Does the privacy policy affirm that users have the right to know the categories of personal data processed about them?\n",
      "             Does the privacy policy affirm that users have the right to know the categories of third parties to whom their data is disclosed?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 47 (Size: 2) [Radius: 6.5512]\n",
      " [CENTROID]  Does the privacy policy affirm that users can contact the company to obtain a copy of the appropriate safeguards in place for data transfers?\n",
      "             Does the privacy policy affirm that users can contact the company via email regarding data transfer safeguards?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 48 (Size: 3) [Radius: 6.5820]\n",
      "             Does the privacy policy affirm that the company may disclose personal data to governmental regulatory authorities as required by law?\n",
      " [CENTROID]  Does the privacy policy affirm that the company may disclose personal data in response to requests from governmental regulatory authorities?\n",
      "             Does the privacy policy affirm that the company may disclose personal data to assist in investigations by governmental regulatory authorities?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 49 (Size: 2) [Radius: 6.6204]\n",
      " [CENTROID]  Does the privacy policy affirm that users should contact the company via email if they believe a child under 13 has provided Personal Data to the company?\n",
      "             Does the privacy policy affirm that users should contact the company via email if they become aware that a child under 18 has provided personal data?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 50 (Size: 2) [Radius: 6.6492]\n",
      " [CENTROID]  Does the privacy policy affirm that personal data may be disclosed to enforce the company's legal rights?\n",
      "             Does the privacy policy affirm that personal data may be disclosed to enforce the legal rights of others?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 51 (Size: 2) [Radius: 6.7168]\n",
      " [CENTROID]  Does the privacy policy affirm that Personal Data is used to prevent illegal activity?\n",
      "             Does the privacy policy affirm that personal data is used to prevent unlawful or criminal activity?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 52 (Size: 2) [Radius: 6.7325]\n",
      " [CENTROID]  Does the privacy policy affirm that data transfer methods to the representative include telephone?\n",
      "             Does the privacy policy affirm that data transfer methods to the representative include email?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 53 (Size: 3) [Radius: 6.8415]\n",
      "             Does the privacy policy affirm that users have the statutory right to withdraw consent at any time where consent is the legal basis for processing?\n",
      " [CENTROID]  Does the privacy policy affirm that users have the right to withdraw consent where processing is based on consent?\n",
      "             Does the privacy policy affirm that users have the right to withdraw their consent?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 54 (Size: 2) [Radius: 6.9004]\n",
      " [CENTROID]  Does the privacy policy affirm that it describes practices regarding Personal Data collected when using the company's website?\n",
      "             Does the privacy policy affirm that it describes practices regarding Personal Data collected when using the company's services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 55 (Size: 3) [Radius: 6.9058]\n",
      "             Does the privacy policy affirm that the company does not knowingly collect information from children under the age of 18?\n",
      " [CENTROID]  Does the privacy policy affirm that the company does not knowingly disclose information from children under the age of 18?\n",
      "             Does the privacy policy affirm that the company does not knowingly share information from children under the age of 18?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 56 (Size: 3) [Radius: 7.4186]\n",
      " [CENTROID]  Does the privacy policy affirm that the company implements commercially reasonable technical measures to protect Personal Data?\n",
      "             Does the privacy policy affirm that the company implements commercially reasonable administrative measures to protect Personal Data?\n",
      "             Does the privacy policy affirm that the company implements commercially reasonable organizational measures to protect Personal Data?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 57 (Size: 2) [Radius: 7.4255]\n",
      " [CENTROID]  Does the privacy policy affirm that users have the right to request information regarding public entities with which the company has shared their data?\n",
      "             Does the privacy policy affirm that users have the right to request information regarding private entities with which the company has shared their data?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 58 (Size: 3) [Radius: 7.4855]\n",
      "             Does the privacy policy affirm that technical information is processed to investigate and resolve security issues?\n",
      " [CENTROID]  Does the privacy policy affirm that personal data is used to investigate security issues?\n",
      "             Does the privacy policy affirm that personal data is used to resolve security issues?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 59 (Size: 3) [Radius: 8.2910]\n",
      "             Does the privacy policy affirm that Personal Data is used to analyze the company's services?\n",
      " [CENTROID]  Does the privacy policy affirm that Personal Data is used to improve the company's services?\n",
      "             Does the privacy policy affirm that Personal Data is used to develop the company's services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 60 (Size: 3) [Radius: 8.3172]\n",
      "             Does the privacy policy affirm that personal data may be shared with service providers for research purposes?\n",
      " [CENTROID]  Does the privacy policy affirm that personal data may be shared with service providers for data processing purposes?\n",
      "             Does the privacy policy affirm that personal data may be shared with service providers for the purpose of providing services to the user?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p9/7lnzfn4x6vl70kl41kn6k6l80000gn/T/ipykernel_10762/941658681.py:113: DeprecationWarning: Arrays of 2-dimensional vectors are deprecated. Use arrays of 3-dimensional vectors instead. (deprecated in NumPy 2.0)\n",
      "  d = np.abs(np.cross(vec_line, p0 - p1)) / norm_line\n"
     ]
    }
   ],
   "source": [
    "# print(f\"\\n{'='*80}\")\n",
    "# print(f\"STAGE 3: INFERENTIAL DUPLICATE FILTERING (Topological Elbow Prior)\")\n",
    "# print(f\"{'='*80}\")\n",
    "\n",
    "# # --- 1. Find the Elbow in the Probability Curve ---\n",
    "# # We look at the distribution of the \"Best Match\" for every item.\n",
    "# # Duplicates create a flat floor; Neighbors create a rising slope.\n",
    "# temp_prob = Prob_Fused.copy()\n",
    "# np.fill_diagonal(temp_prob, 1.0)\t# Ignore self-matches\n",
    "# min_probs = np.min(temp_prob, axis=1)\n",
    "\n",
    "# sorted_probs = np.sort(min_probs)\n",
    "# x = np.arange(len(sorted_probs))\n",
    "# y = sorted_probs\n",
    "\n",
    "# # Define the Secant Line (Start to End)\n",
    "# p1 = np.array([x[0], y[0]])\n",
    "# p2 = np.array([x[-1], y[-1]])\n",
    "# vec_line = p2 - p1\n",
    "# norm_line = np.linalg.norm(vec_line)\n",
    "\n",
    "# # Calculate Perpendicular Distance for every point\n",
    "# distances = []\n",
    "# for i in range(len(x)):\n",
    "# \tp0 = np.array([x[i], y[i]])\n",
    "# \t# Cross product (2D) / Norm\n",
    "# \td = np.abs(np.cross(vec_line, p0 - p1)) / norm_line\n",
    "# \tdistances.append(d)\n",
    "\n",
    "# # The Elbow is the point of maximum curvature (max distance from line)\n",
    "# elbow_idx = np.argmax(distances)\n",
    "# elbow_value = sorted_probs[elbow_idx]\n",
    "\n",
    "# PROB_THRESHOLD = elbow_value\n",
    "\n",
    "# print(f\"Analysis of {len(sorted_probs)} Probability Pairs:\")\n",
    "# print(f\"Elbow Index: {elbow_idx}\")\n",
    "# print(f\"Inferred Threshold: {PROB_THRESHOLD:.6f}\")\n",
    "\n",
    "\n",
    "# # --- 2. Clustering with Inferred Threshold ---\n",
    "\n",
    "# cluster_model = AgglomerativeClustering(\n",
    "# \tn_clusters=None,\n",
    "# \tdistance_threshold=PROB_THRESHOLD,\n",
    "# \tmetric=\"precomputed\",\n",
    "# \tlinkage=\"complete\",\n",
    "# )\n",
    "\n",
    "# labels = cluster_model.fit_predict(Prob_Fused)\n",
    "\n",
    "# df = pd.DataFrame({\"string\": strings, \"label\": labels, \"idx\": range(len(strings))})\n",
    "# groups = [g for _, g in df.groupby(\"label\") if len(g) > 1]\n",
    "# groups.sort(key=lambda x: len(x), reverse=True)\n",
    "\n",
    "# print(f\"Found {len(groups)} significant groups.\\n\")\n",
    "\n",
    "# for i, group in enumerate(groups):\n",
    "# \tindices = group[\"idx\"].tolist()\n",
    "# \tcluster_strs = group[\"string\"].tolist()\n",
    "\n",
    "# \t# Calculate Radius using Model B (as per your previous preference)\n",
    "# \tref_vectors = Model_B[\"vectors\"][indices]\n",
    "# \tref_prec = Model_B[\"precision\"]\n",
    "# \tlocal_mean = np.mean(ref_vectors, axis=0)\n",
    "\n",
    "# \tdists = []\n",
    "# \tmin_d = float(\"inf\")\n",
    "# \trep_i = -1\n",
    "\n",
    "# \tfor loc_i, glob_i in enumerate(indices):\n",
    "# \t\td = mahalanobis(Model_B[\"vectors\"][glob_i], local_mean, ref_prec)\n",
    "# \t\tdists.append(d)\n",
    "# \t\tif d < min_d:\n",
    "# \t\t\tmin_d = d\n",
    "# \t\t\trep_i = loc_i\n",
    "\n",
    "# \tprint(f\"GROUP {i+1} (Size: {len(indices)}) [Radius: {max(dists):.4f}]\")\n",
    "\n",
    "# \tfor idx, s in enumerate(cluster_strs):\n",
    "# \t\tprefix = \" [CENTROID] \" if idx == rep_i else \"            \"\n",
    "# \t\tprint(f\"{prefix} {s}\")\n",
    "# \tprint(\"-\" * 80)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STAGE 3: INFERENTIAL DUPLICATE FILTERING (Topological Elbow Prior)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# # --- 1. Find the Elbow in the Probability Curve ---\n",
    "# temp_prob = Prob_Fused.copy()\n",
    "# np.fill_diagonal(temp_prob, 1.0)\n",
    "# min_probs = np.min(temp_prob, axis=1)\n",
    "\n",
    "# Create the product matrix first\n",
    "Prob_Product = np.sqrt(Prob_A * Prob_B)\n",
    "\n",
    "# Then find the best match for each row (ignoring self-match)\n",
    "temp_prob = Prob_Product.copy()\n",
    "np.fill_diagonal(temp_prob, 1.0)\n",
    "min_probs = np.min(temp_prob, axis=1)\t# finding the best *product* score\n",
    "sorted_probs = np.sort(min_probs)\n",
    "x = np.arange(len(sorted_probs))\n",
    "y = sorted_probs\n",
    "\n",
    "p1 = np.array([x[0], y[0]])\n",
    "p2 = np.array([x[-1], y[-1]])\n",
    "vec_line = p2 - p1\n",
    "norm_line = np.linalg.norm(vec_line)\n",
    "\n",
    "distances = []\n",
    "for i in range(len(x)):\n",
    "\tp0 = np.array([x[i], y[i]])\n",
    "\td = np.abs(np.cross(vec_line, p0 - p1)) / norm_line\n",
    "\tdistances.append(d)\n",
    "\n",
    "elbow_idx = np.argmax(distances)\n",
    "elbow_value = sorted_probs[elbow_idx]\n",
    "\n",
    "PROB_THRESHOLD = elbow_value\n",
    "\n",
    "print(f\"Analysis of {len(sorted_probs)} Probability Pairs:\")\n",
    "print(f\"Elbow Index: {elbow_idx}\")\n",
    "print(f\"Inferred Threshold: {PROB_THRESHOLD:.6f}\")\n",
    "\n",
    "\n",
    "# --- 2. Clustering & Radius Calculation ---\n",
    "\n",
    "cluster_model = AgglomerativeClustering(\n",
    "\tn_clusters=None,\n",
    "\tdistance_threshold=PROB_THRESHOLD,\n",
    "\tmetric=\"precomputed\",\n",
    "\tlinkage=\"complete\",\n",
    ")\n",
    "\n",
    "labels = cluster_model.fit_predict(Prob_Fused)\n",
    "\n",
    "df = pd.DataFrame({\"string\": strings, \"label\": labels, \"idx\": range(len(strings))})\n",
    "\n",
    "# We must pre-process all groups to calculate their radius *before* printing\n",
    "processed_groups = []\n",
    "\n",
    "raw_groups = [g for _, g in df.groupby(\"label\") if len(g) > 1]\n",
    "\n",
    "for group in raw_groups:\n",
    "\tindices = group[\"idx\"].tolist()\n",
    "\tcluster_strs = group[\"string\"].tolist()\n",
    "\n",
    "\t# Calculate Radius using Model B (Consistent with previous steps)\n",
    "\tref_vectors = Model_B[\"vectors\"][indices]\n",
    "\tref_prec = Model_B[\"precision\"]\n",
    "\tlocal_mean = np.mean(ref_vectors, axis=0)\n",
    "\n",
    "\tdists = []\n",
    "\tmin_d = float(\"inf\")\n",
    "\trep_i = -1\n",
    "\n",
    "\tfor loc_i, glob_i in enumerate(indices):\n",
    "\t\td = mahalanobis(Model_B[\"vectors\"][glob_i], local_mean, ref_prec)\n",
    "\t\tdists.append(d)\n",
    "\t\tif d < min_d:\n",
    "\t\t\tmin_d = d\n",
    "\t\t\trep_i = loc_i\n",
    "\n",
    "\tgroup_radius = max(dists)\n",
    "\n",
    "\tprocessed_groups.append(\n",
    "\t\t{\n",
    "\t\t\t\"radius\": group_radius,\n",
    "\t\t\t\"size\": len(indices),\n",
    "\t\t\t\"strings\": cluster_strs,\n",
    "\t\t\t\"centroid_idx\": rep_i,\n",
    "\t\t}\n",
    "\t)\n",
    "\n",
    "# --- 3. Sort by Radius (Smallest/Tightest First) ---\n",
    "processed_groups.sort(key=lambda x: x[\"radius\"])\n",
    "\n",
    "print(f\"Found {len(processed_groups)} significant groups.\\n\")\n",
    "\n",
    "for i, g in enumerate(processed_groups):\n",
    "\tprint(f\"GROUP {i+1} (Size: {g['size']}) [Radius: {g['radius']:.4f}]\")\n",
    "\n",
    "\tfor idx, s in enumerate(g[\"strings\"]):\n",
    "\t\tprefix = \" [CENTROID] \" if idx == g[\"centroid_idx\"] else \"            \"\n",
    "\t\tprint(f\"{prefix} {s}\")\n",
    "\tprint(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c780013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from scipy.spatial.distance import cdist, mahalanobis\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from itertools import combinations\n",
    "\n",
    "# ==========================================\n",
    "# PRE-PROCESSING & MODEL ARTIFACTS\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "def prepare_model_artifacts(raw_vector_list, name=\"Model\"):\n",
    "\tprint(f\"Processing {name}...\")\n",
    "\tdata = np.array(raw_vector_list)\n",
    "\tdata_trunc = data[:, :256]\n",
    "\tnorms = np.linalg.norm(data_trunc, axis=1, keepdims=True)\n",
    "\tcleaned_vectors = data_trunc / (norms + 1e-10)\n",
    "\n",
    "\tlw = LedoitWolf()\n",
    "\tlw.fit(cleaned_vectors)\n",
    "\tprecision_matrix = lw.precision_\n",
    "\n",
    "\tdist_matrix = cdist(\n",
    "\t\tcleaned_vectors, cleaned_vectors, metric=\"mahalanobis\", VI=precision_matrix\n",
    "\t)\n",
    "\n",
    "\treturn {\n",
    "\t\t\"dist_matrix\": dist_matrix,\n",
    "\t\t\"vectors\": cleaned_vectors,\n",
    "\t\t\"precision\": precision_matrix,\n",
    "\t}\n",
    "\n",
    "\n",
    "# --- DATA LOADING (Assumes 'qdata' exists in your scope) ---\n",
    "data_set = qdata\n",
    "strings = list(data_set.keys())\n",
    "\n",
    "raw_vectors_A = [data_set[s][\"embedding_vector\"] for s in strings]\n",
    "raw_vectors_B = [data_set[s][\"retrieval_embedding_vector\"] for s in strings]\n",
    "\n",
    "Model_A = prepare_model_artifacts(raw_vectors_A, \"Model A (Statement)\")\n",
    "Model_B = prepare_model_artifacts(raw_vectors_B, \"Model B (Question)\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "def get_pairs_from_labels(labels):\n",
    "\t\"\"\"Converts cluster labels into a Set of unique pairs (indices).\"\"\"\n",
    "\tdf = pd.DataFrame({\"label\": labels, \"id\": range(len(labels))})\n",
    "\tpairs = set()\n",
    "\tfor label, group in df.groupby(\"label\"):\n",
    "\t\tindices = group[\"id\"].tolist()\n",
    "\t\tif len(indices) > 1:\n",
    "\t\t\tfor p in combinations(sorted(indices), 2):\n",
    "\t\t\t\tpairs.add(p)\n",
    "\treturn pairs\n",
    "\n",
    "\n",
    "def get_clustering_pairs_and_labels(dist_matrix, threshold):\n",
    "\t\"\"\"Runs clustering and returns both the pair set and the labels array.\"\"\"\n",
    "\tmodel = AgglomerativeClustering(\n",
    "\t\tn_clusters=None,\n",
    "\t\tdistance_threshold=threshold,\n",
    "\t\tmetric=\"precomputed\",\n",
    "\t\tlinkage=\"complete\",\n",
    "\t)\n",
    "\tlabels = model.fit_predict(dist_matrix)\n",
    "\treturn get_pairs_from_labels(labels), labels\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# PHASE 1: CONSENSUS GENERATION (EXCESS AGREEMENT)\n",
    "# ==========================================\n",
    "print(\"\\n--- Phase 1: Grid Search for Consensus (Excess Agreement) ---\")\n",
    "\n",
    "dist_A_off = Model_A[\"dist_matrix\"][np.triu_indices_from(Model_A[\"dist_matrix\"], k=1)]\n",
    "dist_B_off = Model_B[\"dist_matrix\"][np.triu_indices_from(Model_B[\"dist_matrix\"], k=1)]\n",
    "all_dists = np.concatenate([dist_A_off, dist_B_off])\n",
    "\n",
    "D_min = np.min(all_dists)\n",
    "D_max = np.max(all_dists)\n",
    "SEARCH_STEP = 0.05\n",
    "\n",
    "TAU_SEARCH_START = max(D_min, 0.0)\n",
    "TAU_SEARCH_END = D_max\n",
    "\n",
    "tau_range = np.arange(TAU_SEARCH_START, TAU_SEARCH_END, SEARCH_STEP)\n",
    "print(f\"Search Range: [{TAU_SEARCH_START:.2f} to {TAU_SEARCH_END:.2f}]\")\n",
    "\n",
    "# Pre-compute Clusters\n",
    "cache_A = {}\n",
    "cache_B = {}\n",
    "print(f\"Pre-computing clusters for {len(tau_range)} thresholds...\")\n",
    "for t in tau_range:\n",
    "\tcache_A[t] = get_clustering_pairs_and_labels(Model_A[\"dist_matrix\"], t)\n",
    "\tcache_B[t] = get_clustering_pairs_and_labels(Model_B[\"dist_matrix\"], t)\n",
    "\n",
    "# Calculate Total Possible Pairs for Expectation\n",
    "N_samples = Model_A[\"dist_matrix\"].shape[0]\n",
    "TOTAL_POSSIBLE_PAIRS = (N_samples * (N_samples - 1)) / 2\n",
    "\n",
    "best_score = -float(\"inf\")\n",
    "P_true = set()\n",
    "labels_A_star = None\n",
    "labels_B_star = None\n",
    "TAU_A = 0\n",
    "TAU_B = 0\n",
    "\n",
    "for t_A in tau_range:\n",
    "\tpairs_A, labels_A = cache_A[t_A]\n",
    "\tn_A = len(pairs_A)\n",
    "\n",
    "\tfor t_B in tau_range:\n",
    "\t\tpairs_B, labels_B = cache_B[t_B]\n",
    "\t\tn_B = len(pairs_B)\n",
    "\n",
    "\t\tintersection = pairs_A.intersection(pairs_B)\n",
    "\t\tn_obs = len(intersection)\n",
    "\n",
    "\t\t# Expected Agreement (Random Chance Baseline)\n",
    "\t\tif TOTAL_POSSIBLE_PAIRS > 0:\n",
    "\t\t\tn_exp = (n_A * n_B) / TOTAL_POSSIBLE_PAIRS\n",
    "\t\telse:\n",
    "\t\t\tn_exp = 0\n",
    "\n",
    "\t\t# METRIC: Excess Agreement\n",
    "\t\tscore = n_obs - n_exp\n",
    "\n",
    "\t\tif score > best_score:\n",
    "\t\t\tbest_score = score\n",
    "\t\t\tP_true = intersection\n",
    "\t\t\tlabels_A_star = labels_A\n",
    "\t\t\tlabels_B_star = labels_B\n",
    "\t\t\tTAU_A = t_A\n",
    "\t\t\tTAU_B = t_B\n",
    "\n",
    "print(f\"\\nOptimization Complete.\")\n",
    "print(f\"Best Excess Agreement Score: {best_score:.4f}\")\n",
    "print(f\"Optimal Taus -> Model A: {TAU_A:.2f}, Model B: {TAU_B:.2f}\")\n",
    "\n",
    "# Count TOTAL clusters (including singletons) for Phase 3\n",
    "# len(set(labels)) gives the total number of unique groups found (including noise/singletons)\n",
    "count_clusters_A = len(set(labels_A_star)) if labels_A_star is not None else 0\n",
    "count_clusters_B = len(set(labels_B_star)) if labels_B_star is not None else 0\n",
    "N_total_clusters_C_prime = (count_clusters_A + count_clusters_B) / 2\n",
    "\n",
    "print(f\"Platinum Pairs Identified: {len(P_true)}\")\n",
    "print(f\"Total Cluster Universe (C_prime): {N_total_clusters_C_prime:.1f}\")\n",
    "\n",
    "print(\"A:\")\n",
    "print(labels_A_star)\n",
    "print(labels_B_star)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# PHASE 2: CALIBRATION OUTPUT\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "def print_final_clusters(model_data, threshold, string_list, title):\n",
    "\tprint(f\"\\n{'='*80}\")\n",
    "\tprint(f\"FINAL OUTPUT: {title} (Threshold: {threshold:.2f})\")\n",
    "\tprint(f\"{'='*80}\")\n",
    "\n",
    "\tcluster_model = AgglomerativeClustering(\n",
    "\t\tn_clusters=None,\n",
    "\t\tdistance_threshold=threshold,\n",
    "\t\tmetric=\"precomputed\",\n",
    "\t\tlinkage=\"complete\",\n",
    "\t)\n",
    "\tlabels = cluster_model.fit_predict(model_data[\"dist_matrix\"])\n",
    "\n",
    "\tdf = pd.DataFrame(\n",
    "\t\t{\"string\": string_list, \"label\": labels, \"idx\": range(len(string_list))}\n",
    "\t)\n",
    "\tgroups = [g for _, g in df.groupby(\"label\") if len(g) > 1]\n",
    "\tgroups.sort(key=lambda x: len(x), reverse=True)\n",
    "\n",
    "\tfinal_pairs, _ = get_clustering_pairs_and_labels(model_data[\"dist_matrix\"], threshold)\n",
    "\ttp = len(final_pairs.intersection(P_true))\n",
    "\tfp = len(final_pairs - P_true)\n",
    "\n",
    "\tprint(f\"Found {len(groups)} total significant groups.\")\n",
    "\tprint(f\"P_true pairs captured: {tp} (False Positives: {fp})\\n\")\n",
    "\n",
    "\tfor i, group in enumerate(groups):\n",
    "\t\tindices = group[\"idx\"].tolist()\n",
    "\t\tcluster_strs = group[\"string\"].tolist()\n",
    "\n",
    "\t\tvecs = model_data[\"vectors\"][indices]\n",
    "\t\tprec = model_data[\"precision\"]\n",
    "\t\tlocal_mean = np.mean(vecs, axis=0)\n",
    "\n",
    "\t\tdistances = []\n",
    "\t\tmin_dist = float(\"inf\")\n",
    "\t\trep_idx = -1\n",
    "\n",
    "\t\tfor local_i, global_i in enumerate(indices):\n",
    "\t\t\td = mahalanobis(model_data[\"vectors\"][global_i], local_mean, prec)\n",
    "\t\t\tdistances.append(d)\n",
    "\t\t\tif d < min_dist:\n",
    "\t\t\t\tmin_dist = d\n",
    "\t\t\t\trep_idx = local_i\n",
    "\n",
    "\t\tgroup_radius = max(distances)\n",
    "\n",
    "\t\tprint(f\"GROUP {i+1} (Size: {len(indices)}) [Radius: {group_radius:.4f}]\")\n",
    "\n",
    "\t\tfor idx, s in enumerate(cluster_strs):\n",
    "\t\t\tprefix = \" [CENTROID] \" if idx == rep_idx else \"            \"\n",
    "\t\t\tprint(f\"{prefix} {s}\")\n",
    "\t\tprint(\"-\" * 80)\n",
    "\n",
    "\n",
    "print_final_clusters(Model_A, TAU_A, strings, \"Model A (Statement Embeddings)\")\n",
    "print_final_clusters(Model_B, TAU_B, strings, \"Model B (Question Embeddings)\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# PHASE 3: PROBABILISTIC MODEL (INFERRING MISSES)\n",
    "# ==========================================\n",
    "print(\"\\n--- Calculating Empirical Probabilities (EVT Logic) ---\")\n",
    "\n",
    "\n",
    "def get_nearest_neighbor_distances(dist_matrix):\n",
    "\td_mat = dist_matrix.copy()\n",
    "\tnp.fill_diagonal(d_mat, float(\"inf\"))\n",
    "\tmin_dists = np.min(d_mat, axis=1)\n",
    "\treturn min_dists\n",
    "\n",
    "\n",
    "def convert_dist_to_prob(dist_matrix, reference_dist_array):\n",
    "\tsorted_refs = np.sort(reference_dist_array)\n",
    "\tn = len(sorted_refs)\n",
    "\tranks = np.searchsorted(sorted_refs, dist_matrix)\n",
    "\tprobs = (ranks + 1) / (n + 1)\n",
    "\treturn probs\n",
    "\n",
    "\n",
    "nn_dists_A = get_nearest_neighbor_distances(Model_A[\"dist_matrix\"])\n",
    "nn_dists_B = get_nearest_neighbor_distances(Model_B[\"dist_matrix\"])\n",
    "\n",
    "Prob_A = convert_dist_to_prob(Model_A[\"dist_matrix\"], nn_dists_A)\n",
    "Prob_B = convert_dist_to_prob(Model_B[\"dist_matrix\"], nn_dists_B)\n",
    "\n",
    "Prob_Fused = np.minimum(Prob_A, Prob_B)\n",
    "\n",
    "# --- THRESHOLD CALCULATION ---\n",
    "# Correct Logic: Pairs (P_true) / Total Universe Clusters (C_prime)\n",
    "if N_total_clusters_C_prime > 0:\n",
    "\tPROB_THRESHOLD = len(P_true) / N_total_clusters_C_prime\n",
    "else:\n",
    "\tPROB_THRESHOLD = 0.0\n",
    "\n",
    "print(f\"Inferred Probability Threshold: {PROB_THRESHOLD:.6f}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STAGE 3 OUTPUT: Probabilistic Outlier Model (P < {PROB_THRESHOLD:.6f})\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "cluster_model = AgglomerativeClustering(\n",
    "\tn_clusters=None,\n",
    "\tdistance_threshold=PROB_THRESHOLD,\n",
    "\tmetric=\"precomputed\",\n",
    "\tlinkage=\"complete\",\n",
    ")\n",
    "\n",
    "labels = cluster_model.fit_predict(Prob_Fused)\n",
    "\n",
    "df = pd.DataFrame({\"string\": strings, \"label\": labels, \"idx\": range(len(strings))})\n",
    "groups = [g for _, g in df.groupby(\"label\") if len(g) > 1]\n",
    "groups.sort(key=lambda x: len(x), reverse=True)\n",
    "\n",
    "print(f\"Found {len(groups)} significant groups.\\n\")\n",
    "\n",
    "for i, group in enumerate(groups):\n",
    "\tindices = group[\"idx\"].tolist()\n",
    "\tcluster_strs = group[\"string\"].tolist()\n",
    "\n",
    "\tref_vectors = Model_B[\"vectors\"][indices]\n",
    "\tref_prec = Model_B[\"precision\"]\n",
    "\tlocal_mean = np.mean(ref_vectors, axis=0)\n",
    "\n",
    "\tdists = []\n",
    "\tmin_d = float(\"inf\")\n",
    "\trep_i = -1\n",
    "\n",
    "\tfor loc_i, glob_i in enumerate(indices):\n",
    "\t\td = mahalanobis(Model_B[\"vectors\"][glob_i], local_mean, ref_prec)\n",
    "\t\tdists.append(d)\n",
    "\t\tif d < min_d:\n",
    "\t\t\tmin_d = d\n",
    "\t\t\trep_i = loc_i\n",
    "\n",
    "\tprint(f\"GROUP {i+1} (Size: {len(indices)}) [Radius: {max(dists):.4f}]\")\n",
    "\n",
    "\tfor idx, s in enumerate(cluster_strs):\n",
    "\t\tprefix = \" [CENTROID] \" if idx == rep_i else \"            \"\n",
    "\t\tprint(f\"{prefix} {s}\")\n",
    "\tprint(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
