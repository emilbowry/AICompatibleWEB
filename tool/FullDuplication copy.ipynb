{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb7eadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a refined and expanded specification document. It integrates the formalized notation, the detailed breakdown of distance metrics (Mahalanobis vs. Cosine), and the domain-adaptive lexical strategies (SentencePiece) discussed in the previous steps.\n",
    "\n",
    "# ---\n",
    "\n",
    "\n",
    "# To infer equivalence rigorously, we move from loose terminology to specific random variables and sets.\n",
    "\n",
    "# -   **Hypothesis ($H_{eq}$)**: A binary random variable where $H_{eq}(i,j)=1$ implies question $i$ and question $j$ are semantically and legally equivalent.\n",
    "# -   **The View Set ($V$)**: The set of all embedding configurations (e.g., `BERT_CLS`, `GEMINI_RETRIEVAL`).\n",
    "# -   **The Metric Set ($M$)**: The set of geometric interpretations of distance: $M = \\{\\text{COSINE}, \\text{EUCLIDEAN}, \\text{MAHALANOBIS}\\}$.\n",
    "# -   **Configuration ($C$)**: A tuple $(v, m)$ representing a specific view and metric combination.\n",
    "# -   **Raw Distance ($d_{v,m}(i,j)$)**: The scalar distance between vectors of $i$ and $j$ under configuration $(v, m)$.\n",
    "# -   **Normalized Distance ($\\phi_{v,m}(d)$)**: The Cumulative Distribution Function (CDF) value of a distance $d$ within configuration $(v,m)$. This maps raw distances to a percentile $p \\in [0,1]$, allowing comparison across different metrics.\n",
    "# -   **Lexical Sequence ($T_i$)**: The sequence of integer token IDs produced by a domain-specific tokenizer (SentencePiece) trained on the **Policy Corpus**.\n",
    "# -   **Information Weight ($W(t)$)**: The self-information (surprisal) of a token $t$, defined as $-\\log P(t)$ from the tokenizer's unigram language model.\n",
    "\n",
    "# ---\n",
    "\n",
    "\n",
    "# We extend the input data types to support metric-specific caching and tokenization data.\n",
    "\n",
    "# ```typescript\n",
    "# // 1.1 Core Identifiers\n",
    "# type TQuestionString = `Does the privacy policy affirm that ${string}?`;\n",
    "# type TViewKey = string; // e.g., 'gemini_retrieval_query'\n",
    "# type TMetricType = 'COSINE' | 'EUCLIDEAN' | 'MAHALANOBIS';\n",
    "\n",
    "# // 1.2 Grounding Data (Origin)\n",
    "# interface IGrounding {\n",
    "#     document_hash: string;\n",
    "#     subsection_hash: string;\n",
    "#     substring_indices: [number, number]; // [Start, End]\n",
    "#     supporting_text: string;\n",
    "# }\n",
    "\n",
    "# // 1.3 Computed Distance Cache\n",
    "# // Keyed by View + Metric to allow geometric specific lookups\n",
    "# interface IDistanceData {\n",
    "#     [view_key: TViewKey]: {\n",
    "#         [metric: TMetricType]: {\n",
    "#             [question_hash: string]: Array<{\n",
    "#                 target_hash: string;\n",
    "#                 raw_distance: number;       // d(i,j)\n",
    "#                 percentile_rank: number;    // phi(d(i,j)) - Normalized 0-1\n",
    "#                 rank_index: number;         // Integer rank (1st NN, 2nd NN...)\n",
    "#             }>\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# // 1.4 Lexical Data (SentencePiece)\n",
    "# interface ILexicalData {\n",
    "#     [question_hash: string]: {\n",
    "#         token_ids: number[];        // The integer sequence\n",
    "#         token_weights: number[];    // -log(P(t)) for each token\n",
    "#         special_tokens: {           // Boolean flags for hard filters\n",
    "#             has_negation: boolean;\n",
    "#             has_temporal_condition: boolean; // e.g., \"72 hours\", \"30 days\"\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "# ```\n",
    "\n",
    "# ---\n",
    "\n",
    "\n",
    "# These are the inputs for $P(H_{eq} | \\textrm{factors})$. We categorize them by the signal source.\n",
    "\n",
    "# #\n",
    "# *Measures derived from the vector space geometry of a specific View ($v$) and Metric ($m$).*\n",
    "\n",
    "# | ID | Factor Name | Computation | Rationale |\n",
    "# | :--- | :--- | :--- | :--- |\n",
    "# | **G1** | **Normalized Proximity** | $\\phi_{v,m}(d(i,j))$ | Using percentiles allows us to compare \"closeness\" between Mahalanobis (ellipsoid) and Cosine (cone) spaces. |\n",
    "# | **G2** | **Lowe’s Ratio** | $d(i,j) / d(i, \\text{NN}_2)$ | Measures distinctiveness. If $j$ is much closer than the next neighbor, it's a specific match, not just a topic cluster. |\n",
    "# | **G3** | **Local Density ($\\rho$)** | $k / \\text{Vol}(m, d(i, \\text{NN}_k))$ | The density of the vector space around $i$. High density regions (common legal boilerplate) require stricter thresholds than sparse regions. |\n",
    "\n",
    "# #\n",
    "# *Measures derived from the neighbor graph, robust to absolute distance scaling.*\n",
    "\n",
    "# | ID | Factor Name | Computation | Rationale |\n",
    "# | :--- | :--- | :--- | :--- |\n",
    "# | **T1** | **Mutual NN ($k$-RNN)** | $\\mathbb{I}(j \\in \\text{NN}_k(i) \\land i \\in \\text{NN}_k(j))$ | Reciprocity is a strong filter for asymmetric relationships (entailment vs equivalence). |\n",
    "# | **T2** | **Shared Neighborhood** | Jaccard($\\text{NN}_k(i), \\text{NN}_k(j)$) | If $i$ and $j$ see the same \"world\" (neighbors), they likely occupy the same semantic point. |\n",
    "# | **T3** | **Cluster Stability** | Membership in optimal **ACCC** or **Jaccard** clusters. | Pre-computed cluster membership acts as a high-confidence prior. |\n",
    "\n",
    "# #\n",
    "# *Measures derived from the agreement across different embedding models.*\n",
    "\n",
    "# | ID | Factor Name | Computation | Rationale |\n",
    "# | :--- | :--- | :--- | :--- |\n",
    "# | **C1** | **View Support** | $\\frac{1}{|V|} \\sum \\mathbb{I}(j \\in \\text{NN}_{k}(i \\mid v))$ | The fraction of models that agree $j$ is relevant to $i$. Noise tends to be uncorrelated across models. |\n",
    "# | **C2** | **Rank Variance** | $\\text{Var}(\\{ \\text{rank}_v(i \\to j) \\})$ | Low variance implies a robust semantic link. High variance suggests the link is an artifact of one specific model's training. |\n",
    "\n",
    "# #\n",
    "# *Measures derived from the **Policy-Trained SentencePiece** tokenizer. This acts as a \"High-Pass Filter\" for precise legal terminology.*\n",
    "\n",
    "# | ID | Factor Name | Computation | Rationale |\n",
    "# | :--- | :--- | :--- | :--- |\n",
    "# | **L1** | **Info-Weighted Overlap** | $\\frac{\\sum_{t \\in T_i \\cap T_j} W(t)}{\\sum_{t \\in T_i \\cup T_j} W(t)}$ | A Jaccard index that ignores stop-words (low $W$) and prioritizes legal terms (high $W$) naturally. |\n",
    "# | **L2** | **Token Edit Distance** | Levenshtein($T_i, T_j$) | Distinguishes structural changes. \"Directed to\" vs \"Intended for\" may appear as high-cost substitutions if trained on policies. |\n",
    "# | **L3** | **Hard Negation Delta** | $\\text{XOR}(\\text{has\\_negation}_i, \\text{has\\_negation}_j)$ | Hard veto. \"Do we share?\" vs \"Do we not share?\" are semantically close vectors but legally opposite. |\n",
    "\n",
    "# #\n",
    "# *Measures derived from the source document mapping.*\n",
    "\n",
    "# | ID | Factor Name | Computation | Rationale |\n",
    "# | :--- | :--- | :--- | :--- |\n",
    "# | **O1** | **Exact Substring Match** | $\\exists doc: \\text{Sub}(i) \\equiv \\text{Sub}(j)$ | If both questions map to the exact same string in the same policy, $P(H_{eq}) \\approx 1$. (The \"Anchor\"). |\n",
    "# | **O2** | **Index Overlap (IoU)** | $\\text{IoU}(\\text{Indices}_i, \\text{Indices}_j)$ | Partial overlap suggests strong correlation. |\n",
    "\n",
    "# ---\n",
    "\n",
    "\n",
    "# We explicitly define the geometries to prevent invalid comparisons.\n",
    "\n",
    "# ##\n",
    "# -   **Shape:** Directional cones radiating from the origin.\n",
    "# -   **Radius:** Angular divergence.\n",
    "# -   **Blind Spot:** Magnitude. Two vectors can be identical in direction but represent different \"intensities\" (though standard embedding usage normalizes this).\n",
    "\n",
    "# ##\n",
    "# -   **Shape:** An ellipsoid defined by the covariance matrix $\\Sigma_v$ of the view.\n",
    "# -   **Radius:** Statistical Distance ($\\sigma$). A distance of $1.0$ means the point is 1 standard deviation away from the centroid relative to the local correlation.\n",
    "# -   **Truncation:** We assume truncated Mahalanobis vectors (e.g., 256 dims) represent the principal components. Distances here are cleaner than full-dimension Euclidean distances because they discard low-variance (noise) dimensions.\n",
    "\n",
    "# ##\n",
    "# When calculating **ACCC** or clustering thresholds $\\tau$:\n",
    "# -   $\\text{Vol}_{\\text{cos}}(\\tau) \\propto \\sin(\\tau)^{D-1}$\n",
    "# -   $\\text{Vol}_{\\text{mah}}(\\tau) \\propto \\tau^D \\sqrt{\\det(\\Sigma)}$\n",
    "# -   **Implication:** We cannot use a fixed scalar threshold $\\tau$ across metrics. We must use the **Normalized Proximity (G1)** to select dynamic thresholds that represent equivalent statistical likelihoods.\n",
    "\n",
    "# ---\n",
    "\n",
    "\n",
    "# #\n",
    "# Since we lack labeled $Y$, we define a proxy label $Y'$:\n",
    "# $$ Y'_{ij} = 1 \\iff \\text{Factor O1 (Exact Substring)} \\text{ is True for } > 1 \\text{ Document} $$\n",
    "# We assume $P(H_{eq} | Y'=1) \\approx 1$. We can use this subset to calibrate the weights of geometric factors for the ungrounded pairs.\n",
    "\n",
    "# #\n",
    "# -   **Embeddings are Low-Pass Filters:** They smooth over minor syntactic variations to capture intent. They excel at recall but fail at precision (e.g., \"Children under 13\" $\\approx$ \"Children under 18\").\n",
    "# -   **Lexical (SP) is a High-Pass Filter:** It captures precise symbol differences but fails at intent.\n",
    "# -   **Inference Rule:** $P(H_{eq})$ is maximized when geometric similarity is high **AND** lexical weighted overlap is high. High geometric similarity with low lexical overlap indicates a **Hard Negative** (e.g., \"Directed\" vs \"Intended\").\n",
    "\n",
    "# #\n",
    "# We assume that noise in the **SentencePiece** tokenization (symbolic) is independent of noise in the **Embedding** generation (semantic).\n",
    "# Therefore, strict agreement between **L2 (Token Edit Distance)** and **C1 (View Support)** implies true equivalence, as accidental collision in both spaces simultaneously is statistically improbable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daad00d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Dict, List, Tuple, NewType, Literal, Union\n",
    "\n",
    "QuestionString = NewType(\"QuestionString\", str)\n",
    "QuestionHash = NewType(\"QuestionHash\", str)\n",
    "DocumentHash = NewType(\"DocumentHash\", str)\n",
    "SubsectionHash = NewType(\"SubsectionHash\", str)\n",
    "ViewKey = NewType(\"ViewKey\", str)\n",
    "\n",
    "MetricType = Literal[\"COSINE\", \"EUCLIDEAN\", \"MAHALANOBIS\"]\n",
    "TaskType = Literal[\n",
    "\t\"SEMANTIC_SIMILARITY\", \"FACT_VERIFICATION\", \"RETRIEVAL_QUERY\", \"RETRIEVAL_DOCUMENT\"\n",
    "]\n",
    "\n",
    "\n",
    "StartIndex = int\n",
    "EndIndex = int\n",
    "SubstringIndices = Tuple[StartIndex, EndIndex]\n",
    "\n",
    "\n",
    "class EvidenceSnippet(TypedDict):\n",
    "\t\"\"\"\n",
    "\tSource text location data.\n",
    "\t\"\"\"\n",
    "\n",
    "\tsupporting_substring: str\n",
    "\tsubstring_indices: SubstringIndices\n",
    "\n",
    "\n",
    "SubsectionData = Dict[SubsectionHash, List[EvidenceSnippet]]\n",
    "\n",
    "\n",
    "PolicyMap = Dict[DocumentHash, SubsectionData]\n",
    "\n",
    "\n",
    "class QuestionData(TypedDict, total=False):\n",
    "\tpolicy_data: PolicyMap\n",
    "\n",
    "\n",
    "InputData = Dict[QuestionString, QuestionData]\n",
    "\n",
    "\n",
    "class SpecialTokens(TypedDict):\n",
    "\thas_negation: bool\n",
    "\thas_temporal_condition: bool\n",
    "\tis_legal_boilerplate: bool\n",
    "\n",
    "\n",
    "class LexicalEntry(TypedDict):\n",
    "\ttoken_ids: List[int]\n",
    "\ttoken_weights: List[float]\n",
    "\tspecial_tokens: SpecialTokens\n",
    "\n",
    "\n",
    "LexicalData = Dict[QuestionHash, LexicalEntry]\n",
    "\n",
    "\n",
    "class NeighborNode(TypedDict):\n",
    "\ttarget_hash: QuestionHash\n",
    "\traw_distance: float\n",
    "\tpercentile_rank: float\n",
    "\trank_index: int\n",
    "\n",
    "\n",
    "DistanceCache = Dict[ViewKey, Dict[MetricType, Dict[QuestionHash, List[NeighborNode]]]]\n",
    "\n",
    "\n",
    "class InferenceFactors(TypedDict):\n",
    "\t\"\"\"\n",
    "\tThe computed vector of factors for a specific candidate pair (i, j).\n",
    "\t\"\"\"\n",
    "\n",
    "\t# --- Geometric (Metric Dependent) ---\n",
    "\tgeo_normalized_proximity: float\t# G1: phi(d)\n",
    "\tgeo_lowe_ratio: float\t# G2: d(NN1) / d(NN2)\n",
    "\tgeo_local_density: float\t# G3: Density estimate around i\n",
    "\n",
    "\t# --- Topological (Graph Structure) ---\n",
    "\ttop_mutual_nn: bool\t# T1: i is NN of j AND j is NN of i\n",
    "\ttop_shared_neighbor_iou: float\t# T2: Jaccard overlap of NN sets\n",
    "\ttop_cluster_stability: bool\t# T3: Co-membership in ACCC/Jaccard clusters\n",
    "\n",
    "\t# --- Consensus (Multi-View) ---\n",
    "\tcon_view_support: float\t# C1: % of views agreeing on relationship\n",
    "\tcon_rank_variance: float\t# C2: Variance of rank(j) across views\n",
    "\n",
    "\t# --- Lexical (Domain-Adaptive) ---\n",
    "\tlex_info_weighted_overlap: float\t# L1: Weighted Jaccard\n",
    "\tlex_token_edit_distance: float\t# L2: SP Token Levenshtein\n",
    "\tlex_hard_negation_delta: bool\t# L3: XOR of negation presence\n",
    "\n",
    "\t# --- Origin (Grounding) ---\n",
    "\t# UPDATED: Replaced Overlap (Float) with Identity (Bool)\n",
    "\torg_text_equivalence: bool\t# O1: Text(i) == Text(j) (Content Match)\n",
    "\torg_location_identity: (\n",
    "\t\tbool\t# O2: Doc(i)==Doc(j) AND Idx(i)==Idx(j) (Strict Reference Match)\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d72fbac",
   "metadata": {},
   "source": [
    "# I will recompute the vectors more concretely\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82924fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "100 // 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d95ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_FILE = \"/data/questions_filter_after.json\"\n",
    "\n",
    "\n",
    "def _saveJson(filepath, data):\n",
    "\twith open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "\t\tjson.dump(data, f, indent=4, default=str)\n",
    "\n",
    "\n",
    "def _loadJson(filepath):\n",
    "\tif not os.path.exists(filepath):\n",
    "\t\tprint(f\"Warning: File not found: {filepath}\")\n",
    "\t\treturn {}\n",
    "\ttry:\n",
    "\t\twith open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "\t\t\treturn json.load(f)\n",
    "\texcept json.JSONDecodeError:\n",
    "\t\tprint(f\"Error decoding JSON: {filepath}\")\n",
    "\t\treturn {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244ad631",
   "metadata": {},
   "outputs": [],
   "source": [
    "101 // 50\n",
    "\n",
    "101 % 50\n",
    "\n",
    "# 101/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70ac70b",
   "metadata": {},
   "source": [
    "I have a collection of questions, that follow the grammar: `Does the privacy policy affirm that {X}`, where `X` is some variable statement.\n",
    "I have some embedding model M, which can compute the embeddings for a string given 8 different `task_types`.\n",
    "I additionally want to compute the embeddings for some manipulations of the string:\n",
    "\n",
    "\n",
    "```python\n",
    "EMBEDDING_MODELS = [\"gemini-embedding-001\"]\n",
    "TASK_TYPES = [\n",
    "\t\"SEMANTIC_SIMILARITY\",\n",
    "\t\"CLASSIFICATION\",\n",
    "\t\"CLUSTERING\",\n",
    "\t\"RETRIEVAL_DOCUMENT\",\n",
    "\t\"RETRIEVAL_QUERY\",\n",
    "\t\"CODE_RETRIEVAL_QUERY\",\n",
    "\t\"QUESTION_ANSWERING\",\n",
    "\t\"FACT_VERIFICATION\",\n",
    "]\n",
    "\n",
    "# Core Question\n",
    "CORE_PREFIX = (\n",
    "\t\"Does the privacy policy affirm that \"\t# Default and form of the data-source\n",
    ")\n",
    "# -- alt prefixes start here --\n",
    "CORE_STATEMENT = \"The privacy policy affirms that \"\n",
    "\n",
    "# NEGATION\n",
    "NEGATION_PREFIX = \"Does the privacy policy not affirm that \"\n",
    "NEGATION_TO_STATEMENT = \"The privacy policy does not affirm that \"\n",
    "\n",
    "# THE CONTRARY\n",
    "\n",
    "CONTRARY_PREFIX = \"Does the privacy policy deny that \"\n",
    "CONTRARY_STATEMENT = \"The privacy policy denies that \"\n",
    "\n",
    "# THE NEGATION OF THE CONTRARY\n",
    "NEG_CONTRARY_PREFIX = \"Does the privacy policy not deny that \"\n",
    "NEG_CONTRARY_STATEMENT = \"The privacy policy does not deny that \"\n",
    "\n",
    "\n",
    "def changePrefix(question, replacement):\n",
    "\tprocessed_text = replacement + question[len(CORE_PREFIX) :]\n",
    "\treturn processed_text\n",
    "\n",
    "\n",
    "def questionToStatement(question, replacement):\n",
    "\tprocessed_text = (replacement + question[len(CORE_PREFIX) :])[:-1] + \".\"\n",
    "\treturn processed_text\n",
    "\n",
    "\n",
    "# also for extraction of purely X: This should only be ran once per statement\n",
    "def propositionExtraction(question):\n",
    "\tprocessed_text = (question[len(CORE_PREFIX) :].upper())[:-1] + \".\"\n",
    "\treturn processed_text\n",
    "```\n",
    "\n",
    "- Per all_variations for single embedding_type: 1*8*520 =4680\n",
    "- Total: 1*9*8*520 = 37440\n",
    "\n",
    "Given our dimensions is `3072` we do not need to truncate the vectors if we run it one test across all embedding mutations. The full length is also already normalised.\n",
    "\n",
    "We shall use a truncation of `768` a dataset including all mutations of the question.\n",
    "Then as before for the single variation we will use `256` as we have done before.\n",
    "\n",
    "We also want to compute the embedding for associated `supporting_substring` each `TASK_TYPE` in `TASK_TYPES`, just incase we want to expand our sources for comparison.\n",
    "\n",
    "I think we may then need to adjust our data structue, I think we should retain the `JSON` for identification, i.e using the original, just without the `TEmbeddingData`:\n",
    "```\n",
    "interface IData {\n",
    "\t[question_string: TQuestionString]: {\n",
    "\t\tpolicy_data: {\n",
    "\t\t\t[document_hash: string]: {\n",
    "\t\t\t\t[subsection_hash: string]: {\n",
    "\t\t\t\t\tsupporting_substring: string;\n",
    "\t\t\t\t\tsubstring_indices: [TStartIndex, TEndIndex];\n",
    "\t\t\t\t}[];\n",
    "\t\t\t};\n",
    "\t\t};\n",
    "\t};\n",
    "}\n",
    "```\n",
    "However the format of our embedding data will have to change to be reasonable. I do not want to use a database since I am just testing out this locally. I think npz should be sufficient for now. Lets use a seperate npz file for each TASK_TYPES, and a seperate file for supporting strings but keep all question manipulations in the file. We will key our arrays in the npz file by the string we used, since theyre guarenteed to remain unique.\n",
    "\n",
    "The processing of `supporting_strings` since it isnt 1-1, and our core task is about data analysis of the questions. I think we will leave processing of the distance matrices until later.\n",
    "\n",
    "I've had a go at writing the code, but commented out the actual api call so i dont end up calling it unecessary:\n",
    "```\n",
    "from model_management import GeminiModel\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "\n",
    "MODEL_NAME = EMBEDDING_MODELS[0]\n",
    "def preprocessQuestionData(question_data):\n",
    "\toutput_dict = {}\n",
    "\tfor k in question_data:\n",
    "\t\tstatement = propositionExtraction(k)\n",
    "\t\tnegation = changePrefix(k, NEGATION_PREFIX)\n",
    "\t\tnegation_statement = questionToStatement(k, NEGATION_TO_STATEMENT)\n",
    "\t\tcontrary = changePrefix(k, CONTRARY_PREFIX)\n",
    "\t\tcontrary_statement = questionToStatement(k, CONTRARY_STATEMENT)\n",
    "\t\tneg_contrary = changePrefix(k, NEG_CONTRARY_PREFIX)\n",
    "\t\tneg_contrary_statement = changePrefix(k, NEG_CONTRARY_STATEMENT)\n",
    "\t\toutput_dict[k] = [\n",
    "\t\t\tk,\n",
    "\t\t\tstatement,\n",
    "\t\t\tnegation,\n",
    "\t\t\tnegation_statement,\n",
    "\t\t\tcontrary,\n",
    "\t\t\tcontrary_statement,\n",
    "\t\t\tneg_contrary,\n",
    "\t\t\tneg_contrary_statement,\n",
    "\t\t]\n",
    "\treturn output_dict\n",
    "\n",
    "\n",
    "class ReRunException(Exception):\n",
    "\tpass\n",
    "\n",
    "\n",
    "def reRunGuard(expected_key_length):\n",
    "\tif os.path.isdir(EMBEDDING_STORAGE_DIR):\n",
    "\t\tentries = os.listdir(EMBEDDING_STORAGE_DIR)\n",
    "\t\tif entries:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tloaded = np.load(f\"{EMBEDDING_STORAGE_DIR}/{entries[0]}\")\n",
    "\t\t\t\tif len(loaded.keys()) == expected_key_length:\n",
    "\t\t\t\t\traise ReRunException(\"Preventing Arbitrary Re-processing\")\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(len(loaded.keys()))\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tif isinstance(e, ReRunException):\n",
    "\t\t\t\t\traise e\n",
    "\t\t\t\tpass\n",
    "\n",
    "\n",
    "def processEmbeddings(data, *, MAX_EMBEDDING_STRINGS=95, save_data=True):\n",
    "\n",
    "\tmodel = GeminiModel()\n",
    "\n",
    "\tdef runEmbedding(questions, task_type):\t# needs closure over model\n",
    "\t\treturn questions\n",
    "\t\t# if len(questions) >= MAX_EMBEDDING_STRINGS:\n",
    "\t\t# \traise Exception(f\"More than {MAX_EMBEDDING_STRINGS} inputs\")\n",
    "\n",
    "\t\t# model.client.models.embed_content(\n",
    "\t\t# \tmodel=MODEL_NAME,\n",
    "\t\t# \tcontents=questions,\n",
    "\t\t# \tconfig=types.EmbedContentConfig(task_type=task_type),\n",
    "\t\t# )\n",
    "\t\t# embeddings = [e.values for e in _embeddings.embeddings]\n",
    "\t\t# return embeddings\n",
    "\n",
    "\tmutated_data = preprocessQuestionData(data)\n",
    "\tmutated_keys = list(mutated_data.keys())\n",
    "\tmutations = len(list(mutated_data.values())[0])\n",
    "\n",
    "\ttotal_strings = len(mutated_keys * mutations)\n",
    "\tmax_embedded_strings = min(MAX_EMBEDDING_STRINGS, total_strings)\n",
    "\n",
    "\treRunGuard(total_strings)\n",
    "\n",
    "\titeration_amount = total_strings // max_embedded_strings\n",
    "\n",
    "\tembeddings_futures_dict = {}\n",
    "\tflat_data = []\n",
    "\tfor k, v in mutated_data.items():\n",
    "\t\tflat_data.extend(v)\n",
    "\n",
    "\t# since we have ~500 questions, makes far more sense to run 8 of iterations of 100 questions, plus we save by task type anyways\n",
    "\treshaped_data = []\n",
    "\tfor i in range(iteration_amount + 1):\n",
    "\t\tdata = flat_data[i * max_embedded_strings : max_embedded_strings * (i + 1)]\n",
    "\t\tif data != []:\n",
    "\t\t\treshaped_data.append(data)\n",
    "\tdata_store = {}\n",
    "\tfor t in TASK_TYPES:\n",
    "\t\t# will move to `Gemini Batch API` at some point but that has a higher latence\n",
    "\t\tprint(f\"computing task type {t}\")\n",
    "\t\texecutor = concurrent.futures.ThreadPoolExecutor(max_workers=len(reshaped_data))\n",
    "\t\tquestion_futures = {}\n",
    "\t\tfor i, e in enumerate(reshaped_data):\n",
    "\t\t\tprint(f\"processing batch of {len(e)} strings\")\n",
    "\n",
    "\t\t\tquestion_futures[i] = executor.submit(runEmbedding, e, t)\n",
    "\n",
    "\t\texecutor.shutdown(wait=True)\n",
    "\t\tfor i in range(len(reshaped_data)):\n",
    "\t\t\tif t in data_store:\n",
    "\t\t\t\tdata_store[t].extend(question_futures[i].result())\n",
    "\t\t\telse:\n",
    "\t\t\t\tdata_store[t] = question_futures[i].result()\n",
    "\toutput_data = {}\n",
    "\tfor k, v in data_store.items():\n",
    "\t\tif len(v)!= total_strings:\n",
    "\t\t\traise ValueError(f\"Expected {total_strings} embeddings: Recieved {len(v)}\")\n",
    "\t\toutput_data[k] = dict(zip(flat_data, list([*np.array(v)])))\n",
    "\n",
    "\t# could be done in above loop but keep seperate for now:\n",
    "\tif save_data:\n",
    "\t\tfor k, v in output_data.items():\n",
    "\t\t\tnp.savez_compressed(f\"{EMBEDDING_STORAGE_DIR}/{k}\", **v)\n",
    "\n",
    "\treturn output_data\n",
    "\n",
    "\n",
    "processEmbeddings(test_keys, MAX_EMBEDDING_STRINGS=3)\n",
    "```\n",
    "\n",
    "Any glaring errors?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275aade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stri = \"Hello everyone GDPR\".capitalize()\n",
    "stri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d49ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODELS = [\"models/gemini-embedding-001\"]\n",
    "TASK_TYPES = [\n",
    "\t\"SEMANTIC_SIMILARITY\",\n",
    "\t\"CLASSIFICATION\",\n",
    "\t\"CLUSTERING\",\n",
    "\t\"RETRIEVAL_DOCUMENT\",\n",
    "\t\"RETRIEVAL_QUERY\",\n",
    "\t\"CODE_RETRIEVAL_QUERY\",\n",
    "\t\"QUESTION_ANSWERING\",\n",
    "\t\"FACT_VERIFICATION\",\n",
    "]\n",
    "\n",
    "# Core Question\n",
    "CORE_PREFIX = (\n",
    "\t\"Does the privacy policy affirm that \"\t# Default and form of the data-source\n",
    ")\n",
    "# -- alt prefixes start here --\n",
    "CORE_STATEMENT = \"The privacy policy affirms that \"\n",
    "\n",
    "# NEGATION\n",
    "NEGATION_PREFIX = \"Does the privacy policy not affirm that \"\n",
    "NEGATION_TO_STATEMENT = \"The privacy policy does not affirm that \"\n",
    "\n",
    "# THE CONTRARY\n",
    "\n",
    "CONTRARY_PREFIX = \"Does the privacy policy deny that \"\n",
    "CONTRARY_STATEMENT = \"The privacy policy denies that \"\n",
    "\n",
    "# THE NEGATION OF THE CONTRARY\n",
    "NEG_CONTRARY_PREFIX = \"Does the privacy policy not deny that \"\n",
    "NEG_CONTRARY_STATEMENT = \"The privacy policy does not deny that \"\n",
    "\n",
    "\n",
    "def changePrefix(question, replacement):\n",
    "\tprocessed_text = replacement + question[len(CORE_PREFIX) :]\n",
    "\treturn processed_text\n",
    "\n",
    "\n",
    "def questionToStatement(question, replacement):\n",
    "\tprocessed_text = (replacement + question[len(CORE_PREFIX) :])[:-1] + \".\"\n",
    "\treturn processed_text\n",
    "\n",
    "\n",
    "# also for extraction of purely X: This should only be ran once per statement\n",
    "def propositionExtraction(question):\n",
    "\tcontent = question[len(CORE_PREFIX) :]\n",
    "\tprocessed_text = content[0].upper() + content[1:-1] + \".\"\n",
    "\treturn processed_text\n",
    "\n",
    "\n",
    "def preprocessQuestionData(question_data):\n",
    "\toutput_dict = {}\n",
    "\tfor k in question_data:\n",
    "\t\tstatement = propositionExtraction(k)\n",
    "\t\tnegation = changePrefix(k, NEGATION_PREFIX)\n",
    "\t\tnegation_statement = questionToStatement(k, NEGATION_TO_STATEMENT)\n",
    "\t\tcontrary = changePrefix(k, CONTRARY_PREFIX)\n",
    "\t\tcontrary_statement = questionToStatement(k, CONTRARY_STATEMENT)\n",
    "\t\tneg_contrary = changePrefix(k, NEG_CONTRARY_PREFIX)\n",
    "\t\tneg_contrary_statement = changePrefix(k, NEG_CONTRARY_STATEMENT)\n",
    "\t\toutput_dict[k] = [\n",
    "\t\t\tk,\n",
    "\t\t\tstatement,\n",
    "\t\t\tnegation,\n",
    "\t\t\tnegation_statement,\n",
    "\t\t\tcontrary,\n",
    "\t\t\tcontrary_statement,\n",
    "\t\t\tneg_contrary,\n",
    "\t\t\tneg_contrary_statement,\n",
    "\t\t]\n",
    "\treturn output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82091ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "SOURCE_FILE = \"./data/questions_filter_after.json\"\n",
    "\n",
    "\n",
    "def _saveJson(filepath, data):\n",
    "\twith open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "\t\tjson.dump(data, f, indent=4, default=str)\n",
    "\n",
    "\n",
    "def _loadJson(filepath):\n",
    "\tif not os.path.exists(filepath):\n",
    "\t\tprint(f\"Warning: File not found: {filepath}\")\n",
    "\t\treturn {}\n",
    "\ttry:\n",
    "\t\twith open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "\t\t\treturn json.load(f)\n",
    "\texcept json.JSONDecodeError:\n",
    "\t\tprint(f\"Error decoding JSON: {filepath}\")\n",
    "\t\treturn {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb36039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inlined_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd53ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "inlined_requests = types.EmbedContentBatchDict(\n",
    "\tmodel=\"models/gemini-embedding-001\",\n",
    "\tcontent=types.Content(parts=[types.Part(text=\"Why is the sky blue?\")]),\n",
    "\tconfig={\"task_type\": \"SEMANTIC_SIMILARITY\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52516ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the response\n",
    "for i, inline_response in enumerate(batch_job_inline.dest.inlined_responses, start=1):\n",
    "\tprint(f\"\\n--- Response {i} ---\")\n",
    "\n",
    "\t# Check for a successful response\n",
    "\tif inline_response.response:\n",
    "\t\t# The .text property is a shortcut to the generated text.\n",
    "\t\tprint(inline_response.response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5211832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inline_requests = [\n",
    "\t{\"contents\": [{\"parts\": [{\"text\": \"Tell me a one-sentence joke.\"}], \"role\": \"user\"}]},\n",
    "\t{\"contents\": [{\"parts\": [{\"text\": \"Why is the sky blue?\"}], \"role\": \"user\"}]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98943ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdata = _loadJson(SOURCE_FILE)\n",
    "\n",
    "test_keys = list(qdata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fbee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareModelArtifacts(\n",
    "\tdata_set, vector_keys, truncation_dim=256, distance_metric=\"mahalanobis\", debug=True\n",
    "):\n",
    "\tsemantic_data = list(data_set.keys())\n",
    "\tmodel_artifacts = {}\n",
    "\traw_vectors = {}\n",
    "\tfor key in vector_keys:\n",
    "\t\traw_vectors[key] = [data_set[s][key] for s in semantic_data]\n",
    "\texecutor = concurrent.futures.ThreadPoolExecutor(max_workers=len(vector_keys))\n",
    "\n",
    "\tfutures = dict()\n",
    "\tfor key in vector_keys:\n",
    "\t\tif debug:\n",
    "\t\t\tprint(f\"Processing {key}...\")\n",
    "\n",
    "\t\tfutures[key] = executor.submit(\n",
    "\t\t\t_prepareModelArtifact,\n",
    "\t\t\traw_vectors[key],\n",
    "\t\t\tsemantic_data,\n",
    "\t\t\ttruncation_dim,\n",
    "\t\t\tdistance_metric,\n",
    "\t\t\tdebug,\n",
    "\t\t)\n",
    "\texecutor.shutdown(wait=True)\n",
    "\tfor key in vector_keys:\n",
    "\t\tmodel_artifacts[key] = futures[key].result()\n",
    "\treturn model_artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc73c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.genai import types, errors\n",
    "# import random\n",
    "\n",
    "\n",
    "# def processEmbeddings(data, *, MAX_EMBEDDING_STRINGS=95, save_data=True):\n",
    "\n",
    "# \tmodel = GeminiModel()\n",
    "\n",
    "# \t# def runEmbedding(questions, task_type):\t# needs closure over model\n",
    "# \t# \t# return questions\n",
    "# \t# \tif len(questions) > MAX_EMBEDDING_STRINGS:\n",
    "# \t# \t\traise Exception(f\"More than {MAX_EMBEDDING_STRINGS} inputs, got {len(questions)}\")\n",
    "\n",
    "# \t# \t_embeddings = model.client.models.embed_content(\n",
    "# \t# \t\tmodel=MODEL_NAME,\n",
    "# \t# \t\tcontents=questions,\n",
    "# \t# \t\tconfig=types.EmbedContentConfig(task_type=task_type),\n",
    "# \t# \t)\n",
    "# \t# \tembeddings = [e.values for e in _embeddings.embeddings]\n",
    "# \t# \treturn embeddings\n",
    "# \t# --- 1. ROBUST RETRY LOGIC ---\n",
    "\n",
    "# \tdef runEmbedding(questions, task_type):\n",
    "# \t\t\"\"\"\n",
    "# \t\tWraps the API call with exponential backoff for 429 errors.\n",
    "# \t\t\"\"\"\n",
    "# \t\tretries = 0\n",
    "# \t\tmax_retries = 8\n",
    "# \t\tbase_delay = 2\t# seconds\n",
    "# \t\tstart_delay = 30\t# seconds\n",
    "\n",
    "# \t\tif len(questions) > MAX_EMBEDDING_STRINGS:\n",
    "# \t\t\traise Exception(f\"More than {MAX_EMBEDDING_STRINGS} inputs, got {len(questions)}\")\n",
    "\n",
    "# \t\tbase_jitter = start_delay * random.uniform(0, 1)\n",
    "\n",
    "# \t\ttime.sleep(base_jitter)\n",
    "# \t\twhile True:\n",
    "# \t\t\ttry:\n",
    "# \t\t\t\t# Actual API Call\n",
    "# \t\t\t\t# Note: Adjust access syntax to match your specific wrapper's return type\n",
    "# \t\t\t\tresult = model.client.models.embed_content(\n",
    "# \t\t\t\t\tmodel=MODEL_NAME,\n",
    "# \t\t\t\t\tcontents=questions,\n",
    "# \t\t\t\t\tconfig=types.EmbedContentConfig(task_type=task_type),\n",
    "# \t\t\t\t)\n",
    "# \t\t\t\t# Ensure we extract the list of vectors correctly\n",
    "# \t\t\t\treturn [e.values for e in result.embeddings]\n",
    "\n",
    "# \t\t\texcept errors.ClientError as e:\n",
    "# \t\t\t\t# This catches 429 Too Many Requests\n",
    "\n",
    "# \t\t\t\tif retries >= max_retries:\n",
    "# \t\t\t\t\tprint(f\"Max retries exceeded for batch. Error: {e}\")\n",
    "# \t\t\t\t\traise e\n",
    "# \t\t\t\t# Exponential Backoff + Jitter (to prevent thundering herd)\n",
    "# \t\t\t\tsleep_time = (base_delay * (2**retries)) + random.uniform(0, 1)\n",
    "# \t\t\t\tprint(\n",
    "# \t\t\t\t\tf\"Hit 429. Retrying in {sleep_time:.2f}s... (Attempt {retries+1}/{max_retries})\"\n",
    "# \t\t\t\t)\n",
    "# \t\t\t\ttime.sleep(sleep_time)\n",
    "# \t\t\t\tretries += 1\n",
    "# \t\t\texcept Exception as e:\n",
    "\n",
    "# \t\t\t\t# Catch other potential errors (500s, 503s)\n",
    "# \t\t\t\tprint(f\"Unexpected error: {e}\")\n",
    "# \t\t\t\tif retries >= max_retries:\n",
    "# \t\t\t\t\traise e\n",
    "# \t\t\t\ttime.sleep(5)\n",
    "\n",
    "# \t\t\t\tretries += 1\n",
    "\n",
    "# \tmutated_data = preprocessQuestionData(data)\n",
    "# \tmutated_keys = list(mutated_data.keys())\n",
    "# \tmutations = len(list(mutated_data.values())[0])\n",
    "\n",
    "# \ttotal_strings = len(mutated_keys * mutations)\n",
    "# \tmax_embedded_strings = min(MAX_EMBEDDING_STRINGS, total_strings)\n",
    "\n",
    "# \t# reRunGuard(total_strings)\n",
    "\n",
    "# \titeration_amount = total_strings // max_embedded_strings\n",
    "\n",
    "# \tembeddings_futures_dict = {}\n",
    "# \tflat_data = []\n",
    "# \tfor k, v in mutated_data.items():\n",
    "# \t\tflat_data.extend(v)\n",
    "# \t# return flat_data\n",
    "# \t# since we have ~500 questions, makes far more sense to run 8 of iterations of 100 questions, plus we save by task type anyways\n",
    "# \treshaped_data = []\n",
    "# \tfor i in range(iteration_amount + 1):\n",
    "# \t\tdata = flat_data[i * max_embedded_strings : max_embedded_strings * (i + 1)]\n",
    "# \t\tif data != []:\n",
    "# \t\t\treshaped_data.append(data)\n",
    "# \tdata_store = {}\n",
    "# \tfor t in TASK_TYPES:\n",
    "# \t\t# will move to `Gemini Batch API` at some point but that has a higher latence\n",
    "# \t\tprint(f\"computing task type {t}\")\n",
    "# \t\texecutor = concurrent.futures.ThreadPoolExecutor(max_workers=8)\n",
    "# \t\tquestion_futures = {}\n",
    "# \t\tfor i, e in enumerate(reshaped_data):\n",
    "# \t\t\tprint(f\"processing batch of {len(e)} strings\")\n",
    "\n",
    "# \t\t\tquestion_futures[i] = executor.submit(runEmbedding, e, t)\n",
    "\n",
    "# \t\texecutor.shutdown(wait=True)\n",
    "# \t\tfor i in range(len(reshaped_data)):\n",
    "# \t\t\tif t in data_store:\n",
    "# \t\t\t\tdata_store[t].extend(question_futures[i].result())\n",
    "# \t\t\telse:\n",
    "# \t\t\t\tdata_store[t] = question_futures[i].result()\n",
    "# \toutput_data = {}\n",
    "# \tfor k, v in data_store.items():\n",
    "# \t\tif len(v) != total_strings:\n",
    "# \t\t\traise ValueError(f\"Expected {total_strings} embeddings: Recieved {len(v)}\")\n",
    "# \t\toutput_data[k] = dict(zip(flat_data, list([*np.array(v)])))\n",
    "\n",
    "# \t# could be done in above loop but keep seperate for now:\n",
    "# \tif save_data:\n",
    "# \t\tfor k, v in output_data.items():\n",
    "# \t\t\tnp.savez_compressed(f\"{EMBEDDING_STORAGE_DIR}/{k}\", **v)\n",
    "\n",
    "# \treturn output_data\n",
    "\n",
    "\n",
    "# processEmbeddings(test_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2305c10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez_compressed(f\"{EMBEDDING_STORAGE_DIR}/SEMANTIC_SIMILARITY\", **test_dict)\n",
    "import numpy as np\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "EMBEDDING_STORAGE_DIR = \"./embedding_storage\"\n",
    "\n",
    "loaded = np.load(f\"{EMBEDDING_STORAGE_DIR}/SEMANTIC_SIMILARITY.npz\")\n",
    "KEYS = [\n",
    "\t\"CLASSIFICATION\",\n",
    "\t\"CLUSTERING\",\n",
    "\t\"RETRIEVAL_DOCUMENT\",\n",
    "\t\"RETRIEVAL_QUERY\",\n",
    "\t\"CODE_RETRIEVAL_QUERY\",\n",
    "\t\"QUESTION_ANSWERING\",\n",
    "\t\"FACT_VERIFICATION\",\n",
    "]\n",
    "# len(list(loaded.keys()))\n",
    "mat = [v for k, v in loaded.items()]\n",
    "\n",
    "arr = np.array(mat)[:, :768]\n",
    "norms = np.linalg.norm(arr, axis=1, keepdims=True)\n",
    "normalised = arr / norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b8d5b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.covariance import LedoitWolf\n",
    "import concurrent.futures\n",
    "\n",
    "executor = concurrent.futures.ThreadPoolExecutor(max_workers=7)\n",
    "\n",
    "KEYS = [\n",
    "\t\"CLASSIFICATION\",\n",
    "\t\"CLUSTERING\",\n",
    "\t\"RETRIEVAL_DOCUMENT\",\n",
    "\t\"RETRIEVAL_QUERY\",\n",
    "\t\"CODE_RETRIEVAL_QUERY\",\n",
    "\t\"QUESTION_ANSWERING\",\n",
    "\t\"FACT_VERIFICATION\",\n",
    "]\n",
    "\n",
    "\n",
    "def processMal(key):\n",
    "\n",
    "\tloaded = np.load(f\"{EMBEDDING_STORAGE_DIR}/{key}.npz\")\n",
    "\n",
    "\tmat = [v for k, v in loaded.items()]\n",
    "\n",
    "\tarr = np.array(mat)[:, :768]\n",
    "\tnorms = np.linalg.norm(arr, axis=1, keepdims=True)\n",
    "\tnormalised = arr / norms\n",
    "\n",
    "\tlw = LedoitWolf()\n",
    "\n",
    "\tlw.fit(normalised)\n",
    "\tprecision_matrix = lw.precision_\n",
    "\n",
    "\tdist_matrix = cdist(normalised, normalised, metric=\"mahalanobis\", VI=precision_matrix)\n",
    "\tnp.savez_compressed(f\"./distances/{key}_mahalanobis_lw\", **{key: dist_matrix})\n",
    "\treturn dist_matrix\n",
    "\n",
    "\n",
    "matrix_futures = {}\n",
    "for key in KEYS:\n",
    "\tmatrix_futures[key] = executor.submit(processMal, key)\n",
    "executor.shutdown(wait=True)\n",
    "save_res = {}\n",
    "for k in matrix_futures:\n",
    "\tsave_res[k] = matrix_futures[k].result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e4659f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "072920b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = cdist(normalised, normalised, metric=\"mahalanobis\", VI=precision_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6e1652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savez_compressed(f\"./distances/mahalanobis_lw\", SEMANTIC_SIMILARITY=dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf9d13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.savez_compressed(f\"./distances/test\", b=np.array((1, 2)))\n",
    "np.savez_compressed(f\"./distances/test\", **{\"a\": np.array((1, 2))})\n",
    "\n",
    "\n",
    "loaded = np.load(f\"./distances/test.npz\")\n",
    "loaded[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6332ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_rows = arr.shape[0]\n",
    "D = arr.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071ce72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = np.empty((N_rows, N_rows), dtype=normalised.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c869703",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd2846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i_start in range(0, N_rows, block_size):\n",
    "\n",
    "# \ti_end = min(i_start + block_size, N_rows)\n",
    "\n",
    "# \tBlock_A = normalised[i_start:i_end, :]\n",
    "\n",
    "# \tfor j_start in range(0, N_rows, block_size):\n",
    "# \t\tj_end = min(j_start + block_size, N_rows)\n",
    "\n",
    "# \t\tBlock_B = normalised[j_start:j_end, :]\n",
    "\n",
    "# \t\tdiff_block = Block_A[:, None, :] - Block_B[None, :, :]\n",
    "\n",
    "# \t\tVI_diff = diff_block @ precision_matrix\n",
    "\n",
    "# \t\tmahalanobis_sq_block = np.sum(diff_block * VI_diff, axis=-1)\n",
    "\n",
    "# \t\tdist_block = np.sqrt(mahalanobis_sq_block)\n",
    "# \t\tdist_matrix[i_start:i_end, j_start:j_end] = dist_block\n",
    "# \t\tprint(j_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4bc1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff_matrix = normalised[:, None, :] - normalised[None, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd62a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VI_diff = diff_matrix @ precision_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72a2b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "mahalanobis_sq_matrix = np.sum(diff_matrix * VI_diff, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = np.sqrt(mahalanobis_sq_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957098e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = np.maximum(dist_matrix, 0.0)\n",
    "\n",
    "# return dist_matrix, precision_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd7aa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684f7fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead8294",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "def getMahalanobisDistances_Custom(vectors_a, vectors_b):\n",
    "\n",
    "\tnorms = np.linalg.norm(vectors_a, axis=1, keepdims=True)\n",
    "\tcleaned_vectors = vectors_a / (norms + 1e-10)\n",
    "\n",
    "\tif vectors_a is vectors_b:\n",
    "\t\tdata_vectors = cleaned_vectors\n",
    "\telse:\n",
    "\n",
    "\t\tdata_vectors = cleaned_vectors\n",
    "\n",
    "\tlw = LedoitWolf()\n",
    "\tlw.fit(data_vectors)\n",
    "\tprecision_matrix = lw.precision_\n",
    "\tVI = precision_matrix\n",
    "\n",
    "\tdiff_matrix = cleaned_vectors[:, None, :] - cleaned_vectors[None, :, :]\n",
    "\n",
    "\tVI_diff = diff_matrix @ VI\n",
    "\n",
    "\tmahalanobis_sq_matrix = np.sum(diff_matrix * VI_diff, axis=-1)\n",
    "\n",
    "\tdist_matrix = np.sqrt(mahalanobis_sq_matrix)\n",
    "\n",
    "\tdist_matrix = np.maximum(dist_matrix, 0.0)\n",
    "\n",
    "\treturn dist_matrix, precision_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d5c919",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = cdist(normalised, normalised, metric=\"mahalanobis\", VI=precision_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f4072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_matrix = cdist(normalised, normalised, metric=\"mahalanobis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575ad98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMahalanobisDistances(vectors_a, vectors_b):\n",
    "\tnorms = np.linalg.norm(vectors_a, axis=1, keepdims=True)\n",
    "\tcleaned_vectors = vectors_a / (norms + 1e-10)\n",
    "\n",
    "\tlw = LedoitWolf()\n",
    "\tlw.fit(cleaned_vectors)\n",
    "\tprecision_matrix = lw.precision_\n",
    "\n",
    "\tdist_matrix = cdist(\n",
    "\t\tcleaned_vectors, cleaned_vectors, metric=\"mahalanobis\", VI=precision_matrix\n",
    "\t)\n",
    "\treturn dist_matrix, precision_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cb5d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c419a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {1: 2, 3: 4}\n",
    "\"Hello wORld\".upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6d62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_TYPES = [\n",
    "\t\"SEMANTIC_SIMILARITY\",\n",
    "\t\"CLASSIFICATION\",\n",
    "\t\"CLUSTERING\",\n",
    "\t\"RETRIEVAL_DOCUMENT\",\n",
    "\t\"RETRIEVAL_QUERY\",\n",
    "\t\"CODE_RETRIEVAL_QUERY\",\n",
    "\t\"QUESTION_ANSWERING\",\n",
    "\t\"FACT_VERIFICATION\",\n",
    "]\n",
    "\n",
    "len(EMBEDDING_TYPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7d3c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_TYPES = [\n",
    "\t\"SEMANTIC_SIMILARITY\",\n",
    "\t\"CLASSIFICATION\",\n",
    "\t\"CLUSTERING\",\n",
    "\t\"RETRIEVAL_DOCUMENT\",\n",
    "\t\"RETRIEVAL_QUERY\",\n",
    "\t\"CODE_RETRIEVAL_QUERY\",\n",
    "\t\"QUESTION_ANSWERING\",\n",
    "\t\"FACT_VERIFICATION\",\n",
    "]\n",
    "\n",
    "# Core Question\n",
    "CORE_PREFIX = (\n",
    "\t\"Does the privacy policy affirm that \"\t# Default and form of the data-source\n",
    ")\n",
    "CORE_STATEMENT = \"The privacy policy affirms that \"\n",
    "\n",
    "# NEGATION\n",
    "NEGATION_PREFIX = \"Does the privacy policy not affirm that \"\n",
    "NEGATION_TO_STATEMENT = \"The privacy policy does not affirm that \"\n",
    "\n",
    "# THE CONTRARY\n",
    "\n",
    "CONTRARY_PREFIX = \"Does the privacy policy deny that \"\n",
    "CONTRARY_PREFIX = \"The privacy policy denies that \"\n",
    "\n",
    "# THE NEGATION OF THE CONTRARY\n",
    "NEG_CONTRARY_PREFIX = \"Does the privacy policy not deny that \"\n",
    "NEG_CONTRARY_STATEMENT = \"The privacy policy does not deny that \"\n",
    "\n",
    "\n",
    "def statementConversion(question, prefix, replacement):\n",
    "\tprocessed_text = (replacement + question[len(prefix) :])[:-1] + \".\"\n",
    "\n",
    "\treturn processed_text\n",
    "\n",
    "\n",
    "# also for extraction of purely X: This should only be ran once per statement\n",
    "def propositionExtraction(question):\n",
    "\tprocessed_text = (question[len(CORE_PREFIX) :].capitalize())[:-1] + \".\"\n",
    "\treturn processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be9e1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keys corresponding to data dictionary\n",
    "# import os\n",
    "# import json\n",
    "\n",
    "# QUESTIONS_FILE = \"./data/questions_filter_after.json\"\n",
    "# POLICIES_FILE = \"./data/policies_testing.json\"\n",
    "# OUTPUT_Q_FILE = \"./output_q.json\"\n",
    "# OUTPUT_P_FILE = \"./output_p.json\"\n",
    "\n",
    "\n",
    "# def _loadJson(filepath):\n",
    "# \tif not os.path.exists(filepath):\n",
    "# \t\tprint(f\"Warning: File not found: {filepath}\")\n",
    "# \t\treturn {}\n",
    "# \ttry:\n",
    "# \t\twith open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "# \t\t\treturn json.load(f)\n",
    "# \texcept json.JSONDecodeError:\n",
    "# \t\tprint(f\"Error decoding JSON: {filepath}\")\n",
    "# \t\treturn {}\n",
    "\n",
    "\n",
    "# qdata = _loadJson(QUESTIONS_FILE)\n",
    "# model_keys = [\"embedding_vector\", \"retrieval_embedding_vector\"]\n",
    "# metric = \"Jaccard\"\n",
    "# artifacts = prepareModelArtifacts(qdata, model_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b8621",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(((1, 0), (0, 1)))\n",
    "import scipy\n",
    "\n",
    "type(scipy.linalg.norm(a, axis=1, keepdims=True))\n",
    "a[:, :None].shape == a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8688000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMahalanobisDistances(vectors_a, vectors_b):\n",
    "\tnorms = np.linalg.norm(vectors_a, axis=1, keepdims=True)\n",
    "\tcleaned_vectors = vectors_a / (norms + 1e-10)\n",
    "\n",
    "\tlw = LedoitWolf()\n",
    "\tlw.fit(cleaned_vectors)\n",
    "\tprecision_matrix = lw.precision_\n",
    "\n",
    "\tdist_matrix = cdist(\n",
    "\t\tcleaned_vectors, cleaned_vectors, metric=\"mahalanobis\", VI=precision_matrix\n",
    "\t)\n",
    "\treturn dist_matrix, precision_matrix\n",
    "\n",
    "\n",
    "def _prepareModelArtifact(\n",
    "\traw_vectors,\n",
    "\tsemantic_data,\n",
    "\ttruncation_dim=256,\n",
    "\tdistance_metric=\"mahalanobis\",\n",
    "\tdebug=True,\n",
    "):\n",
    "\tdata_matrix = np.array(raw_vectors)\n",
    "\tdata_truncated = data_matrix[:, :truncation_dim]\n",
    "\n",
    "\tdist_output = Distance_Processors[distance_metric](data_truncated, data_truncated)\n",
    "\n",
    "\tif distance_metric == \"mahalanobis\":\n",
    "\t\tdist_matrix, precision_matrix = dist_output\n",
    "\telse:\n",
    "\t\tdist_matrix = dist_output\n",
    "\t\tprecision_matrix = None\n",
    "\n",
    "\treturn {\n",
    "\t\t\"dist_matrix\": dist_matrix,\n",
    "\t\t\"vectors\": data_truncated,\n",
    "\t\t\"precision\": precision_matrix,\n",
    "\t\t\"semantic_data\": semantic_data,\n",
    "\t\t\"metric\": distance_metric,\n",
    "\t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330fa1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.covariance import LedoitWolf\n",
    "import concurrent.futures\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import concurrent.futures\n",
    "from itertools import combinations\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Distance & Model Utilities\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\n",
    "def getMahalanobisDistances(vectors_a, vectors_b):\n",
    "\tnorms = np.linalg.norm(vectors_a, axis=1, keepdims=True)\n",
    "\tcleaned_vectors = vectors_a / (norms + 1e-10)\n",
    "\n",
    "\tlw = LedoitWolf()\n",
    "\tlw.fit(cleaned_vectors)\n",
    "\tprecision_matrix = lw.precision_\n",
    "\n",
    "\tdist_matrix = cdist(\n",
    "\t\tcleaned_vectors, cleaned_vectors, metric=\"mahalanobis\", VI=precision_matrix\n",
    "\t)\n",
    "\treturn dist_matrix, precision_matrix\n",
    "\n",
    "\n",
    "Distance_Processors = {\n",
    "\t\"cosine\": lambda emb_a, emb_b: 1.0\n",
    "\t- (emb_a @ emb_b.T)\n",
    "\t/ (\n",
    "\t\tnp.linalg.norm(emb_a, axis=1, keepdims=True)\n",
    "\t\t@ np.linalg.norm(emb_b, axis=1, keepdims=True).T\n",
    "\t\t+ 1e-10\n",
    "\t),\n",
    "\t\"l1\": lambda emb_a, emb_b: np.sum(np.abs(emb_a[..., np.newaxis] - emb_b.T), axis=1),\n",
    "\t\"l2\": lambda emb_a, emb_b: np.linalg.norm(emb_a[..., np.newaxis] - emb_b.T, axis=1),\n",
    "\t\"dot\": lambda emb_a, emb_b: emb_a @ emb_b.T,\n",
    "\t\"mahalanobis\": lambda emb_a, emb_b: getMahalanobisDistances(emb_a, emb_b),\n",
    "}\n",
    "\n",
    "\n",
    "def _prepareModelArtifact(\n",
    "\traw_vectors,\n",
    "\tsemantic_data,\n",
    "\ttruncation_dim=256,\n",
    "\tdistance_metric=\"mahalanobis\",\n",
    "\tdebug=True,\n",
    "):\n",
    "\tdata_matrix = np.array(raw_vectors)\n",
    "\tdata_truncated = data_matrix[:, :truncation_dim]\n",
    "\n",
    "\tdist_output = Distance_Processors[distance_metric](data_truncated, data_truncated)\n",
    "\n",
    "\tif distance_metric == \"mahalanobis\":\n",
    "\t\tdist_matrix, precision_matrix = dist_output\n",
    "\telse:\n",
    "\t\tdist_matrix = dist_output\n",
    "\t\tprecision_matrix = None\n",
    "\n",
    "\tnp.fill_diagonal(dist_matrix, float(\"inf\"))\n",
    "\tnn_indices = np.argmin(dist_matrix, axis=1)\n",
    "\tnp.fill_diagonal(dist_matrix, 0.0)\n",
    "\n",
    "\treturn {\n",
    "\t\t\"dist_matrix\": dist_matrix,\n",
    "\t\t\"vectors\": data_truncated,\n",
    "\t\t\"precision\": precision_matrix,\n",
    "\t\t\"semantic_data\": semantic_data,\n",
    "\t\t\"metric\": distance_metric,\n",
    "\t\t\"nn_indices\": nn_indices,\n",
    "\t}\n",
    "\n",
    "\n",
    "def prepareModelArtifacts(\n",
    "\tdata_set, vector_keys, truncation_dim=256, distance_metric=\"mahalanobis\", debug=True\n",
    "):\n",
    "\tsemantic_data = list(data_set.keys())\n",
    "\tmodel_artifacts = {}\n",
    "\traw_vectors = {}\n",
    "\tfor key in vector_keys:\n",
    "\t\traw_vectors[key] = [data_set[s][key] for s in semantic_data]\n",
    "\n",
    "\texecutor = concurrent.futures.ThreadPoolExecutor(max_workers=len(vector_keys))\n",
    "\tfutures = dict()\n",
    "\n",
    "\tfor key in vector_keys:\n",
    "\t\tif debug:\n",
    "\t\t\tprint(f\"Processing {key}...\")\n",
    "\t\tfutures[key] = executor.submit(\n",
    "\t\t\t_prepareModelArtifact,\n",
    "\t\t\traw_vectors[key],\n",
    "\t\t\tsemantic_data,\n",
    "\t\t\ttruncation_dim,\n",
    "\t\t\tdistance_metric,\n",
    "\t\t\tdebug,\n",
    "\t\t)\n",
    "\texecutor.shutdown(wait=True)\n",
    "\tfor key in vector_keys:\n",
    "\t\tmodel_artifacts[key] = futures[key].result()\n",
    "\treturn model_artifacts\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Clustering Utilities\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\n",
    "def getGroupsFromLabels(labels):\n",
    "\tgroups = {}\n",
    "\tfor idx, label in enumerate(labels):\n",
    "\t\tgroups.setdefault(label, []).append(idx)\n",
    "\treturn [g for g in groups.values() if len(g) > 1]\n",
    "\n",
    "\n",
    "def getNNPairsFromGroups(groups, nn_indices):\n",
    "\tpairs = set()\n",
    "\tfor group in groups:\n",
    "\t\tif len(group) < 2:\n",
    "\t\t\tcontinue\n",
    "\t\tgroup_set = set(group)\n",
    "\t\tfor idx in group:\n",
    "\t\t\tnn_idx = nn_indices[idx]\n",
    "\t\t\tif nn_idx in group_set:\n",
    "\t\t\t\tpairs.add(tuple(sorted((idx, nn_idx))))\n",
    "\treturn pairs\n",
    "\n",
    "\n",
    "def clusterAndGetArtifacts(dist_matrix, threshold):\n",
    "\tmodel = AgglomerativeClustering(\n",
    "\t\tn_clusters=None,\n",
    "\t\tdistance_threshold=threshold,\n",
    "\t\tmetric=\"precomputed\",\n",
    "\t\tlinkage=\"complete\",\n",
    "\t)\n",
    "\tlabels = model.fit_predict(dist_matrix)\n",
    "\treturn getGroupsFromLabels(labels), labels\n",
    "\n",
    "\n",
    "def getPairsFromLablesCombinations(labels):\n",
    "\tgroups = {}\n",
    "\tfor idx, label in enumerate(labels):\n",
    "\t\tgroups.setdefault(label, []).append(idx)\n",
    "\tpairs = set()\n",
    "\tfor label, indices in groups.items():\n",
    "\t\tif len(indices) > 1:\n",
    "\t\t\tfor p in combinations(sorted(indices), 2):\n",
    "\t\t\t\tpairs.add(p)\n",
    "\treturn pairs\n",
    "\n",
    "\n",
    "def getComponentsFromPairs(pairs):\n",
    "\t\"\"\"\n",
    "\tConverts a set of pairs (edges) into connected components (clusters).\n",
    "\t\"\"\"\n",
    "\tadj = defaultdict(set)\n",
    "\tnodes = set()\n",
    "\tfor u, v in pairs:\n",
    "\t\tadj[u].add(v)\n",
    "\t\tadj[v].add(u)\n",
    "\t\tnodes.add(u)\n",
    "\t\tnodes.add(v)\n",
    "\n",
    "\tcomponents = []\n",
    "\tvisited = set()\n",
    "\n",
    "\tfor node in nodes:\n",
    "\t\tif node not in visited:\n",
    "\t\t\tstack = [node]\n",
    "\t\t\tvisited.add(node)\n",
    "\t\t\tcomp = []\n",
    "\t\t\twhile stack:\n",
    "\t\t\t\tcurr = stack.pop()\n",
    "\t\t\t\tcomp.append(curr)\n",
    "\t\t\t\tfor neighbor in adj[curr]:\n",
    "\t\t\t\t\tif neighbor not in visited:\n",
    "\t\t\t\t\t\tvisited.add(neighbor)\n",
    "\t\t\t\t\t\tstack.append(neighbor)\n",
    "\t\t\tcomponents.append(comp)\n",
    "\treturn components\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Stochastic Analysis Logic\n",
    "# # ---------------------------------------------------------\n",
    "\n",
    "\n",
    "# def getCharacteristicLength(dist_matrix, nn_indices):\n",
    "# \t\"\"\"\n",
    "# \tReturns the scalar distance to the nearest neighbor (r).\n",
    "# \t\"\"\"\n",
    "# \tN = dist_matrix.shape[0]\n",
    "# \tsigmas = dist_matrix[np.arange(N), nn_indices]\n",
    "# \tsigmas[sigmas == 0] = 1e-9\n",
    "# \treturn sigmas.reshape(-1, 1)\n",
    "def getCharacteristicLength(dist_matrix, k=10, dim=None):\n",
    "\t\"\"\"\n",
    "\tReturns the Characteristic Length Scale (sigma) derived from local density.\n",
    "\n",
    "\tFormula: sigma ~ rho^(-1/D)\n",
    "\tWhere rho = k / Vol(r_k) ~ k / (r_k ** D)\n",
    "\tTherefore: sigma = r_k / (k ** (1/D))\n",
    "\n",
    "\tThis normalizes the k-th neighbor distance to the average inter-point spacing.\n",
    "\t\"\"\"\n",
    "\tN = dist_matrix.shape[0]\n",
    "\n",
    "\t# 1. Find Distance to k-th Neighbor (r_k)\n",
    "\t# Clamp k to valid range\n",
    "\tprint(N)\n",
    "\tprint(k)\n",
    "\n",
    "\tvalid_k = min(max(1, k), N - 1)\n",
    "\t# Efficiently find the k-th smallest distance in each row\n",
    "\tr_k = np.partition(dist_matrix, valid_k, axis=1)[:, valid_k]\n",
    "\n",
    "\t# Handle zeros (duplicates)\n",
    "\tr_k[r_k == 0] = 1e-9\n",
    "\tr_k = r_k.reshape(-1, 1)\n",
    "\n",
    "\t# 2. Apply Density Scaling: sigma = r_k * k^(-1/D)\n",
    "\t# If dim is not provided or 0, we assume D=1 (linear scaling)\n",
    "\tif dim and dim > 0:\n",
    "\t\tscaling_factor = valid_k ** (-1.0 / dim)\n",
    "\t\tsigma = r_k * scaling_factor\n",
    "\telse:\n",
    "\t\tsigma = r_k\n",
    "\n",
    "\treturn sigma\n",
    "\n",
    "\n",
    "def runStochasticAnalysis(\n",
    "\tartifacts,\n",
    "\toptimal_taus,\n",
    "\tn_iterations=200,\n",
    "\tnoise_fraction=0.15,\n",
    "\tactive_tests=[\"NN\"],\n",
    "\tdim=256,\n",
    "):\n",
    "\t\"\"\"\n",
    "\tRuns noise iterations on the distance matrices directly.\n",
    "\n",
    "\tReturns:\n",
    "\t    diameter_distributions: Dict {test_mode: [list_of_all_diameters]}\n",
    "\t    pair_counters: Dict {test_mode: Counter((a,b): count)}\n",
    "\t    all_pair_sets: Dict {test_mode: [set_iter_1, set_iter_2, ...]}\n",
    "\t\"\"\"\n",
    "\tmodel_keys = list(artifacts.keys())\n",
    "\n",
    "\tdiameter_distributions = {t: [] for t in active_tests}\n",
    "\tpair_counters = {t: Counter() for t in active_tests}\n",
    "\tall_pair_sets = {t: [] for t in active_tests}\t# Storing raw sets as requested\n",
    "\n",
    "\t# Pre-calculate Characteristic Lengths\n",
    "\tmodel_sigmas = {}\n",
    "\tfor key in model_keys:\n",
    "\t\tart = artifacts[key]\n",
    "\t\t# model_sigmas[key] = getCharacteristicLength(\n",
    "\t\t# \tart[\"dist_matrix\"], art[\"nn_indices\"], dim=dim\n",
    "\t\t# )\n",
    "\t\tmodel_sigmas[key] = getCharacteristicLength(art[\"dist_matrix\"], k=10, dim=256)\n",
    "\n",
    "\tref_key = model_keys[0]\n",
    "\tref_dist_matrix = artifacts[ref_key][\"dist_matrix\"]\n",
    "\n",
    "\tprint(f\"Running {n_iterations} iterations (Noise Fraction: {noise_fraction})...\")\n",
    "\n",
    "\tfor i in range(n_iterations):\n",
    "\t\tif i % 50 == 0:\n",
    "\t\t\tprint(i)\n",
    "\t\titer_groups = {}\n",
    "\t\titer_labels = {}\n",
    "\n",
    "\t\t# 1. Perturb Matrices & Cluster\n",
    "\t\tfor key in model_keys:\n",
    "\t\t\tart = artifacts[key]\n",
    "\t\t\tsigma = model_sigmas[key]\n",
    "\t\t\tN = art[\"dist_matrix\"].shape[0]\n",
    "\n",
    "\t\t\t# Symmetric Noise Scaled by Fraction\n",
    "\t\t\tscale = ((sigma + sigma.T) / 2.0) * noise_fraction\n",
    "\t\t\tnoise = np.random.normal(0, 1, size=(N, N)) * scale\n",
    "\t\t\tsym_noise = (noise + noise.T) / 2.0\n",
    "\n",
    "\t\t\tnoisy_dist = art[\"dist_matrix\"] + sym_noise\n",
    "\t\t\tnp.fill_diagonal(noisy_dist, 0.0)\n",
    "\t\t\t# noisy_dist[noisy_dist < 0] = 0.0\n",
    "\n",
    "\t\t\tt = optimal_taus[key]\n",
    "\t\t\tgroups, labels = clusterAndGetArtifacts(noisy_dist, t)\n",
    "\t\t\titer_groups[key] = groups\n",
    "\t\t\titer_labels[key] = labels\n",
    "\n",
    "\t\t# 2. Process active tests\n",
    "\t\tfor test_mode in active_tests:\n",
    "\t\t\tpair_sets = []\n",
    "\t\t\tfor key in model_keys:\n",
    "\t\t\t\tif test_mode == \"NN\":\n",
    "\t\t\t\t\tp = getNNPairsFromGroups(iter_groups[key], artifacts[key][\"nn_indices\"])\n",
    "\t\t\t\telif test_mode == \"Combinations\":\n",
    "\t\t\t\t\tp = getPairsFromLablesCombinations(iter_labels[key])\n",
    "\t\t\t\tpair_sets.append(p)\n",
    "\n",
    "\t\t\tif pair_sets:\n",
    "\t\t\t\tp_true = set.intersection(*pair_sets)\n",
    "\n",
    "\t\t\t\t# A. Store Raw Set (Constraint check)\n",
    "\t\t\t\tall_pair_sets[test_mode].append(p_true)\n",
    "\n",
    "\t\t\t\t# B. Update Pair Counter\n",
    "\t\t\t\tpair_counters[test_mode].update(p_true)\n",
    "\n",
    "\t\t\t\t# C. Build Consensus Clusters (Connected Components)\n",
    "\t\t\t\tconsensus_clusters = getComponentsFromPairs(p_true)\n",
    "\n",
    "\t\t\t\t# D. Calculate Diameter\n",
    "\t\t\t\tfor cluster_indices in consensus_clusters:\n",
    "\t\t\t\t\tif len(cluster_indices) > 1:\n",
    "\t\t\t\t\t\tsub_dist = ref_dist_matrix[np.ix_(cluster_indices, cluster_indices)]\n",
    "\t\t\t\t\t\td = np.max(sub_dist)\n",
    "\t\t\t\t\t\tdiameter_distributions[test_mode].append(d)\n",
    "\n",
    "\treturn diameter_distributions, pair_counters, all_pair_sets\n",
    "\n",
    "\n",
    "def plotStochasticResults(diameter_data, pair_counters, n_iterations):\n",
    "\t\"\"\"\n",
    "\tPlots:\n",
    "\t1. Average Cumulative Distribution of Consensus Cluster Diameters.\n",
    "\t2. Histogram of Pair Co-occurrence Counts.\n",
    "\t\"\"\"\n",
    "\tfig = make_subplots(\n",
    "\t\trows=1,\n",
    "\t\tcols=2,\n",
    "\t\tsubplot_titles=(\n",
    "\t\t\t\"Avg Cumulative Clusters vs Diameter\",\n",
    "\t\t\t\"Pair Co-occurrence Distribution\",\n",
    "\t\t),\n",
    "\t)\n",
    "\n",
    "\tfor test_mode in diameter_data.keys():\n",
    "\t\tdiameters = diameter_data[test_mode]\n",
    "\t\tcounts = list(pair_counters[test_mode].values())\n",
    "\n",
    "\t\tif not diameters:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# --- Plot 1: Average CDF ---\n",
    "\t\tsorted_d = np.sort(diameters)\n",
    "\t\t# We normalize y_vals by n_iterations to get Average Count per Run\n",
    "\t\ty_vals = np.arange(1, len(sorted_d) + 1) / n_iterations\n",
    "\n",
    "\t\tfig.add_trace(\n",
    "\t\t\tgo.Scatter(\n",
    "\t\t\t\tx=sorted_d,\n",
    "\t\t\t\ty=y_vals,\n",
    "\t\t\t\tmode=\"lines\",\n",
    "\t\t\t\tname=f\"{test_mode} (CDF)\",\n",
    "\t\t\t\tlegendgroup=test_mode,\t# Toggle grouping\n",
    "\t\t\t\topacity=0.8,\n",
    "\t\t\t),\n",
    "\t\t\trow=1,\n",
    "\t\t\tcol=1,\n",
    "\t\t)\n",
    "\n",
    "\t\t# --- Plot 2: Histogram of Counts ---\n",
    "\t\tfig.add_trace(\n",
    "\t\t\tgo.Histogram(\n",
    "\t\t\t\tx=counts,\n",
    "\t\t\t\tname=f\"{test_mode} (Counts)\",\n",
    "\t\t\t\tlegendgroup=test_mode,\t# Toggle grouping\n",
    "\t\t\t\topacity=0.6,\n",
    "\t\t\t\tnbinsx=50,\n",
    "\t\t\t),\n",
    "\t\t\trow=1,\n",
    "\t\t\tcol=2,\n",
    "\t\t)\n",
    "\n",
    "\t# Layout Updates\n",
    "\tfig.update_xaxes(title_text=\"Cluster Diameter (Physical)\", row=1, col=1)\n",
    "\tfig.update_yaxes(title_text=\"Avg Count of Clusters ≤ Diameter\", row=1, col=1)\n",
    "\n",
    "\tfig.update_xaxes(\n",
    "\t\ttitle_text=\"Co-occurrence Count (Max: {})\".format(n_iterations), row=1, col=2\n",
    "\t)\n",
    "\tfig.update_yaxes(title_text=\"Number of Pairs\", row=1, col=2)\n",
    "\n",
    "\tfig.update_layout(\n",
    "\t\ttitle=\"Stochastic Resonance Analysis\", hovermode=\"x unified\", width=1200, height=600\n",
    "\t)\n",
    "\tfig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36f12a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuration\n",
    "active_tests = [\"NN\", \"Combinations\"]\t# Can be set to [\"NN\", \"Combinations\"]\n",
    "iterations = 2000\n",
    "optimal_taus = {\"embedding_vector\": 17.0, \"retrieval_embedding_vector\": 17}\n",
    "\n",
    "\n",
    "dist_data, counters, all_sets = runStochasticAnalysis(\n",
    "\tartifacts,\n",
    "\toptimal_taus,\n",
    "\tn_iterations=iterations,\n",
    "\tnoise_fraction=0.4,\n",
    "\tactive_tests=active_tests,\n",
    "\tdim=256,\n",
    ")\n",
    "\n",
    "# 2. Plot with S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e4b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ubplots & Averaging\n",
    "plotStochasticResults(dist_data, counters, iterations)\n",
    "\n",
    "# 3. Print Top 5 Pairs\n",
    "semantic_list = artifacts[list(artifacts.keys())[0]][\"semantic_data\"]\n",
    "\n",
    "print(\"\\n--- Top 5 Coincident Pairs ---\")\n",
    "for mode in active_tests:\n",
    "\tprint(f\"\\n[Mode: {mode}]\")\n",
    "\ttop_5 = counters[mode].most_common(5)\n",
    "\tfor (idx_a, idx_b), count in top_5:\n",
    "\t\tstr_a = semantic_list[idx_a]\n",
    "\t\tstr_b = semantic_list[idx_b]\n",
    "\t\tprint(f\" ({count}/{iterations}) {str_a} <--> {str_b}\")\n",
    "\n",
    "# 3. Print Pairs above Threshold\n",
    "threshold_X = 5\t# Adjust this value as needed\n",
    "semantic_list = artifacts[list(artifacts.keys())[0]][\"semantic_data\"]\n",
    "\n",
    "print(f\"\\n--- Coincident Pairs (Count > {threshold_X}) ---\")\n",
    "for mode in active_tests:\n",
    "\tprint(f\"\\n[Mode: {mode}]\")\n",
    "\n",
    "\t# Sort by count descending for readability\n",
    "\tsorted_pairs = sorted(counters[mode].items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\tfound_any = False\n",
    "\tfor (idx_a, idx_b), count in sorted_pairs:\n",
    "\t\tif count >= threshold_X:\n",
    "\t\t\tfound_any = True\n",
    "\t\t\t# Truncate strings for cleaner output\n",
    "\t\t\tstr_a = semantic_list[idx_a]\n",
    "\t\t\tstr_b = semantic_list[idx_b]\n",
    "\t\t\tprint(f\" {count}:\")\n",
    "\t\t\tprint(f\"    - {str_a}\")\n",
    "\t\t\tprint(f\"    - {str_b}\")\n",
    "\n",
    "\tif not found_any:\n",
    "\t\tprint(\" No pairs found above threshold.\")\n",
    "# dist_data, counters, all_pair_sets = runStochasticAnalysis(\n",
    "# \tartifacts, optimal_taus, n_iterations=iterations, active_tests=active_tests\n",
    "# )\n",
    "\n",
    "# # 2. Plot CDF\n",
    "# plotDiameterCDF(dist_data)\n",
    "\n",
    "# # 3. Print Top 5 Pairs\n",
    "# semantic_list = artifacts[list(artifacts.keys())[0]][\"semantic_data\"]\n",
    "\n",
    "# print(\"\\n--- Top 5 Coincident Pairs ---\")\n",
    "# for mode in active_tests:\n",
    "# \tprint(f\"\\n[Mode: {mode}]\")\n",
    "# \ttop_5 = counters[mode].most_common(10)\n",
    "# \tfor (idx_a, idx_b), count in top_5:\n",
    "# \t\tstr_a = semantic_list[idx_a]\n",
    "# \t\tstr_b = semantic_list[idx_b]\n",
    "# \t\tprint(f\" ({count}/{iterations}) {str_a} <--> {str_b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5fed95",
   "metadata": {},
   "source": [
    "# prior on freq\n",
    "# prior on length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e037b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(go.Scatter(x=[1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b788741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_pair_sets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
