{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2696aaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING ALL policy processing\n",
      "STARTING policy processing openai\n",
      "STARTING policy processing anthropic\n",
      "FINISHED chunk 14 in anthropic\n",
      "total chunks for openai: 13\n",
      "STARTING chunk 0 in openai\n",
      "STARTING chunk 1 in openai\n",
      "STARTING chunk 2 in openai\n",
      "STARTING chunk 3 in openai\n",
      "STARTING chunk 4 in openai\n",
      "STARTING chunk 5 in openai\n",
      "STARTING chunk 6 in openai\n",
      "STARTING chunk 7 in openai\n",
      "STARTING chunk 8 in openai\n",
      "STARTING chunk 9 in openai\n",
      "STARTING chunk 10 in openai\n",
      "STARTING chunk 11 in openai\n",
      "STARTING chunk 12 in openai\n",
      "FINISHED chunk 5 in openai\n",
      "total chunks for anthropic: 19\n",
      "STARTING chunk 0 in anthropic\n",
      "STARTING chunk 1 in anthropic\n",
      "STARTING chunk 2 in anthropic\n",
      "STARTING chunk 3 in anthropic\n",
      "STARTING chunk 4 in anthropic\n",
      "STARTING chunk 5 in anthropic\n",
      "STARTING chunk 6 in anthropic\n",
      "STARTING chunk 7 in anthropic\n",
      "STARTING chunk 8 in anthropic\n",
      "STARTING chunk 9 in anthropic\n",
      "STARTING chunk 10 in anthropic\n",
      "STARTING chunk 11 in anthropic\n",
      "STARTING chunk 12 in anthropic\n",
      "STARTING chunk 13 in anthropic\n",
      "STARTING chunk 14 in anthropic\n",
      "STARTING chunk 15 in anthropic\n",
      "STARTING chunk 16 in anthropic\n",
      "STARTING chunk 17 in anthropic\n",
      "STARTING chunk 18 in anthropic\n",
      "FINISHED chunk 8 in anthropic\n",
      "FINISHED chunk 2 in anthropic\n",
      "FINISHED chunk 10 in anthropic\n",
      "FINISHED chunk 1 in anthropic\n",
      "FINISHED chunk 11 in openai\n",
      "FINISHED chunk 18 in anthropic\n",
      "FINISHED chunk 13 in anthropic\n",
      "FINISHED chunk 0 in anthropic\n",
      "FINISHED chunk 1 in openai\n",
      "FINISHED chunk 16 in anthropic\n",
      "FINISHED chunk 6 in openai\n",
      "FINISHED chunk 7 in openai\n",
      "FINISHED chunk 9 in anthropic\n",
      "FINISHED chunk 12 in openai\n",
      "FINISHED chunk 11 in anthropic\n",
      "FINISHED chunk 10 in openai\n",
      "FINISHED chunk 9 in openai\n",
      "FINISHED chunk 14 in anthropic\n",
      "FINISHED chunk 4 in anthropic\n",
      "FINISHED chunk 12 in anthropic\n",
      "FINISHED chunk 3 in openai\n",
      "FINISHED chunk 0 in openai\n",
      "FINISHED chunk 6 in anthropic\n",
      "FINISHED chunk 8 in openai\n",
      "FINISHED chunk 5 in openai\n",
      "FINISHED chunk 8 in anthropic\n",
      "FINISHED chunk 3 in anthropic\n",
      "FINISHED chunk 0 in anthropic\n",
      "FINISHED chunk 2 in anthropic\n",
      "FINISHED chunk 10 in anthropic\n",
      "FINISHED chunk 3 in openai\n",
      "FINISHED chunk 4 in openai\n",
      "FINISHED chunk 17 in anthropic\n",
      "FINISHED chunk 18 in anthropic\n",
      "FINISHED chunk 5 in anthropic\n",
      "FINISHED chunk 15 in anthropic\n",
      "FINISHED chunk 7 in anthropic\n",
      "FINISHED chunk 16 in anthropic\n",
      "FINISHED chunk 6 in openai\n",
      "FINISHED chunk 5 in anthropic\n",
      "FINISHED ALL chunks for anthropic\n",
      "FINISHED chunk 2 in openai\n",
      "FINISHED ALL chunks for openai\n",
      "FINISHED chunk 7 in anthropic\n",
      "FINISHED chunk 17 in anthropic\n",
      "FINISHED chunk 6 in anthropic\n",
      "FINISHED chunk 4 in openai\n",
      "FINISHED chunk 3 in anthropic\n",
      "FINISHED chunk 2 in openai\n",
      "FINISHED chunk 15 in anthropic\n",
      "FINISHED ALL chunks for anthropic\n",
      "FINISHED chunk 9 in openai\n",
      "FINISHED ALL chunks for openai\n",
      "FINISHED PROCESSING ALL Policies\n",
      "Strings to remove:\n",
      "duplicate: Does the privacy policy affirm that the company does not knowingly use any information from children under the age of 18?, main:Does the privacy policy affirm that the company does not knowingly collect any information from children under the age of 18?\n",
      "updating: {'8797960062c67485b46319247c3666b728326a5365ee8cf8f6be6cf6f19e295d': []}\n",
      "with: {'8797960062c67485b46319247c3666b728326a5365ee8cf8f6be6cf6f19e295d': []}\n",
      "duplicate: Does the privacy policy affirm that information regarding the use of training data for language models is provided in a separate help center article?, main:Does the privacy policy affirm that information regarding the collection of training data for language models is provided in a separate help center article?\n",
      "updating: {'a58f6449acc07a431dd2bf48b20865edf2523f3c41b57eade43cc30d4ee25f07': []}\n",
      "with: {'a58f6449acc07a431dd2bf48b20865edf2523f3c41b57eade43cc30d4ee25f07': []}\n",
      "duplicate: Does the privacy policy affirm that personal data is used to improve the Services?, main:Does the privacy policy affirm that Personal Data may be used to improve the Services?\n",
      "adding: {'fac08b738949bae239a1ea2e33ba9508b1abc152fb00bd2bd2e836986c3bc67c': {'36a9b7a0e55afe9557ed2c28e59ae26a7d84fbfe8dec8bca4f3be00fb7b32356': []}}\n",
      "to: {'64b194a739dade62a6c90a81d803b42e298c2dcccb79a1f1fcca83df888ad67c': {'a7abb652d7c925d671ba7206a00c07e0df3c50ad63ffdf4e1e1033994077de00': []}}\n",
      "duplicate: Does the privacy policy affirm that personal data is used to conduct research?, main:Does the privacy policy affirm that Personal Data may be used to conduct research?\n",
      "adding: {'fac08b738949bae239a1ea2e33ba9508b1abc152fb00bd2bd2e836986c3bc67c': {'36a9b7a0e55afe9557ed2c28e59ae26a7d84fbfe8dec8bca4f3be00fb7b32356': []}}\n",
      "to: {'64b194a739dade62a6c90a81d803b42e298c2dcccb79a1f1fcca83df888ad67c': {'a7abb652d7c925d671ba7206a00c07e0df3c50ad63ffdf4e1e1033994077de00': []}}\n",
      "duplicate: Does the privacy policy affirm that expressly consenting to the policy means the user consents to the processing of their personal data?, main:Does the privacy policy affirm that expressly consenting to the policy means the user consents to the use of their personal data?\n",
      "updating: {'c00cfa5642b1a71eda79f3d57072784ecadf16a1176068b79cb240bfa5801d91': []}\n",
      "with: {'c00cfa5642b1a71eda79f3d57072784ecadf16a1176068b79cb240bfa5801d91': []}\n",
      "duplicate: Does the privacy policy affirm that the company's subprocessors may process personal data when users use the company's commercial services?, main:Does the privacy policy affirm that the company's subprocessors may receive personal data when users use the company's commercial services?\n",
      "updating: {'4a0f284c58919760369216fff0ba4c2c4903d3ca09186eac852ec54c35a8364a': []}\n",
      "with: {'4a0f284c58919760369216fff0ba4c2c4903d3ca09186eac852ec54c35a8364a': []}\n",
      "duplicate: Does the privacy policy affirm that the company's subprocessors may store personal data when users use the company's commercial services?, main:Does the privacy policy affirm that the company's subprocessors may receive personal data when users use the company's commercial services?\n",
      "updating: {'4a0f284c58919760369216fff0ba4c2c4903d3ca09186eac852ec54c35a8364a': []}\n",
      "with: {'4a0f284c58919760369216fff0ba4c2c4903d3ca09186eac852ec54c35a8364a': []}\n",
      "duplicate: Does the privacy policy affirm that the company's subprocessors may host personal data when users use the company's commercial services?, main:Does the privacy policy affirm that the company's subprocessors may receive personal data when users use the company's commercial services?\n",
      "updating: {'4a0f284c58919760369216fff0ba4c2c4903d3ca09186eac852ec54c35a8364a': []}\n",
      "with: {'4a0f284c58919760369216fff0ba4c2c4903d3ca09186eac852ec54c35a8364a': []}\n",
      "duplicate: Does the privacy policy affirm that Usage Data collected includes the types of content that the user engages with?, main:Does the privacy policy affirm that Usage Data collected includes the types of content that the user views?\n",
      "updating: {'71d65acf597a239d891187f32fd89fa994a3d2b43aa14e6343ec82f546ee2622': []}\n",
      "with: {'71d65acf597a239d891187f32fd89fa994a3d2b43aa14e6343ec82f546ee2622': []}\n",
      "duplicate: Does the privacy policy affirm that Feedback is processed to improve the services and conduct research, including model training?, main:Does the privacy policy affirm that Feedback is processed to improve the services and conduct research, excluding model training?\n",
      "updating: {'2ce02079318c1d330bdec7d58f4e35d4e69039d0e91a10970be85370ca22774c': []}\n",
      "with: {'2ce02079318c1d330bdec7d58f4e35d4e69039d0e91a10970be85370ca22774c': []}\n",
      "duplicate: Does the privacy policy affirm that Inputs and Outputs are processed to improve the services and conduct research, including model training?, main:Does the privacy policy affirm that Inputs and Outputs are processed to improve the services and conduct research, excluding model training?\n",
      "updating: {'2ce02079318c1d330bdec7d58f4e35d4e69039d0e91a10970be85370ca22774c': []}\n",
      "with: {'2ce02079318c1d330bdec7d58f4e35d4e69039d0e91a10970be85370ca22774c': []}\n",
      "FINISHED FILTERING Retrieval Embeddings\n",
      "FINISHED adding Retrieval Embeddings\n"
     ]
    }
   ],
   "source": [
    "from analysis_processor import AnalysisProcessor\n",
    "\n",
    "data_source = {\n",
    "\t\"openai\": \"https://openai.com/policies/privacy-policy/\",\n",
    "\t\"anthropic\": \"https://www.anthropic.com/legal/privacy\",\n",
    "\t# \"perplexity\": \"https://www.perplexity.ai/hub/legal/privacy-policy\",\t# HTTPError: 403 Client Error: Forbidden for url: https://www.perplexity.ai/hub/legal/privacy-policy\n",
    "\t# \"deepseek\": \"https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy.html\", # Returns None\n",
    "}\n",
    "a = AnalysisProcessor(data_source, debug=False)\n",
    "a.runAnalyses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dff28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strings to remove:\n",
      "duplicate: Does the privacy policy affirm that the company does not knowingly use information from children under the age of 18?, main:Does the privacy policy affirm that the company does not knowingly collect information from children under the age of 18?\n",
      "updating: {'8797960062c67485b46319247c3666b728326a5365ee8cf8f6be6cf6f19e295d': []}\n",
      "with: {'8797960062c67485b46319247c3666b728326a5365ee8cf8f6be6cf6f19e295d': []}\n",
      "duplicate: Does the privacy policy affirm that expressly consenting to the Privacy Policy confirms the user's consent to the processing of personal data?, main:Does the privacy policy affirm that expressly consenting to the Privacy Policy confirms the user's consent to the use of personal data?\n",
      "updating: {'c00cfa5642b1a71eda79f3d57072784ecadf16a1176068b79cb240bfa5801d91': []}\n",
      "with: {'c00cfa5642b1a71eda79f3d57072784ecadf16a1176068b79cb240bfa5801d91': []}\n",
      "duplicate: Does the privacy policy affirm that personal data is used to communicate with the user?, main:Does the privacy policy affirm that Personal Data is used to communicate with the user?\n",
      "adding: {'fac08b738949bae239a1ea2e33ba9508b1abc152fb00bd2bd2e836986c3bc67c': {'36a9b7a0e55afe9557ed2c28e59ae26a7d84fbfe8dec8bca4f3be00fb7b32356': []}}\n",
      "to: {'64b194a739dade62a6c90a81d803b42e298c2dcccb79a1f1fcca83df888ad67c': {'a7abb652d7c925d671ba7206a00c07e0df3c50ad63ffdf4e1e1033994077de00': []}}\n",
      "duplicate: Does the privacy policy affirm that personal data is used to prevent fraud?, main:Does the privacy policy affirm that Personal Data is used to prevent fraud?\n",
      "adding: {'fac08b738949bae239a1ea2e33ba9508b1abc152fb00bd2bd2e836986c3bc67c': {'36a9b7a0e55afe9557ed2c28e59ae26a7d84fbfe8dec8bca4f3be00fb7b32356': []}}\n",
      "to: {'64b194a739dade62a6c90a81d803b42e298c2dcccb79a1f1fcca83df888ad67c': {'a7abb652d7c925d671ba7206a00c07e0df3c50ad63ffdf4e1e1033994077de00': []}}\n",
      "duplicate: Does the privacy policy affirm that personal data is used to conduct research?, main:Does the privacy policy affirm that Personal Data is used to conduct research?\n",
      "adding: {'fac08b738949bae239a1ea2e33ba9508b1abc152fb00bd2bd2e836986c3bc67c': {'36a9b7a0e55afe9557ed2c28e59ae26a7d84fbfe8dec8bca4f3be00fb7b32356': []}}\n",
      "to: {'64b194a739dade62a6c90a81d803b42e298c2dcccb79a1f1fcca83df888ad67c': {'a7abb652d7c925d671ba7206a00c07e0df3c50ad63ffdf4e1e1033994077de00': []}}\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': '* BatchEmbedContentsRequest.requests: requests must not be empty\\n', 'status': 'INVALID_ARGUMENT'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m a.filterDuplicates()\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# print(\"FINISHED FILTERING Retrieval Embeddings\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43maddFactEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# print(\"FINISHED adding Retrieval Embeddings\")\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# self.saveData()\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/tool/analysis_processor.py:512\u001b[39m, in \u001b[36mAnalysisProcessor.addFactEmbeddings\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34maddFactEmbeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m \tquestion_emb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_interface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocessFactEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquestion_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m \t\u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m question_emb:\n\u001b[32m    514\u001b[39m \t\t\u001b[38;5;28mself\u001b[39m.question_data[i[\u001b[32m0\u001b[39m]].update({\u001b[33m\"\u001b[39m\u001b[33mretrieval_embedding_vector\u001b[39m\u001b[33m\"\u001b[39m: i[\u001b[32m1\u001b[39m]})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/tool/question_analysis.py:121\u001b[39m, in \u001b[36mLLMInterface.processFactEmbeddings\u001b[39m\u001b[34m(self, question_keyed_dict, add_facts)\u001b[39m\n\u001b[32m    116\u001b[39m embeddings = []\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m end:\n\u001b[32m    119\u001b[39m \tem: \u001b[38;5;28mlist\u001b[39m = [\n\u001b[32m    120\u001b[39m \t\te.values\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \t\t\u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m\t\t\t\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemini-embedding-001\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m\t\t\t\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m\t\t\t\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbedContentConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFACT_VERIFICATION\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m\t\t\u001b[49m\u001b[43m)\u001b[49m.embeddings\n\u001b[32m    126\u001b[39m \t]\n\u001b[32m    127\u001b[39m \t\u001b[38;5;28;01mif\u001b[39;00m start >= \u001b[38;5;28mlen\u001b[39m(inputs):\n\u001b[32m    128\u001b[39m \t\tend = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/google/genai/models.py:4025\u001b[39m, in \u001b[36mModels.embed_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4022\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   4023\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4025\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4026\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4027\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4029\u001b[39m response_dict = {} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response.body \u001b[38;5;28;01melse\u001b[39;00m json.loads(response.body)\n\u001b[32m   4031\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._api_client.vertexai:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/google/genai/_api_client.py:1331\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1321\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1323\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1326\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1327\u001b[39m ) -> SdkHttpResponse:\n\u001b[32m   1328\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1329\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m   1330\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1331\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1332\u001b[39m   response_body = (\n\u001b[32m   1333\u001b[39m       response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1334\u001b[39m   )\n\u001b[32m   1335\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers=response.headers, body=response_body)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/google/genai/_api_client.py:1167\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1164\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m   1165\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1167\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/tenacity/__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/tenacity/__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/google/genai/_api_client.py:1144\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1137\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1138\u001b[39m       method=http_request.method,\n\u001b[32m   1139\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1142\u001b[39m       timeout=http_request.timeout,\n\u001b[32m   1143\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1145\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1146\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1147\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/.venv/lib/python3.13/site-packages/google/genai/errors.py:108\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    106\u001b[39m status_code = response.status_code\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    110\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[31mClientError\u001b[39m: 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': '* BatchEmbedContentsRequest.requests: requests must not be empty\\n', 'status': 'INVALID_ARGUMENT'}}"
     ]
    }
   ],
   "source": [
    "a.filterDuplicates()\n",
    "# print(\"FINISHED FILTERING Retrieval Embeddings\")\n",
    "\n",
    "\n",
    "# print(\"FINISHED adding Retrieval Embeddings\")\n",
    "# self.saveData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14666920",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43ma\u001b[49m.addFactEmbeddings()\n",
      "\u001b[31mNameError\u001b[39m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a.addFactEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f2fb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arg_max: 0.8154822347987545\n",
      "arg_max: 0.7785115614711727\n",
      "arg_max: 0.7636897238462672\n",
      "arg_max: 0.7876893859570298\n",
      "arg_max: 0.8005100270620508\n",
      "arg_max: 0.8160055460930307\n",
      "arg_max: 0.8110761999104599\n",
      "arg_max: 0.8124730738498347\n",
      "arg_max: 0.7838888484205083\n",
      "arg_max: 0.789587501634968\n",
      "arg_max: 0.7884234204626059\n",
      "arg_max: 0.7933264499111192\n",
      "arg_max: 0.7713240494981017\n",
      "arg_max: 0.7900870705308061\n",
      "arg_max: 0.7900028959066683\n",
      "arg_max: 0.7607020953557138\n",
      "arg_max: 0.8132821762126792\n",
      "arg_max: 0.8030040201322103\n",
      "arg_max: 0.8348736555865537\n",
      "arg_max: 0.8012844039902214\n",
      "arg_max: 0.8142477179911228\n",
      "arg_max: 0.7956246102158705\n",
      "arg_max: 0.8090199758633856\n",
      "arg_max: 0.7963495588556766\n",
      "arg_max: 0.7864624050375516\n",
      "arg_max: 0.7940792260262169\n",
      "arg_max: 0.7960120570321789\n",
      "arg_max: 0.794044418007121\n",
      "arg_max: 0.7925871441055031\n",
      "arg_max: 0.7905018934267262\n",
      "arg_max: 0.7555205764885982\n",
      "arg_max: 0.7345365341293062\n",
      "arg_max: 0.7321064224731562\n",
      "arg_max: 0.7605498868215159\n",
      "arg_max: 0.7505203355061221\n",
      "arg_max: 0.8105290873689277\n",
      "arg_max: 0.793312306166184\n",
      "arg_max: 0.7901353063167641\n",
      "arg_max: 0.7826434679964325\n",
      "arg_max: 0.7952931006512476\n",
      "arg_max: 0.7656415967086048\n",
      "arg_max: 0.7737713170698913\n",
      "arg_max: 0.784909200092607\n",
      "arg_max: 0.726902803309827\n",
      "arg_max: 0.7402214682940508\n",
      "arg_max: 0.7337112496885219\n",
      "arg_max: 0.846609051109487\n",
      "arg_max: 0.8529848719846873\n",
      "arg_max: 0.8154418836725176\n",
      "arg_max: 0.8487021567531647\n",
      "arg_max: 0.8384128769963484\n",
      "arg_max: 0.7989077399204226\n",
      "arg_max: 0.8310015322079847\n",
      "arg_max: 0.8176180050374174\n",
      "arg_max: 0.8189362487682019\n",
      "arg_max: 0.7814947885666443\n",
      "arg_max: 0.8011775526788373\n",
      "arg_max: 0.7827165774691145\n",
      "arg_max: 0.812831976843505\n",
      "arg_max: 0.7984928551849116\n",
      "arg_max: 0.7384016889672707\n",
      "arg_max: 0.7894954112620265\n",
      "arg_max: 0.7692357067168447\n",
      "arg_max: 0.7869381296875313\n",
      "arg_max: 0.8301693994358341\n",
      "arg_max: 0.8095500745601976\n",
      "arg_max: 0.7985291642480187\n",
      "arg_max: 0.8013648875523566\n",
      "arg_max: 0.7971685842216975\n",
      "arg_max: 0.7876273191022987\n",
      "arg_max: 0.7619858024586819\n",
      "arg_max: 0.792505951066116\n",
      "arg_max: 0.7895634120339796\n",
      "arg_max: 0.7903078641736214\n",
      "arg_max: 0.7913386976312948\n",
      "arg_max: 0.7960961238870989\n",
      "arg_max: 0.795867341234296\n",
      "arg_max: 0.788017273895586\n",
      "arg_max: 0.7898320181103688\n",
      "arg_max: 0.7861636085764104\n",
      "arg_max: 0.7894993590882994\n",
      "arg_max: 0.7562177792268043\n",
      "arg_max: 0.7472377966397112\n",
      "arg_max: 0.7592754238202479\n",
      "arg_max: 0.7475078239931668\n",
      "arg_max: 0.7696445650426502\n",
      "arg_max: 0.7602878181309305\n",
      "arg_max: 0.77681629583568\n",
      "arg_max: 0.7702181733385758\n",
      "arg_max: 0.7785299830963235\n",
      "arg_max: 0.800509719519876\n",
      "arg_max: 0.7995593472008058\n",
      "arg_max: 0.7862993329021095\n",
      "arg_max: 0.8063143865031547\n",
      "arg_max: 0.7996134522916996\n",
      "arg_max: 0.7887363293766227\n",
      "arg_max: 0.8166440628474159\n",
      "arg_max: 0.8285887223892218\n",
      "arg_max: 0.8243387848621758\n",
      "arg_max: 0.8025931244901614\n",
      "arg_max: 0.7826488047547049\n",
      "arg_max: 0.8155979725140593\n",
      "arg_max: 0.7965182856876593\n",
      "arg_max: 0.8519688900086287\n",
      "arg_max: 0.7822384537793637\n",
      "arg_max: 0.806527213619894\n",
      "arg_max: 0.8173032346914801\n",
      "arg_max: 0.8305213910191541\n",
      "arg_max: 0.825350467070792\n",
      "arg_max: 0.8416937690133788\n",
      "arg_max: 0.8103409656214852\n",
      "arg_max: 0.8616158874995368\n",
      "arg_max: 0.8611707347869934\n",
      "arg_max: 0.7910547603087131\n",
      "arg_max: 0.8177657866021265\n",
      "arg_max: 0.7990824733855542\n",
      "arg_max: 0.8190834343327724\n",
      "arg_max: 0.8108743089686213\n",
      "arg_max: 0.8088627253407914\n",
      "arg_max: 0.7874868798044046\n",
      "arg_max: 0.7692367657050586\n",
      "arg_max: 0.7796411322673091\n",
      "arg_max: 0.7642749220593571\n",
      "arg_max: 0.7847163824250447\n",
      "arg_max: 0.8409015816259927\n",
      "arg_max: 0.7844352121273419\n",
      "arg_max: 0.7909776392825195\n",
      "arg_max: 0.7926612771891832\n",
      "arg_max: 0.7738759865228192\n",
      "arg_max: 0.7620165961707563\n",
      "arg_max: 0.7657886420710109\n",
      "arg_max: 0.7625413586501695\n",
      "arg_max: 0.751303088828796\n",
      "arg_max: 0.7524497350448045\n",
      "arg_max: 0.7673046890914916\n",
      "arg_max: 0.8367273417858475\n",
      "arg_max: 0.843039928748462\n",
      "arg_max: 0.842727644519143\n",
      "arg_max: 0.8290858065054644\n",
      "arg_max: 0.8260142132614815\n",
      "arg_max: 0.8320997048596902\n",
      "arg_max: 0.8220117970153435\n",
      "arg_max: 0.8247401200738504\n",
      "arg_max: 0.7901249648936071\n",
      "arg_max: 0.8237508850829156\n",
      "arg_max: 0.8318050103871643\n",
      "arg_max: 0.7810834810216458\n",
      "arg_max: 0.7788890451577869\n",
      "arg_max: 0.7789278165606699\n",
      "arg_max: 0.8029436928662526\n",
      "arg_max: 0.7945415397837707\n",
      "arg_max: 0.7381368386860826\n",
      "arg_max: 0.7659926365485157\n",
      "arg_max: 0.8205204495305793\n",
      "arg_max: 0.850616459689246\n",
      "arg_max: 0.8254656569441117\n",
      "arg_max: 0.8273345624380016\n",
      "arg_max: 0.8519991722550709\n",
      "arg_max: 0.7874249006322316\n",
      "arg_max: 0.7819827061093013\n",
      "arg_max: 0.8146498694540405\n",
      "arg_max: 0.8486311738643976\n",
      "arg_max: 0.8545703978213064\n",
      "arg_max: 0.8674149941638566\n",
      "arg_max: 0.7919775186972009\n",
      "arg_max: 0.7999634961022455\n",
      "arg_max: 0.7950531258626039\n",
      "arg_max: 0.7956719538124566\n",
      "arg_max: 0.7890593582076502\n",
      "arg_max: 0.8014784543013798\n",
      "arg_max: 0.781596404163661\n",
      "arg_max: 0.7751956794137568\n",
      "arg_max: 0.8224922550851839\n",
      "arg_max: 0.7963595178311001\n",
      "arg_max: 0.7843275620689771\n",
      "arg_max: 0.7893438722481533\n",
      "arg_max: 0.7777353927276431\n",
      "arg_max: 0.7834611136711602\n",
      "arg_max: 0.8388587181907112\n",
      "arg_max: 0.7962644565209493\n",
      "arg_max: 0.7929064052755698\n",
      "arg_max: 0.7781047636265286\n",
      "arg_max: 0.818782111785276\n",
      "arg_max: 0.8246970448865747\n",
      "arg_max: 0.8216637415196484\n",
      "arg_max: 0.818763239910218\n",
      "arg_max: 0.832658297724595\n",
      "arg_max: 0.7812757792496581\n",
      "arg_max: 0.7879781040002705\n",
      "arg_max: 0.7719383733479966\n",
      "arg_max: 0.7754786432902401\n",
      "arg_max: 0.8407381937114735\n",
      "arg_max: 0.7745138487865806\n",
      "arg_max: 0.7797085527101069\n",
      "arg_max: 0.7717764315091544\n",
      "arg_max: 0.7599053170754476\n",
      "arg_max: 0.7663239045919611\n",
      "arg_max: 0.8610389809464764\n",
      "arg_max: 0.8419525992068249\n",
      "arg_max: 0.7914852609344418\n",
      "arg_max: 0.8132797698650734\n",
      "arg_max: 0.7763836318261302\n",
      "arg_max: 0.7748220438807731\n",
      "arg_max: 0.7561882508986324\n",
      "arg_max: 0.7909793572262437\n",
      "arg_max: 0.7990009039319828\n",
      "arg_max: 0.7962077653799104\n",
      "arg_max: 0.7853452390313165\n",
      "arg_max: 0.784436934895435\n",
      "arg_max: 0.7634060025254705\n",
      "arg_max: 0.7838602984533052\n",
      "arg_max: 0.7837635732466155\n",
      "arg_max: 0.7826035348234736\n",
      "arg_max: 0.78391601020212\n",
      "arg_max: 0.8056296373871654\n",
      "arg_max: 0.8107666224805669\n",
      "arg_max: 0.766124959305031\n",
      "arg_max: 0.7846476371285824\n",
      "arg_max: 0.7827962158425998\n",
      "arg_max: 0.7908382978606676\n",
      "arg_max: 0.7736312665383892\n",
      "arg_max: 0.7824324344893314\n",
      "arg_max: 0.7524961037166102\n",
      "arg_max: 0.7683582853372455\n",
      "arg_max: 0.7672258492372921\n",
      "arg_max: 0.7491742166782921\n",
      "arg_max: 0.7347519608519635\n",
      "arg_max: 0.7686018025675234\n",
      "arg_max: 0.7494647545131724\n",
      "arg_max: 0.7795831469354549\n",
      "arg_max: 0.7483528493256619\n",
      "arg_max: 0.7957753782138067\n",
      "arg_max: 0.7827354313424132\n",
      "arg_max: 0.8421712849350108\n",
      "arg_max: 0.8322390143804305\n",
      "arg_max: 0.7530152041904999\n",
      "arg_max: 0.744855910898964\n",
      "arg_max: 0.7615678706776765\n",
      "arg_max: 0.7521826381263057\n",
      "arg_max: 0.7608019729774619\n",
      "arg_max: 0.7548953104490088\n",
      "arg_max: 0.7667767122597334\n",
      "arg_max: 0.7640157995598517\n",
      "arg_max: 0.7510940985142742\n",
      "arg_max: 0.7715109070181405\n",
      "arg_max: 0.7530111304223427\n",
      "arg_max: 0.7626678746782194\n",
      "arg_max: 0.7566513269609297\n",
      "arg_max: 0.7728747215506482\n",
      "arg_max: 0.7511316326153443\n",
      "arg_max: 0.7549086321493745\n",
      "arg_max: 0.7442017036741131\n",
      "arg_max: 0.7849974442099096\n",
      "arg_max: 0.7818014268070853\n",
      "arg_max: 0.7891938188394769\n",
      "arg_max: 0.7889828271617652\n",
      "arg_max: 0.7620483371816233\n",
      "arg_max: 0.7678920498229187\n",
      "arg_max: 0.7660259377447376\n",
      "arg_max: 0.7132087618851194\n",
      "arg_max: 0.7449234759896373\n",
      "arg_max: 0.739087860558719\n",
      "arg_max: 0.7600729380554011\n",
      "arg_max: 0.7354813890854575\n",
      "arg_max: 0.7542906581334006\n",
      "arg_max: 0.742546442034967\n",
      "arg_max: 0.755999807040607\n",
      "arg_max: 0.7598247148683671\n",
      "arg_max: 0.7696713077418778\n",
      "arg_max: 0.759681268101754\n",
      "arg_max: 0.7680890096680709\n",
      "arg_max: 0.7667002016917808\n",
      "arg_max: 0.751470355064387\n",
      "arg_max: 0.7498056659266816\n",
      "arg_max: 0.7627285476878912\n",
      "arg_max: 0.7533355667725127\n",
      "arg_max: 0.7491671878005093\n",
      "arg_max: 0.7792145987886165\n",
      "arg_max: 0.7463005958906577\n",
      "arg_max: 0.770478672108581\n",
      "arg_max: 0.7601021949579486\n",
      "arg_max: 0.7470995088296939\n",
      "arg_max: 0.7502540173484329\n",
      "arg_max: 0.7439341885587835\n",
      "arg_max: 0.7480383274232356\n",
      "arg_max: 0.740364655454518\n",
      "arg_max: 0.7472681133019609\n",
      "arg_max: 0.744178198646781\n",
      "arg_max: 0.7450210063396542\n",
      "arg_max: 0.7512025656495577\n",
      "arg_max: 0.7406907510964249\n",
      "arg_max: 0.7443244907671559\n",
      "arg_max: 0.7409164301446002\n",
      "arg_max: 0.7531228891296785\n",
      "arg_max: 0.7368757507936367\n",
      "arg_max: 0.7656954323536366\n",
      "arg_max: 0.7795162973895512\n",
      "arg_max: 0.7866650199568715\n",
      "arg_max: 0.7856558122334826\n",
      "arg_max: 0.7669570223955965\n",
      "arg_max: 0.755244169188435\n",
      "arg_max: 0.789856637256048\n",
      "arg_max: 0.8291755984967447\n",
      "arg_max: 0.8194361983574714\n",
      "arg_max: 0.7711035107116301\n",
      "arg_max: 0.7299450769723953\n",
      "arg_max: 0.8039126560998535\n",
      "arg_max: 0.771972271122192\n",
      "arg_max: 0.7679686833941293\n",
      "arg_max: 0.7563737928096077\n",
      "arg_max: 0.7580918004937185\n",
      "arg_max: 0.7598305456646024\n",
      "arg_max: 0.7635567258093471\n",
      "arg_max: 0.755135424048049\n",
      "arg_max: 0.7587214225086039\n",
      "arg_max: 0.765169290888634\n",
      "arg_max: 0.7646433459444653\n",
      "arg_max: 0.7837612597563404\n",
      "arg_max: 0.7813771327717032\n",
      "arg_max: 0.7249865823378804\n",
      "arg_max: 0.7610616588778005\n",
      "arg_max: 0.7495867167425161\n",
      "arg_max: 0.7580881551015634\n",
      "arg_max: 0.7476427541218621\n",
      "arg_max: 0.7374431930089325\n",
      "arg_max: 0.7787166949177908\n",
      "arg_max: 0.7844669613681976\n",
      "arg_max: 0.7850093800768749\n",
      "arg_max: 0.7830243034990224\n",
      "arg_max: 0.7423643082137071\n",
      "arg_max: 0.7700968742388739\n",
      "arg_max: 0.7825759653465285\n",
      "arg_max: 0.7833286189152523\n",
      "arg_max: 0.7943699126404176\n",
      "arg_max: 0.7852869624119541\n",
      "arg_max: 0.777023893453279\n",
      "arg_max: 0.7587646815138576\n",
      "arg_max: 0.8401778866470989\n",
      "arg_max: 0.8363487426943071\n",
      "arg_max: 0.8498113882278324\n",
      "arg_max: 0.8702569835451354\n",
      "arg_max: 0.8730400338148024\n",
      "arg_max: 0.8702622938181536\n",
      "arg_max: 0.8464565079482123\n",
      "arg_max: 0.7874925689923687\n",
      "arg_max: 0.7775836181655896\n",
      "arg_max: 0.8275837311992731\n",
      "arg_max: 0.7619553251275081\n",
      "arg_max: 0.7255155352003096\n",
      "arg_max: 0.7388857152891508\n",
      "arg_max: 0.7732079766441768\n",
      "arg_max: 0.7234060974832748\n",
      "arg_max: 0.7352227722713625\n",
      "arg_max: 0.7196759786665186\n",
      "arg_max: 0.7654376492538968\n",
      "arg_max: 0.7621078863268247\n",
      "arg_max: 0.7636500441205364\n",
      "arg_max: 0.7513565412110841\n",
      "arg_max: 0.7772846416159559\n",
      "arg_max: 0.7824368776552941\n",
      "arg_max: 0.7773501095052198\n",
      "arg_max: 0.791053033042372\n",
      "arg_max: 0.7808909212245302\n",
      "arg_max: 0.765944528637411\n",
      "arg_max: 0.7549688224909874\n",
      "arg_max: 0.8062132485855649\n",
      "arg_max: 0.7894907404550151\n",
      "arg_max: 0.8446370041397175\n",
      "arg_max: 0.803892162641917\n",
      "arg_max: 0.8269550025108262\n",
      "arg_max: 0.7893652627946932\n",
      "arg_max: 0.7848754553287058\n",
      "arg_max: 0.7772782247124759\n",
      "arg_max: 0.7805889733244944\n",
      "arg_max: 0.7290563345902892\n",
      "arg_max: 0.8021519021202996\n",
      "arg_max: 0.782084649684181\n",
      "arg_max: 0.7689360443888634\n",
      "arg_max: 0.7945474727883866\n",
      "arg_max: 0.749589274084987\n",
      "arg_max: 0.7546226962994256\n",
      "arg_max: 0.7514578885659385\n",
      "arg_max: 0.8103381541550497\n",
      "arg_max: 0.7927870839276083\n",
      "arg_max: 0.7979291250800615\n",
      "arg_max: 0.7889285644872726\n",
      "arg_max: 0.7880202254900888\n",
      "arg_max: 0.8040706015461293\n",
      "arg_max: 0.7839935705362873\n",
      "arg_max: 0.7679005063269386\n",
      "arg_max: 0.7875297867276106\n",
      "arg_max: 0.7881427015471054\n",
      "arg_max: 0.790506033318122\n",
      "arg_max: 0.7644439246714231\n",
      "arg_max: 0.782201584490479\n",
      "arg_max: 0.7864954245257418\n",
      "arg_max: 0.7862828023240759\n",
      "arg_max: 0.7753313367922618\n",
      "arg_max: 0.7754701901428366\n",
      "arg_max: 0.7540569109318966\n",
      "arg_max: 0.780209862253086\n",
      "arg_max: 0.7718573407107147\n",
      "arg_max: 0.767514305715736\n",
      "arg_max: 0.793658765784081\n",
      "arg_max: 0.7838402579800643\n",
      "arg_max: 0.7595169524556526\n",
      "arg_max: 0.774536462516645\n",
      "arg_max: 0.7840800402921989\n",
      "arg_max: 0.7800343618719826\n",
      "arg_max: 0.78112504405211\n",
      "arg_max: 0.7875672720200783\n",
      "arg_max: 0.7834644453454747\n",
      "arg_max: 0.7882211031489487\n",
      "arg_max: 0.7563002335895702\n",
      "arg_max: 0.8026957622960695\n",
      "arg_max: 0.7962112423459504\n",
      "arg_max: 0.7853153619574451\n",
      "arg_max: 0.7863170292407203\n",
      "arg_max: 0.7840676184152026\n",
      "arg_max: 0.7899571298392577\n",
      "arg_max: 0.7436949347622959\n",
      "arg_max: 0.7402626406926286\n",
      "arg_max: 0.7275174058473816\n",
      "arg_max: 0.7231818256131316\n",
      "arg_max: 0.7296463554735085\n",
      "arg_max: 0.7428046658387231\n",
      "arg_max: 0.7345430874830292\n",
      "arg_max: 0.7168885553653642\n",
      "arg_max: 0.7991254706355585\n",
      "arg_max: 0.795330504880287\n",
      "arg_max: 0.7651344687019849\n",
      "arg_max: 0.7898798458193965\n",
      "arg_max: 0.7593225382019002\n",
      "arg_max: 0.7541464816398569\n",
      "arg_max: 0.7687571666628586\n",
      "arg_max: 0.7282597407725064\n",
      "arg_max: 0.7515554117078561\n",
      "arg_max: 0.7541184245463896\n",
      "arg_max: 0.76976270689925\n",
      "arg_max: 0.7830884790803062\n",
      "arg_max: 0.7785813014888748\n",
      "arg_max: 0.8044532460667899\n",
      "arg_max: 0.798954329653392\n",
      "arg_max: 0.7608880675530012\n",
      "arg_max: 0.7766422695360833\n",
      "arg_max: 0.7735281434393562\n",
      "arg_max: 0.7721941291140657\n",
      "arg_max: 0.775908281817238\n",
      "arg_max: 0.7617826354917462\n",
      "arg_max: 0.7671996481385869\n",
      "arg_max: 0.7546333819749076\n",
      "arg_max: 0.7623435351459787\n",
      "arg_max: 0.7683566446810618\n",
      "arg_max: 0.7734753328998464\n",
      "arg_max: 0.7895233035623151\n",
      "arg_max: 0.8081115570988806\n",
      "arg_max: 0.8180628290456236\n",
      "arg_max: 0.7978551017496983\n",
      "arg_max: 0.8043713460742712\n",
      "arg_max: 0.8074802567298511\n",
      "arg_max: 0.8135057527979985\n",
      "arg_max: 0.7861605846589774\n",
      "arg_max: 0.7685737563647812\n",
      "arg_max: 0.7852933971950129\n",
      "arg_max: 0.7856139944203337\n",
      "arg_max: 0.8028310642131123\n",
      "arg_max: 0.7854865205816478\n",
      "arg_max: 0.7901069264845499\n",
      "arg_max: 0.7856139398512525\n",
      "arg_max: 0.8634626855160389\n",
      "arg_max: 0.8492732281117068\n",
      "arg_max: 0.811166478188064\n",
      "arg_max: 0.7814052981760151\n",
      "arg_max: 0.7712324600652429\n",
      "arg_max: 0.7627911873194244\n",
      "arg_max: 0.8010562285588252\n",
      "arg_max: 0.773322842330023\n",
      "arg_max: 0.7859480562588494\n",
      "arg_max: 0.7933872170080352\n",
      "arg_max: 0.779407873188314\n",
      "arg_max: 0.7748436594624734\n",
      "arg_max: 0.7810307925704906\n",
      "arg_max: 0.7772257811475166\n",
      "arg_max: 0.7667662358575424\n",
      "arg_max: 0.7833594829969216\n",
      "arg_max: 0.775845196299372\n",
      "arg_max: 0.7924732081938625\n",
      "arg_max: 0.7803026592127006\n",
      "arg_max: 0.7845627728234752\n",
      "arg_max: 0.7815114921643148\n",
      "arg_max: 0.769894122590981\n",
      "arg_max: 0.817327656670523\n",
      "arg_max: 0.7899481000274151\n",
      "arg_max: 0.7978607443538926\n",
      "arg_max: 0.7835994224501952\n",
      "arg_max: 0.7528451434664334\n",
      "arg_max: 0.7349781037647664\n",
      "arg_max: 0.7628195933944598\n",
      "arg_max: 0.7646169542322414\n",
      "arg_max: 0.7381179165861395\n",
      "arg_max: 0.7675118554796176\n",
      "arg_max: 0.787396624908844\n",
      "arg_max: 0.7662524786098911\n",
      "arg_max: 0.7543017338517942\n",
      "arg_max: 0.7601995030459416\n",
      "arg_max: 0.7901191819148389\n",
      "arg_max: 0.7896353282131269\n",
      "arg_max: 0.7881960914278323\n",
      "arg_max: 0.7716203289657215\n",
      "arg_max: 0.7702744434983863\n",
      "arg_max: 0.7783787484291617\n",
      "arg_max: 0.7649599695340648\n",
      "arg_max: 0.7815526742667275\n",
      "arg_max: 0.7914155074047042\n",
      "arg_max: 0.7799630332883658\n",
      "arg_max: 0.8379608795419621\n",
      "arg_max: 0.7755619989501511\n",
      "arg_max: 0.7817380808590955\n",
      "arg_max: 0.7878824200254941\n",
      "arg_max: 0.7703382049200016\n",
      "arg_max: 0.7707031711922705\n",
      "arg_max: 0.7784912916960774\n",
      "arg_max: 0.7612831901513271\n",
      "arg_max: 0.7428187212704124\n",
      "arg_max: 0.741463949507755\n",
      "arg_max: 0.7469724995831847\n",
      "arg_max: 0.7633769619685276\n",
      "arg_max: 0.7581240721694955\n",
      "arg_max: 0.7660532444794454\n",
      "arg_max: 0.7992791092814896\n",
      "arg_max: 0.7917946492991829\n",
      "arg_max: 0.7805777806017794\n",
      "arg_max: 0.7715907685103484\n",
      "arg_max: 0.7661762825455614\n",
      "arg_max: 0.7883985274581486\n",
      "arg_max: 0.7673607869247429\n",
      "arg_max: 0.7781377123487208\n",
      "arg_max: 0.7919511425067234\n",
      "arg_max: 0.7809476050976204\n",
      "arg_max: 0.7723658713852941\n",
      "arg_max: 0.7586785649340851\n",
      "arg_max: 0.7754629609744728\n",
      "arg_max: 0.7720453480325653\n",
      "arg_max: 0.7873041841012114\n",
      "arg_max: 0.7861869065817209\n",
      "arg_max: 0.7960533152395184\n",
      "arg_max: 0.7784832844356216\n",
      "arg_max: 0.7974270101481833\n",
      "arg_max: 0.7628560268893718\n",
      "arg_max: 0.7722346102685311\n",
      "arg_max: 0.7842935604564717\n",
      "arg_max: 0.7846381027536093\n",
      "arg_max: 0.7900873181131536\n",
      "arg_max: 0.7725150575173272\n",
      "arg_max: 0.7741919972974967\n",
      "arg_max: 0.7719984840808963\n",
      "arg_max: 0.7659267704310604\n",
      "arg_max: 0.7844950369674195\n",
      "arg_max: 0.7845392997767986\n",
      "arg_max: 0.7742841867834661\n",
      "arg_max: 0.7963187145069328\n",
      "arg_max: 0.7957998130014695\n",
      "arg_max: 0.7984584176483629\n",
      "arg_max: 0.798871276428359\n",
      "arg_max: 0.7818723258606797\n",
      "arg_max: 0.7738969675371956\n",
      "arg_max: 0.7790901647990819\n",
      "arg_max: 0.7686757348889275\n",
      "arg_max: 0.7881350020270091\n",
      "arg_max: 0.7629154084868496\n",
      "arg_max: 0.8705465846445856\n",
      "arg_max: 0.7784619443414427\n",
      "arg_max: 0.7700149881369274\n",
      "arg_max: 0.7635423229349975\n",
      "arg_max: 0.7702009038888215\n",
      "arg_max: 0.7641887193086174\n",
      "arg_max: 0.7644693996813655\n",
      "arg_max: 0.7642875283534456\n",
      "arg_max: 0.7563112517970709\n",
      "arg_max: 0.7638509007719433\n",
      "arg_max: 0.7589194487630784\n",
      "arg_max: 0.78867519724928\n",
      "arg_max: 0.7776299298554274\n",
      "arg_max: 0.7784182614067109\n",
      "arg_max: 0.7861809512065611\n",
      "arg_max: 0.7663965852684859\n",
      "arg_max: 0.7737276529818555\n",
      "arg_max: 0.7754751921304752\n",
      "arg_max: 0.7668022881422599\n",
      "arg_max: 0.7781704500151895\n",
      "arg_max: 0.7935513907211549\n",
      "arg_max: 0.8199852405389088\n",
      "arg_max: 0.7896629838946652\n",
      "arg_max: 0.7704583004975688\n",
      "arg_max: 0.7534512449968427\n",
      "arg_max: 0.7738756089228592\n",
      "arg_max: 0.7630979547179964\n",
      "arg_max: 0.7617983848180727\n",
      "arg_max: 0.7646035395928219\n",
      "arg_max: 0.7686039365779205\n",
      "arg_max: 0.749575075772799\n",
      "arg_max: 0.7618347715991629\n",
      "arg_max: 0.8028551421926992\n",
      "arg_max: 0.7923037819300901\n",
      "arg_max: 0.7871723111030337\n",
      "arg_max: 0.7788907931269113\n",
      "arg_max: 0.7975595186591966\n",
      "arg_max: 0.7848942373952115\n",
      "arg_max: 0.7793329230111856\n",
      "arg_max: 0.7844304745614424\n",
      "arg_max: 0.7811867869905963\n",
      "arg_max: 0.7878482078264064\n",
      "arg_max: 0.7846734116561107\n",
      "arg_max: 0.8008000165915384\n",
      "arg_max: 0.7835445124258826\n",
      "arg_max: 0.7917966872572717\n",
      "arg_max: 0.7786854380607324\n",
      "arg_max: 0.7782433955203608\n",
      "arg_max: 0.7621612299602855\n",
      "arg_max: 0.7800849002269268\n",
      "arg_max: 0.7676200004966218\n",
      "arg_max: 0.7658383219883077\n",
      "arg_max: 0.77601857959179\n",
      "arg_max: 0.7664289022020231\n",
      "arg_max: 0.7808553845846987\n",
      "arg_max: 0.7706615209173096\n",
      "arg_max: 0.7536352362001543\n",
      "arg_max: 0.7663076964846273\n",
      "arg_max: 0.7687690775087277\n",
      "arg_max: 0.7599874005796382\n",
      "arg_max: 0.7724974134765903\n",
      "arg_max: 0.7783938947977116\n",
      "arg_max: 0.7414624776052231\n",
      "arg_max: 0.7731502760210215\n",
      "arg_max: 0.8094979940862616\n",
      "arg_max: 0.8068696642042424\n",
      "arg_max: 0.8184468078006872\n",
      "arg_max: 0.7958258021090457\n",
      "arg_max: 0.7683508734808577\n",
      "arg_max: 0.7855801666838681\n",
      "arg_max: 0.7691687454078404\n",
      "arg_max: 0.7588223570225884\n",
      "arg_max: 0.7661048918529869\n",
      "arg_max: 0.7639415071087754\n",
      "arg_max: 0.7455943495488082\n",
      "arg_max: 0.7491071965946304\n",
      "arg_max: 0.7870348938035703\n",
      "arg_max: 0.7550359682112833\n",
      "arg_max: 0.7764933527495744\n",
      "arg_max: 0.7795041918302716\n",
      "arg_max: 0.789819943850805\n",
      "arg_max: 0.7780322036526034\n",
      "arg_max: 0.8051972299666303\n",
      "arg_max: 0.767560557846841\n",
      "arg_max: 0.7591502384464146\n",
      "arg_max: 0.7655500646260922\n",
      "arg_max: 0.7448704618964175\n",
      "arg_max: 0.7615357213752989\n",
      "arg_max: 0.7621263155954414\n",
      "arg_max: 0.7958299688107757\n",
      "arg_max: 0.792829191998012\n",
      "arg_max: 0.7988610231180793\n",
      "arg_max: 0.7911972233119182\n",
      "arg_max: 0.7947716711176929\n",
      "arg_max: 0.8025549746471282\n",
      "arg_max: 0.7964855815493699\n",
      "arg_max: 0.7931696295774608\n",
      "arg_max: 0.7880466186564505\n",
      "arg_max: 0.7747186641210475\n",
      "arg_max: 0.7693121861458437\n",
      "arg_max: 0.8326303725297723\n",
      "arg_max: 0.831122320811305\n",
      "arg_max: 0.8191491167024252\n",
      "arg_max: 0.7755656564049918\n",
      "arg_max: 0.825393983454537\n",
      "arg_max: 0.8563791411271806\n",
      "arg_max: 0.8162860228833302\n",
      "arg_max: 0.8416335831147084\n",
      "arg_max: 0.7652858183055723\n",
      "arg_max: 0.7828725196785575\n",
      "arg_max: 0.8224083682979915\n",
      "arg_max: 0.8390686268663436\n",
      "arg_max: 0.808349840983901\n",
      "arg_max: 0.8000362124182928\n",
      "arg_max: 0.7672172502831057\n",
      "arg_max: 0.773697112591485\n",
      "arg_max: 0.7902253483871192\n",
      "arg_max: 0.7967551647994783\n"
     ]
    }
   ],
   "source": [
    "# a.runSubstringAnalysis(\"FACT_EMBEDDINGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fa97ee4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrunSubstringAnalysis\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCROSS_FACTS\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AICompatibleWEB/tool/analysis_processor.py:641\u001b[39m, in \u001b[36mAnalysisProcessor.runSubstringAnalysis\u001b[39m\u001b[34m(self, substring_mode)\u001b[39m\n\u001b[32m    638\u001b[39m all_sec_hashes = []\n\u001b[32m    640\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _policy_name, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.policy_data.items():\n\u001b[32m--> \u001b[39m\u001b[32m641\u001b[39m \t\u001b[38;5;28;01mfor\u001b[39;00m _policy_hash, w \u001b[38;5;129;01min\u001b[39;00m v:\n\u001b[32m    642\u001b[39m \t\t\u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m w[\u001b[33m\"\u001b[39m\u001b[33mpolicy_chunks\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    643\u001b[39m \t\t\tall_secs.append(\u001b[38;5;28mlist\u001b[39m(i[\u001b[33m\"\u001b[39m\u001b[33mchunk_content\u001b[39m\u001b[33m\"\u001b[39m].values())[\u001b[32m0\u001b[39m])\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "a.runSubstringAnalysis(\"CROSS_FACTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4888cd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanString = lambda x: re.sub(r\"[^a-zA-Z]\", \"\", re.sub(r\"\\\\u[0-9a-fA-F]{4}\", \"\", x))\n",
    "\n",
    "\n",
    "def getMaxMatchSubstring(substring, superstring):\n",
    "\tclean_sub = cleanString(substring)\n",
    "\n",
    "\tlow = 0\n",
    "\thigh = len(clean_sub)\n",
    "\tbest_match = None\n",
    "\twhile low <= high:\n",
    "\t\tmid = (low + high) // 2\n",
    "\t\tcurrent = clean_sub[:mid]\n",
    "\n",
    "\t\tif current in superstring:\n",
    "\t\t\tbest_match = current\n",
    "\t\t\tlow = mid + 1\n",
    "\t\telse:\n",
    "\t\t\thigh = mid - 1\n",
    "\n",
    "\treturn best_match\n",
    "\n",
    "\n",
    "def getCleanedMatch(sub_match, substring, superstring):\n",
    "\n",
    "\ti = superstring.find(sub_match)\n",
    "\tif i == -1:\n",
    "\t\treturn None\n",
    "\n",
    "\ttarget_alpha_len = len(cleanString(substring))\n",
    "\n",
    "\tif target_alpha_len == 0:\n",
    "\t\treturn None\n",
    "\n",
    "\tlow = 0\n",
    "\thigh = len(superstring)\n",
    "\tend_index = high\n",
    "\n",
    "\twhile low <= high:\n",
    "\t\tmid = (low + high) // 2\n",
    "\n",
    "\t\tcurrent_alpha_len = len(cleanString(superstring[i:mid]))\n",
    "\n",
    "\t\tif current_alpha_len >= target_alpha_len:\n",
    "\n",
    "\t\t\tend_index = mid\n",
    "\t\t\thigh = mid - 1\n",
    "\t\telse:\n",
    "\t\t\tlow = mid + 1\n",
    "\n",
    "\tif high == 0:\n",
    "\t\treturn None\n",
    "\n",
    "\treturn superstring[i:end_index]\n",
    "\n",
    "\n",
    "# [TODO]:\n",
    "# - Supplemental RAG retreval\n",
    "# \t- add alt embeddings for RETRIEVAL_DOCUMENT\n",
    "# - Better question generation, (more general)\n",
    "# - Clustering, filtering and pruning questions\n",
    "# - Re-analysis mode\n",
    "# - Make getCleanedMatch better for parts where we get multiple matches of substring edge case\n",
    "# - Make extractContent more robust\n",
    "# - Thinkabout where we can parellelise API calls\n",
    "\n",
    "\n",
    "import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "import re\n",
    "import datetime\n",
    "import hashlib\n",
    "import ast\n",
    "from question_analysis import LLMInterface\n",
    "from model_management import GeminiModel\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from google.genai import types\n",
    "import concurrent.futures\n",
    "\n",
    "QUESTIONS_FILE = \"./questions_filter_after.json\"\n",
    "POLICIES_FILE = \"./policies_testing.json\"\n",
    "\n",
    "\n",
    "def splitParargraphs(input_string):\n",
    "\treturn input_string.split(\"\\n\\n\")\n",
    "\n",
    "\n",
    "def splitNewlines(input_string):\n",
    "\twhile \"\\n\\n\" in input_string:\n",
    "\t\tinput_string = input_string.replace(\"\\n\\n\", \"\\n\")\n",
    "\treturn input_string.split(\"\\n\")\n",
    "\n",
    "\n",
    "def __loadJson(filepath):\n",
    "\tif not os.path.exists(filepath):\n",
    "\t\treturn {}\n",
    "\ttry:\n",
    "\t\twith open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "\t\t\treturn json.load(f)\n",
    "\texcept json.JSONDecodeError:\n",
    "\t\treturn {}\n",
    "\n",
    "\n",
    "def __saveJson(filepath, data):\n",
    "\twith open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "\t\tjson.dump(data, f, indent=4, default=str)\n",
    "\n",
    "\n",
    "temp_cache_questions = __loadJson(QUESTIONS_FILE)\n",
    "\n",
    "\n",
    "def _loadJson(filepath):\n",
    "\tif not os.path.exists(filepath):\n",
    "\t\treturn {}\n",
    "\ttry:\n",
    "\t\twith open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "\t\t\treturn json.load(f)\n",
    "\texcept json.JSONDecodeError:\n",
    "\t\treturn {}\n",
    "\n",
    "\n",
    "def _saveJson(filepath, data):\n",
    "\twith open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "\t\tjson.dump(data, f, indent=4, default=str)\n",
    "\n",
    "\n",
    "def loadQuestions(debug=True):\n",
    "\tif debug:\n",
    "\t\treturn temp_cache_questions\n",
    "\treturn _loadJson(QUESTIONS_FILE)\n",
    "\n",
    "\n",
    "def loadPolicies():\n",
    "\treturn _loadJson(POLICIES_FILE)\n",
    "\n",
    "\n",
    "def updateQuestionEntry(question_text, new_policy_data):\n",
    "\tdata = _loadJson(QUESTIONS_FILE)\n",
    "\tdata[question_text].update(new_policy_data)\n",
    "\t_saveJson(QUESTIONS_FILE, data)\n",
    "\n",
    "\n",
    "def updateQuestionEntry2(question_text, policy_hash, chunk_hash, new_policy_data):\n",
    "\tdata = _loadJson(QUESTIONS_FILE)\n",
    "\tdata[question_text][policy_hash][chunk_hash].update(new_policy_data)\n",
    "\t_saveJson(QUESTIONS_FILE, data)\n",
    "\n",
    "\n",
    "def addNewQuestionChunk(question_text, policy_hash, chunk_hash, debug=True):\n",
    "\tif debug:\n",
    "\t\ttemp_cache_questions[question_text][\"policy_data\"][policy_hash].update(\n",
    "\t\t\t{chunk_hash: []}\n",
    "\t\t)\n",
    "\n",
    "\telse:\n",
    "\t\tdata = _loadJson(QUESTIONS_FILE)\n",
    "\n",
    "\t\tdata[question_text][\"policy_data\"][policy_hash].update({chunk_hash: []})\n",
    "\t\t_saveJson(QUESTIONS_FILE, data)\n",
    "\n",
    "\n",
    "def addNewQuestion(question_text, question_data, debug=True):\n",
    "\tif debug:\n",
    "\t\ttemp_cache_questions[question_text] = question_data\n",
    "\telse:\n",
    "\t\tdata = _loadJson(QUESTIONS_FILE)\n",
    "\t\tdata[question_text] = question_data\n",
    "\t\t_saveJson(QUESTIONS_FILE, data)\n",
    "\n",
    "\n",
    "def updateQuestionChunk(\n",
    "\tquestion_text, policy_hash, chunk_hash, new_substring_data, debug=True\n",
    "):\n",
    "\tif new_substring_data == []:\n",
    "\t\treturn\n",
    "\tif debug:\n",
    "\t\ttemp_cache_questions[question_text][\"policy_data\"][policy_hash][chunk_hash].append(\n",
    "\t\t\tnew_substring_data\n",
    "\t\t)\n",
    "\telse:\n",
    "\t\tdata = _loadJson(QUESTIONS_FILE)\n",
    "\t\tdata[question_text][\"policy_data\"][policy_hash][chunk_hash].append(new_substring_data)\n",
    "\t\t_saveJson(QUESTIONS_FILE, data)\n",
    "\n",
    "\n",
    "def updateQuestionChunk(question_text, policy_hash, chunk_hash, new_substring_data):\n",
    "\tif new_substring_data == []:\n",
    "\t\treturn\n",
    "\tdata = _loadJson(QUESTIONS_FILE)\n",
    "\tdata[question_text][\"policy_data\"][policy_hash][chunk_hash].append(new_substring_data)\n",
    "\t_saveJson(QUESTIONS_FILE, data)\n",
    "\n",
    "\n",
    "def updatePolicyEntry(policy_name, new_version_data):\n",
    "\tdata = _loadJson(POLICIES_FILE)\n",
    "\tdata[policy_name].update(new_version_data)\n",
    "\t_saveJson(POLICIES_FILE, data)\n",
    "\n",
    "\n",
    "def addNewPolicy(policy_name, policy_data):\n",
    "\tdata = _loadJson(POLICIES_FILE)\n",
    "\tdata[policy_name] = policy_data\n",
    "\t_saveJson(POLICIES_FILE, data)\n",
    "\n",
    "\n",
    "def isInString(string, substring):\n",
    "\treturn substring in string\n",
    "\n",
    "\n",
    "def getSubstringIndices(string, substring):\n",
    "\tstart_index = string.find(substring)\n",
    "\tend_index = start_index + len(substring)\n",
    "\treturn (start_index, end_index)\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "class AnalysisProcessor:\n",
    "\n",
    "\tmodes = [\"ANALYSIS\", \"DEBUG_QUESTION_GENERATION\"]\n",
    "\tsubstring_modes = [\"PROMPT\", \"FACT_EMBEDDINGS\", \"TESTING\"]\n",
    "\n",
    "\tuseRetreval = True\n",
    "\tMARKDOWN_LINK_PATTERN = re.compile(r\"(\\[.*?\\])\\((.*?)\\)\")\n",
    "\tURL_PLACEHOLDER = \"(DYNAMIC_URL_REMOVED)\"\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef extractContent(url, headers=None):\n",
    "\t\tif headers is None:\n",
    "\t\t\theaders = {\n",
    "\t\t\t\t\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\tresponse = requests.get(url, headers=headers, timeout=10)\n",
    "\t\t\tresponse.raise_for_status()\n",
    "\t\t\thtml_content = response.text\n",
    "\n",
    "\t\t\treturn BeautifulSoup(html_content, \"lxml\").find(\"main\")\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef extractMarkdown(main_content):\n",
    "\t\treturn md(str(main_content), heading_style=\"ATX\")\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef splitMarkdown(markdown_text):\n",
    "\t\theading_pattern = r\"^#{1,6}\\s+.*\"\n",
    "\t\tparts = re.split(heading_pattern, markdown_text, flags=re.MULTILINE)\n",
    "\t\tcontent_list = [part.strip() for part in parts[1:] if part.strip()]\n",
    "\t\treturn content_list\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef removePreamble(markdown_text):\n",
    "\t\tpattern = r\"\\A.*?(?=^#\\s)\"\n",
    "\t\tcleaned_text = re.sub(pattern, \"\", markdown_text, flags=re.DOTALL | re.MULTILINE)\n",
    "\t\treturn cleaned_text\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef normaliseMarkdownLinks(markdown_text):\n",
    "\t\tdef replacer(match):\n",
    "\t\t\treturn match.group(1) + AnalysisProcessor.URL_PLACEHOLDER\n",
    "\n",
    "\t\tnormalized_text = AnalysisProcessor.MARKDOWN_LINK_PATTERN.sub(replacer, markdown_text)\n",
    "\t\treturn normalized_text\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef getHash(data):\n",
    "\t\treturn hashlib.sha256(data.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef processSimilarity(emb_a, emb_b):\n",
    "\t\treturn np.dot(emb_a, emb_b) / (np.linalg.norm(emb_a) * np.linalg.norm(emb_b))\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef processUrl(policy_url):\n",
    "\t\traw_policy_content = AnalysisProcessor.extractContent(policy_url)\n",
    "\t\traw_markdown_content = AnalysisProcessor.extractMarkdown(raw_policy_content)\n",
    "\t\tpolicy_content = AnalysisProcessor.removePreamble(raw_markdown_content)\n",
    "\t\tnormalised_content = AnalysisProcessor.normaliseMarkdownLinks(policy_content)\n",
    "\t\tpolicy_hash = AnalysisProcessor.getHash(normalised_content)\n",
    "\t\treturn policy_content, policy_hash\n",
    "\n",
    "\tdef getDuplicateQuestions(self, question_data):\n",
    "\t\t# saved_data = loadQuestions()\n",
    "\t\tpreprocessed_questions = []\n",
    "\t\tfor i, q in enumerate(question_data):\n",
    "\t\t\tembedding_vector = q[\"embedding_vector\"]\n",
    "\t\t\tquestion = q[\"question\"]\n",
    "\t\t\tif question in self.question_data:\n",
    "\t\t\t\tprint(f\"Exact duplicated: {question}\")\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tfor k, v in self.question_data.items():\n",
    "\t\t\t\ttrail_embedding = v[\"embedding_vector\"]\n",
    "\n",
    "\t\t\t\tif AnalysisProcessor.processSimilarity(embedding_vector, trail_embedding) > 0.98:\n",
    "\t\t\t\t\tprint(f\"Near duplicate found: '{k}','{question}'\")\t# works suprisingly well\n",
    "\n",
    "\t\t\t\t\tpreprocessed_questions.append([i, k])\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\treturn preprocessed_questions\n",
    "\n",
    "\tdef isStored(self, policy_name, policy_hash):\n",
    "\t\tpolicy_data = loadPolicies()\n",
    "\t\treturn (policy_name in policy_data) and (policy_hash in policy_data[policy_name])\n",
    "\n",
    "\tdef swapSeenQuestions(self, with_embeddings):\n",
    "\n",
    "\t\t_ = list(with_embeddings.keys())[0]\n",
    "\t\tquestion_data = with_embeddings[_]\n",
    "\t\tpreprocessed_questions = self.getDuplicateQuestions(question_data)\n",
    "\n",
    "\t\tmodified_question_set = with_embeddings\t# ast.literal_eval(_question_set)\n",
    "\t\tquestion_dict_key = list(modified_question_set.keys())[0]\n",
    "\t\tmodified_question_data = modified_question_set[question_dict_key]\n",
    "\t\tfor i in preprocessed_questions:\n",
    "\t\t\tmodified_question_data[i[0]].update({\"question\": i[1]})\n",
    "\n",
    "\t\treturn modified_question_data\n",
    "\n",
    "\tdef processChunks(self, policy_content):\n",
    "\t\tproto_chunks = AnalysisProcessor.splitMarkdown(policy_content)\n",
    "\t\t_embeddings = self.analysis_model.client.models.embed_content(\n",
    "\t\t\tmodel=\"gemini-embedding-001\",\n",
    "\t\t\tcontents=proto_chunks,\n",
    "\t\t\tconfig=types.EmbedContentConfig(task_type=\"RETRIEVAL_DOCUMENT\"),\n",
    "\t\t)\n",
    "\t\tembeddings = [e for e in _embeddings.embeddings]\n",
    "\t\tchunks = []\n",
    "\t\tfor i, chunk in enumerate(proto_chunks):\n",
    "\t\t\t# if self.useRetreval and type(embeddings == list):\n",
    "\t\t\tchunks.append(\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"chunk_content\": {AnalysisProcessor.getHash(chunk): chunk},\n",
    "\t\t\t\t\t\"retrieval_embedding_vector\": embeddings[i].values,\n",
    "\t\t\t\t}\n",
    "\t\t\t)\n",
    "\n",
    "\t\treturn chunks\n",
    "\n",
    "\tdef __init__(self, data_source, mode=0, substring_mode=1):\n",
    "\t\t_mode = 0\n",
    "\t\tif type(mode) == int and mode < len(self.modes):\n",
    "\t\t\t_mode = mode\n",
    "\t\t_substring_mode = 1\n",
    "\t\tif type(substring_mode) == int and substring_mode < len(self.substring_modes):\n",
    "\t\t\t_substring_mode = substring_mode\n",
    "\t\tself.substring_mode = self.substring_modes[_substring_mode]\n",
    "\t\tself.mode = self.modes[_mode]\n",
    "\t\tself.data_source = data_source\n",
    "\t\tself.analysis_model = GeminiModel()\n",
    "\t\tself.model_interface = LLMInterface(self.analysis_model)\n",
    "\t\tself.loadData()\n",
    "\n",
    "\tdef updatePolicyData(self, policy_content, policy_chunks, policy_name, policy_hash):\n",
    "\t\tfetch_date = datetime.datetime.now().__str__()\n",
    "\n",
    "\t\tpolicy_data = {\n",
    "\t\t\tpolicy_hash: {\n",
    "\t\t\t\t\"policy_content\": policy_content,\n",
    "\t\t\t\t\"policy_chunks\": policy_chunks,\n",
    "\t\t\t\t\"fetch_date\": fetch_date,\n",
    "\t\t\t}\n",
    "\t\t}\n",
    "\t\tif policy_name in loadPolicies():\n",
    "\t\t\tself.updatePolicyEntry(policy_name, policy_data)\n",
    "\t\telse:\n",
    "\t\t\tself.addNewPolicy(policy_name, policy_data)\n",
    "\n",
    "\tdef addNewPolicy(self, policy_name, policy_data):\n",
    "\t\tself.policy_data[policy_name] = policy_data\n",
    "\n",
    "\tdef loadData(self):\n",
    "\t\tself.question_data = loadQuestions(debug=False)\n",
    "\t\tself.policy_data = loadPolicies()\n",
    "\n",
    "\tdef saveData(self):\n",
    "\t\t_saveJson(QUESTIONS_FILE, self.question_data)\n",
    "\t\t_saveJson(POLICIES_FILE, self.policy_data)\n",
    "\n",
    "\tdef addNewQuestionChunk(self, question_text, policy_hash, chunk_hash, debug=True):\n",
    "\n",
    "\t\tself.question_data[question_text][\"policy_data\"][policy_hash].update({chunk_hash: []})\n",
    "\n",
    "\tdef addNewQuestion(self, question_text, question_data, debug=True):\n",
    "\n",
    "\t\tself.question_data[question_text] = question_data\n",
    "\n",
    "\tdef updateQuestionChunk(\n",
    "\t\tself, question_text, policy_hash, chunk_hash, new_substring_data\n",
    "\t):\n",
    "\n",
    "\t\tself.question_data[question_text][\"policy_data\"][policy_hash][chunk_hash].append(\n",
    "\t\t\tnew_substring_data\n",
    "\t\t)\n",
    "\n",
    "\tdef updatePolicyEntry(self, policy_name, new_version_data):\n",
    "\t\t# data = _loadJson(POLICIES_FILE)\n",
    "\t\tself.policy_data[policy_name].update(new_version_data)\n",
    "\t\t# _saveJson(POLICIES_FILE, data)\n",
    "\n",
    "\tdef filterDuplicates(self):\n",
    "\t\tself.loadData()\n",
    "\t\told_data = self.question_data.copy()\n",
    "\t\t# clean_data = {} # will be question, embedding_vector\n",
    "\t\tindexed_questions = []\n",
    "\t\tindexed_embeddings = []\n",
    "\t\tfor k, v in old_data.items():\n",
    "\t\t\tindexed_questions.append(k)\n",
    "\t\t\tindexed_embeddings.append(v[\"embedding_vector\"])\n",
    "\t\tarray_indexed_embeddings = np.array(indexed_embeddings)\n",
    "\t\tarray_indexed_questions = np.array(indexed_questions)\n",
    "\n",
    "\t\tsim_matrix = array_indexed_embeddings @ array_indexed_embeddings.T\n",
    "\n",
    "\t\tis_duplicate_matrix = np.triu(sim_matrix > 0.98, k=1)\n",
    "\n",
    "\t\tduplicates_mask = np.any(is_duplicate_matrix, axis=0)\n",
    "\n",
    "\t\tmasters_all = np.argmax(is_duplicate_matrix, axis=0)\n",
    "\n",
    "\t\tindices_to_drop = np.where(duplicates_mask)[0]\n",
    "\t\tassociated_masters = masters_all[duplicates_mask]\n",
    "\t\tstrings_to_pop = array_indexed_questions[indices_to_drop]\n",
    "\t\tstrings_overrides = array_indexed_questions[associated_masters]\n",
    "\n",
    "\t\tprint(f\"Strings to remove:\")\n",
    "\n",
    "\t\tfor i, key in enumerate(strings_to_pop):\n",
    "\t\t\tprint(f\"duplicate: {key}, main:{strings_overrides[i]}\")\n",
    "\t\t\tpolicy_data = self.question_data[key][\"policy_data\"]\n",
    "\t\t\tfor k, v in policy_data.items():\n",
    "\t\t\t\tif k in self.question_data[strings_overrides[i]][\"policy_data\"]:\n",
    "\t\t\t\t\tprint(f\"updating: {self.question_data[strings_overrides[i]][\"policy_data\"][k]}\")\n",
    "\t\t\t\t\tprint(f\"with: {v}\")\n",
    "\t\t\t\t\tself.question_data[strings_overrides[i]][\"policy_data\"][k].update(v)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(f\"adding: {dict({k:v})}\")\n",
    "\t\t\t\t\tprint(f\"to: {self.question_data[strings_overrides[i]][\"policy_data\"]}\")\n",
    "\n",
    "\t\t\t\t\tself.question_data[strings_overrides[i]][\"policy_data\"].update({k: v})\n",
    "\t\t\tpolicy_data = self.question_data.pop(key)\n",
    "\t\tself.saveData()\n",
    "\n",
    "\tdef processPolicyChunks(self, chunk, policy_hash, policy_name=None, index=0):\n",
    "\t\tprint(f\"STARTING chunk {index} in {policy_name}\")\n",
    "\t\tchunk_hash = list(chunk[\"chunk_content\"].keys())[0]\n",
    "\t\tchunk_content = list(chunk[\"chunk_content\"].values())[0]\n",
    "\n",
    "\t\t_question_set: str = self.model_interface.generateQuestions(chunk_content)\n",
    "\t\tstart_time = time.time()\n",
    "\t\twith_embeddings: dict = self.model_interface.processEmbeddings(\n",
    "\t\t\t_question_set, add_facts=False\n",
    "\t\t)\n",
    "\t\treformed_question_set = list(with_embeddings.values())[0]\n",
    "\t\tfor i in reformed_question_set:\n",
    "\t\t\tquestion = i[\"question\"]\n",
    "\t\t\tif question in self.question_data:\n",
    "\t\t\t\tif policy_hash in self.question_data[question][\"policy_data\"]:\n",
    "\t\t\t\t\tif chunk_hash in self.question_data[question][\"policy_data\"][policy_hash]:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tself.addNewQuestionChunk(question, policy_hash, chunk_hash)\n",
    "\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.addNewQuestion(\n",
    "\t\t\t\t\tquestion,\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\"embedding_vector\": i[\"embedding_vector\"],\n",
    "\t\t\t\t\t\t\"policy_data\": {policy_hash: {chunk_hash: []}},\n",
    "\t\t\t\t\t},\n",
    "\t\t\t\t)\n",
    "\t\tprint(f\"FINISHED chunk {index} in {policy_name}\")\n",
    "\n",
    "\tdef processPolicy(self, policy_name, policy_url):\n",
    "\t\tprint(f\"STARTING policy processing {policy_name}\")\n",
    "\n",
    "\t\tpolicy_content, policy_hash = AnalysisProcessor.processUrl(policy_url)\n",
    "\n",
    "\t\tpolicy_chunks = self.processChunks(policy_content)\n",
    "\t\tself.updatePolicyData(policy_content, policy_chunks, policy_name, policy_hash)\n",
    "\n",
    "\t\titerations = 0\n",
    "\t\tstart_time = 0\n",
    "\t\tnum_policy_chunks = len(policy_chunks)\n",
    "\t\tprint(f\"total chunks for {policy_name}: {num_policy_chunks}\")\n",
    "\t\texecutor = concurrent.futures.ThreadPoolExecutor(max_workers=num_policy_chunks)\n",
    "\t\te_dict = dict()\n",
    "\t\tfor index, chunk in enumerate(policy_chunks):\n",
    "\t\t\te_dict[index] = executor.submit(\n",
    "\t\t\t\tself.processPolicyChunks, chunk, policy_hash, policy_name, index\n",
    "\t\t\t)\n",
    "\t\texecutor.shutdown(wait=True)\n",
    "\t\tprint(f\"FINISHED ALL chunks for {policy_name}\")\n",
    "\n",
    "\t\t# self.saveData()\n",
    "\n",
    "\tdef addFactEmbeddings(self):\n",
    "\t\tquestion_emb = self.model_interface.processFactEmbeddings(self.question_data)\n",
    "\t\tfor i in question_emb:\n",
    "\t\t\tself.question_data[i[0]].update({\"retrieval_embedding_vector\": i[1]})\n",
    "\n",
    "\tdef runAnalyses(self, mode=None, limit_iterations=None):\n",
    "\t\tself.loadData()\n",
    "\t\tif mode is not None and mode in self.modes:\n",
    "\t\t\tself.mode = mode\n",
    "\t\tprint(\"STARTING ALL policy processing\")\n",
    "\n",
    "\t\tpolicies_total = len(list(self.data_source.keys()))\n",
    "\t\texecutor = concurrent.futures.ThreadPoolExecutor(max_workers=policies_total)\n",
    "\t\te = dict()\n",
    "\t\tfor policy_name, policy_url in self.data_source.items():\n",
    "\t\t\te[f\"{[policy_name]}_{policy_url}\"] = executor.submit(\n",
    "\t\t\t\tself.processPolicy, policy_name, policy_url\n",
    "\t\t\t)\n",
    "\t\texecutor.shutdown(wait=True)\n",
    "\t\tprint(\"FINISHED PROCESSING ALL Policies\")\n",
    "\t\tself.saveData()\n",
    "\n",
    "\t\tself.filterDuplicates()\n",
    "\t\tprint(\"FINISHED FILTERING Retrieval Embeddings\")\n",
    "\n",
    "\t\tself.addFactEmbeddings()\n",
    "\t\tprint(\"FINISHED adding Retrieval Embeddings\")\n",
    "\t\tself.saveData()\n",
    "\n",
    "\tdef getUpdates(self):\n",
    "\t\tself.loadData()\n",
    "\t\tneeds_facts = []\n",
    "\t\tneeds_prompts = []\n",
    "\t\tfor k, v in self.question_data.items():\n",
    "\t\t\tpolicy_data = v[\"policy_data\"]\n",
    "\t\t\tfor l, w in policy_data.items():\n",
    "\t\t\t\tfor m, x in w.items():\n",
    "\t\t\t\t\tif type(x) != list or len(x) == 0:\n",
    "\t\t\t\t\t\tneeds_facts.append([k, l, m])\n",
    "\t\t\t\t\t\tneeds_prompts.append([k, l, m])\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\thas_facts = False\n",
    "\t\t\t\t\t\thas_prompts = False\n",
    "\t\t\t\t\t\tfor i in x:\n",
    "\t\t\t\t\t\t\tif has_facts and has_prompts:\n",
    "\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\t\tif \"method\" not in i:\n",
    "\t\t\t\t\t\t\t\tneeds_facts.append([k, l, m])\n",
    "\t\t\t\t\t\t\t\tneeds_prompts.append([k, l, m])\n",
    "\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\t\tmethod = i[\"method\"]\n",
    "\t\t\t\t\t\t\t\tif method == \"PROMPT\":\n",
    "\t\t\t\t\t\t\t\t\thas_prompts = True\n",
    "\t\t\t\t\t\t\t\telif method == \"FACT_EMBEDDINGS\":\n",
    "\t\t\t\t\t\t\t\t\thas_facts = True\n",
    "\t\t\t\t\t\tif not has_facts:\n",
    "\t\t\t\t\t\t\tneeds_facts.append([k, l, m])\n",
    "\t\t\t\t\t\tif not has_prompts:\n",
    "\t\t\t\t\t\t\tneeds_prompts.append([k, l, m])\n",
    "\t\treturn needs_facts, needs_prompts\n",
    "\n",
    "\tdef runSubstringAnalysis(self, substring_mode=None):\n",
    "\t\tself.loadData()\n",
    "\n",
    "\t\tif substring_mode is not None and substring_mode in self.substring_modes:\n",
    "\t\t\tself.substring_mode = substring_mode\n",
    "\t\tif self.substring_mode == \"PROMPT\":\n",
    "\t\t\tpass\n",
    "\t\telif self.substring_mode == \"FACT_EMBEDDINGS\":\n",
    "\t\t\t# similarly use whole question retrieval_embedding_vector to identify any other missed duplicates\n",
    "\t\t\tneeds_facts, _ = self.getUpdates()\n",
    "\t\t\t# policy_data = loadPolicies()\n",
    "\t\t\t# question_data = loadQuestions()\n",
    "\n",
    "\t\t\tfor item in needs_facts:\n",
    "\t\t\t\tpolicy = None\n",
    "\t\t\t\tchunk = None\n",
    "\t\t\t\tfor k, v in self.policy_data.items():\n",
    "\t\t\t\t\tif item[1] in v:\n",
    "\t\t\t\t\t\tpolicy = v[item[1]]\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\tif policy is not None:\n",
    "\t\t\t\t\tchunks = policy[\"policy_chunks\"]\n",
    "\t\t\t\t\tfor c in chunks:\n",
    "\t\t\t\t\t\tchunk_data = c[\"chunk_content\"]\n",
    "\t\t\t\t\t\tif item[2] in chunk_data:\n",
    "\t\t\t\t\t\t\tchunk = chunk_data[item[2]]\n",
    "\t\t\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\t\tif chunk is not None:\n",
    "\t\t\t\t\t\tchunk_lines = splitNewlines(chunk)\n",
    "\t\t\t\t\t\tfact_embedding = self.question_data[item[0]][\"retrieval_embedding_vector\"]\n",
    "\t\t\t\t\t\tchunk_embeddings: list = [\n",
    "\t\t\t\t\t\t\te.values\n",
    "\t\t\t\t\t\t\tfor e in self.analysis_model.client.models.embed_content(\n",
    "\t\t\t\t\t\t\t\tmodel=\"gemini-embedding-001\",\n",
    "\t\t\t\t\t\t\t\tcontents=chunk_lines,\n",
    "\t\t\t\t\t\t\t\tconfig=types.EmbedContentConfig(task_type=\"RETRIEVAL_DOCUMENT\"),\n",
    "\t\t\t\t\t\t\t).embeddings\n",
    "\t\t\t\t\t\t]\n",
    "\t\t\t\t\t\tmat = np.dot(chunk_embeddings, fact_embedding)\n",
    "\t\t\t\t\t\tidx = np.argmax(mat)\n",
    "\t\t\t\t\t\tprint(mat[idx])\n",
    "\t\t\t\t\t\tsubstring = chunk_lines[idx]\n",
    "\t\t\t\t\t\tsubstring_indices = getSubstringIndices(policy[\"policy_content\"], substring)\n",
    "\t\t\t\t\t\tquestion_update = {\n",
    "\t\t\t\t\t\t\t\"substring\": substring,\n",
    "\t\t\t\t\t\t\t\"substring_indices\": substring_indices,\n",
    "\t\t\t\t\t\t\t\"method\": \"FACT_EMBEDDINGS\",\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\t\t\t\tself.updateQuestionChunk(*item, question_update)\n",
    "\t\t\t\tself.saveData()\n",
    "\n",
    "\t\telif self.substring_mode == \"TESTING\":\n",
    "\t\t\tneeds_facts, needs_prompts = AnalysisProcessor.getUpdates()\n",
    "\t\t\tprint(len(needs_facts), len(needs_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ae4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = {\n",
    "\t\"openai\": \"https://openai.com/policies/privacy-policy/\",\n",
    "\t# \"anthropic\": \"https://www.anthropic.com/legal/privacy\",\n",
    "\t# \"perplexity\": \"https://www.perplexity.ai/hub/legal/privacy-policy\",\t# HTTPError: 403 Client Error: Forbidden for url: https://www.perplexity.ai/hub/legal/privacy-policy\n",
    "\t# \"deepseek\": \"https://cdn.deepseek.com/policies/en-US/deepseek-privacy-policy.html\", # Returns None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca188c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list(temp_cache_questions.keys())))\n",
    "temp_cache_questions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e38b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AnalysisProcessor(data_source)\n",
    "a.runAnalyses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b2ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AnalysisProcessor(data_source, mode=1)\n",
    "\n",
    "a.runSubstringAnalysis(\"FACT_EMBEDDINGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f945470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Setup: Create dummy data for demonstration\n",
    "# In this scenario:\n",
    "# Index 2 is a duplicate of Index 0\n",
    "# Index 3 is a duplicate of Index 1\n",
    "# Index 4 is a duplicate of Index 0 (and Index 2)\n",
    "embeddings = np.array(\n",
    "\t[\n",
    "\t\t[1.0, 0.0],\t# 0: Original A\n",
    "\t\t[0.0, 1.0],\t# 1: Original B\n",
    "\t\t[1.0, 0.0],\t# 2: Copy of A\n",
    "\t\t[0.0, 1.0],\t# 3: Copy of B\n",
    "\t\t[1.0, 0.0],\t# 4: Copy of A\n",
    "\t]\n",
    ")\n",
    "\n",
    "# 2. Normalize (Standard procedure for cosine similarity via dot product)\n",
    "norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "normalized_emb = embeddings / norms\n",
    "\n",
    "# 3. Compute Similarity Matrix\n",
    "sim_matrix = normalized_emb @ normalized_emb.T\n",
    "\n",
    "is_duplicate_matrix = np.triu(sim_matrix > 0.98, k=1)\n",
    "\n",
    "duplicates_mask = np.any(is_duplicate_matrix, axis=0)\n",
    "\n",
    "masters_all = np.argmax(is_duplicate_matrix, axis=0)\n",
    "\n",
    "indices_to_drop = np.where(duplicates_mask)[0]\n",
    "associated_masters = masters_all[duplicates_mask]\n",
    "\n",
    "# Output\n",
    "print(f\"Indices to drop:    {indices_to_drop}\")\n",
    "print(f\"Associated Masters: {associated_masters}\")\n",
    "\n",
    "# Verification Loop\n",
    "for drop, master in zip(indices_to_drop, associated_masters):\n",
    "\tprint(f\"Item {drop} is a duplicate of Item {master}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a8063e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = loadQuestions(debug=False)\n",
    "for k in qs:\n",
    "\tprint(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b254a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.runSubstringAnalysis(\"TESTING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb8da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_data = temp_cache_questions.copy()\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "\n",
    "def get_most_similar_mahalanobis(question_data):\n",
    "\t\"\"\"\n",
    "\tIdentifies the two most semantically similar strings using Mahalanobis distance.\n",
    "\n",
    "\tArgs:\n",
    "\t    question_data (dict): Dictionary of form {string: {\"embedding_vector\": [list]}}\n",
    "\n",
    "\tReturns:\n",
    "\t    tuple: (string_A, string_B, distance_score)\n",
    "\t\"\"\"\n",
    "\n",
    "\tkeys = list(question_data.keys())\n",
    "\n",
    "\tif len(keys) < 2:\n",
    "\t\treturn None, None, 0.0\n",
    "\n",
    "\tX = np.array([question_data[k][\"embedding_vector\"] for k in keys])\n",
    "\n",
    "\tcov_matrix = np.cov(X, rowvar=False)\n",
    "\n",
    "\tinv_cov_matrix = np.linalg.pinv(cov_matrix)\n",
    "\n",
    "\tcondensed_distances = pdist(X, metric=\"mahalanobis\", VI=inv_cov_matrix)\n",
    "\n",
    "\tmin_index_condensed = np.argmin(condensed_distances)\n",
    "\tmin_distance = condensed_distances[min_index_condensed]\n",
    "\n",
    "\tdist_matrix = squareform(condensed_distances)\n",
    "\n",
    "\tnp.fill_diagonal(dist_matrix, np.inf)\n",
    "\n",
    "\ti, j = np.unravel_index(np.argmin(dist_matrix), dist_matrix.shape)\n",
    "\n",
    "\treturn keys[i], keys[j], min_distance\n",
    "\n",
    "\n",
    "get_most_similar_mahalanobis(question_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
