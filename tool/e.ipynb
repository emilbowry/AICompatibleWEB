{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "67b58bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Loading Data...\n",
      ">>> Preparing Model Artifacts...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from itertools import product\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import rankdata, expon, beta\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. DATA LOADING\n",
    "# ==============================================================================\n",
    "\n",
    "QUESTIONS_FILE = \"./data/questions_filter_after.json\"\n",
    "\n",
    "\n",
    "def _loadJson(filepath):\n",
    "\tif not os.path.exists(filepath):\n",
    "\t\tprint(f\"Warning: File not found: {filepath}\")\n",
    "\t\treturn {}\n",
    "\ttry:\n",
    "\t\twith open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "\t\t\treturn json.load(f)\n",
    "\texcept json.JSONDecodeError:\n",
    "\t\tprint(f\"Error decoding JSON: {filepath}\")\n",
    "\t\treturn {}\n",
    "\n",
    "\n",
    "print(\">>> Loading Data...\")\n",
    "qdata = _loadJson(QUESTIONS_FILE)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DISTANCE & ARTIFACT PREPARATION\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def getMahalanobisDistances(vectors_a, vectors_b):\n",
    "\tnorms = np.linalg.norm(vectors_a, axis=1, keepdims=True)\n",
    "\tcleaned_vectors = vectors_a / (norms + 1e-10)\n",
    "\n",
    "\tlw = LedoitWolf()\n",
    "\tlw.fit(cleaned_vectors)\n",
    "\tprecision_matrix = lw.precision_\n",
    "\n",
    "\tdist_matrix = cdist(\n",
    "\t\tcleaned_vectors, cleaned_vectors, metric=\"mahalanobis\", VI=precision_matrix\n",
    "\t)\n",
    "\treturn dist_matrix, precision_matrix\n",
    "\n",
    "\n",
    "Distance_Processors = {\n",
    "\t\"cosine\": lambda emb_a, emb_b: 1.0\n",
    "\t- (emb_a @ emb_b.T)\n",
    "\t/ (\n",
    "\t\tnp.linalg.norm(emb_a, axis=1, keepdims=True)\n",
    "\t\t@ np.linalg.norm(emb_b, axis=1, keepdims=True).T\n",
    "\t\t+ 1e-10\n",
    "\t),\n",
    "\t\"l1\": lambda emb_a, emb_b: np.sum(np.abs(emb_a[..., np.newaxis] - emb_b.T), axis=1),\n",
    "\t\"l2\": lambda emb_a, emb_b: np.linalg.norm(emb_a[..., np.newaxis] - emb_b.T, axis=1),\n",
    "\t\"dot\": lambda emb_a, emb_b: emb_a @ emb_b.T,\n",
    "\t\"mahalanobis\": lambda emb_a, emb_b: getMahalanobisDistances(emb_a, emb_b),\n",
    "}\n",
    "\n",
    "\n",
    "def _prepareModelArtifact(\n",
    "\traw_vectors,\n",
    "\tsemantic_data,\n",
    "\ttruncation_dim=256,\n",
    "\tdistance_metric=\"mahalanobis\",\n",
    "\tdebug=True,\n",
    "):\n",
    "\tdata_matrix = np.array(raw_vectors)\n",
    "\tinput_dim = data_matrix.shape[1]\n",
    "\n",
    "\tif truncation_dim and input_dim < truncation_dim and debug:\n",
    "\t\tprint(\n",
    "\t\t\tf\"Warning: Vector dimension ({input_dim}) < limit ({truncation_dim}). No truncation.\"\n",
    "\t\t)\n",
    "\tdata_truncated = data_matrix\n",
    "\tif truncation_dim:\n",
    "\t\tdata_truncated = data_matrix[:, :truncation_dim]\n",
    "\n",
    "\tdist_output = Distance_Processors[distance_metric](data_truncated, data_truncated)\n",
    "\n",
    "\tprecision_matrix = None\n",
    "\tif distance_metric == \"mahalanobis\":\n",
    "\t\tdist_matrix, precision_matrix = dist_output\n",
    "\telse:\n",
    "\t\tdist_matrix = dist_output\n",
    "\n",
    "\t# Calculate NN indices (excluding self) for cache usage\n",
    "\td_temp = dist_matrix.copy()\n",
    "\tnp.fill_diagonal(d_temp, float(\"inf\"))\n",
    "\tnn_indices = np.argmin(d_temp, axis=1)\n",
    "\n",
    "\treturn {\n",
    "\t\t\"dist_matrix\": dist_matrix,\n",
    "\t\t\"vectors\": data_truncated,\n",
    "\t\t\"precision\": precision_matrix,\n",
    "\t\t\"semantic_data\": semantic_data,\n",
    "\t\t\"metric\": distance_metric,\n",
    "\t\t\"nn_indices\": nn_indices,\n",
    "\t}\n",
    "\n",
    "\n",
    "def prepareModelArtifacts(\n",
    "\tdata_set, vector_keys, truncation_dim=256, distance_metric=\"mahalanobis\", debug=True\n",
    "):\n",
    "\tsemantic_data = list(data_set.keys())\n",
    "\tmodel_artifacts = {}\n",
    "\traw_vectors = {}\n",
    "\tfor key in vector_keys:\n",
    "\t\traw_vectors[key] = [data_set[s][key] for s in semantic_data]\n",
    "\n",
    "\texecutor = concurrent.futures.ThreadPoolExecutor(max_workers=len(vector_keys))\n",
    "\tfutures = dict()\n",
    "\tfor key in vector_keys:\n",
    "\t\tif debug:\n",
    "\t\t\tprint(f\"Processing {key}...\")\n",
    "\t\tfutures[key] = executor.submit(\n",
    "\t\t\t_prepareModelArtifact,\n",
    "\t\t\traw_vectors[key],\n",
    "\t\t\tsemantic_data,\n",
    "\t\t\ttruncation_dim,\n",
    "\t\t\tdistance_metric,\n",
    "\t\t\tdebug,\n",
    "\t\t)\n",
    "\texecutor.shutdown(wait=True)\n",
    "\tfor key in vector_keys:\n",
    "\t\tmodel_artifacts[key] = futures[key].result()\n",
    "\treturn model_artifacts\n",
    "\n",
    "\n",
    "print(\">>> Preparing Model Artifacts...\")\n",
    "model_keys = [\"embedding_vector\", \"retrieval_embedding_vector\"]\n",
    "# artifacts = prepareModelArtifacts(\n",
    "# \tqdata, model_keys, truncation_dim=None, distance_metric=\"cosine\", debug=False\n",
    "# )\n",
    "artifacts = prepareModelArtifacts(\n",
    "\tqdata, model_keys, truncation_dim=256, distance_metric=\"mahalanobis\", debug=False\n",
    ")\n",
    "# print(\"\\n>>> Phase 1: Deriving Consensus Priors ...\")\n",
    "max_dist = max(np.max(a[\"dist_matrix\"]) for a in artifacts.values())\n",
    "tau_range = np.arange(0.1, max_dist, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff740b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_vector': np.float64(9.250000000000004), 'retrieval_embedding_vector': np.float64(8.800000000000002)}\n",
      "\n",
      ">>> Phase 2: Running EVT Pipeline...\n",
      "safety: 18.500000000000007\n",
      "safety: 17.600000000000005\n",
      "EM Model Converged.\n",
      "\n",
      ">>> Phase 3: Final Inference\n",
      "\n",
      "================================================================================\n",
      "PROBABILISTIC CLUSTERING RESULTS (Threshold P > 0.5)\n",
      "================================================================================\n",
      "Found 5 significant groups.\n",
      "\n",
      "GROUP 1 (Size: 2) [Cohesion: 0.4358]\n",
      " - Does the privacy policy affirm that the company obtains personal data from third-party sources to train its models?\n",
      " - Does the privacy policy affirm that the company trains its models using data provided by users?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 2 (Size: 2) [Cohesion: 0.4317]\n",
      " - Does the privacy policy affirm that the company processes data to train and improve models based on legitimate interests?\n",
      " - Does the privacy policy affirm that personal data is used to train the company's models?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 3 (Size: 2) [Cohesion: 0.4749]\n",
      " - Does the privacy policy affirm that Personal Data is used to provide the company's services?\n",
      " - Does the privacy policy affirm that Personal Data is used to improve the company's services?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 4 (Size: 2) [Cohesion: 0.3554]\n",
      " - Does the privacy policy affirm that personal data included in Inputs may be reproduced in Outputs?\n",
      " - Does the privacy policy affirm that user Inputs and Outputs may be used to train the company's models?\n",
      "--------------------------------------------------------------------------------\n",
      "GROUP 5 (Size: 2) [Cohesion: 0.4669]\n",
      " - Does the privacy policy affirm that personal data may be shared with service providers for data processing purposes?\n",
      " - Does the privacy policy affirm that personal data may be shared with service providers for the purpose of providing services to the user?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from itertools import product\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import rankdata, expon, beta\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. DATA LOADING\n",
    "# ==============================================================================\n",
    "\n",
    "QUESTIONS_FILE = \"./data/questions_filter_after.json\"\n",
    "\n",
    "\n",
    "def _loadJson(filepath):\n",
    "\tif not os.path.exists(filepath):\n",
    "\t\tprint(f\"Warning: File not found: {filepath}\")\n",
    "\t\treturn {}\n",
    "\ttry:\n",
    "\t\twith open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "\t\t\treturn json.load(f)\n",
    "\texcept json.JSONDecodeError:\n",
    "\t\tprint(f\"Error decoding JSON: {filepath}\")\n",
    "\t\treturn {}\n",
    "\n",
    "\n",
    "print(\">>> Loading Data...\")\n",
    "qdata = _loadJson(QUESTIONS_FILE)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DISTANCE & ARTIFACT PREPARATION\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def getMahalanobisDistances(vectors_a, vectors_b):\n",
    "\tnorms = np.linalg.norm(vectors_a, axis=1, keepdims=True)\n",
    "\tcleaned_vectors = vectors_a / (norms + 1e-10)\n",
    "\n",
    "\tlw = LedoitWolf()\n",
    "\tlw.fit(cleaned_vectors)\n",
    "\tprecision_matrix = lw.precision_\n",
    "\n",
    "\tdist_matrix = cdist(\n",
    "\t\tcleaned_vectors, cleaned_vectors, metric=\"mahalanobis\", VI=precision_matrix\n",
    "\t)\n",
    "\treturn dist_matrix, precision_matrix\n",
    "\n",
    "\n",
    "Distance_Processors = {\n",
    "\t\"cosine\": lambda emb_a, emb_b: 1.0\n",
    "\t- (emb_a @ emb_b.T)\n",
    "\t/ (\n",
    "\t\tnp.linalg.norm(emb_a, axis=1, keepdims=True)\n",
    "\t\t@ np.linalg.norm(emb_b, axis=1, keepdims=True).T\n",
    "\t\t+ 1e-10\n",
    "\t),\n",
    "\t\"l1\": lambda emb_a, emb_b: np.sum(np.abs(emb_a[..., np.newaxis] - emb_b.T), axis=1),\n",
    "\t\"l2\": lambda emb_a, emb_b: np.linalg.norm(emb_a[..., np.newaxis] - emb_b.T, axis=1),\n",
    "\t\"dot\": lambda emb_a, emb_b: emb_a @ emb_b.T,\n",
    "\t\"mahalanobis\": lambda emb_a, emb_b: getMahalanobisDistances(emb_a, emb_b),\n",
    "}\n",
    "\n",
    "\n",
    "def _prepareModelArtifact(\n",
    "\traw_vectors,\n",
    "\tsemantic_data,\n",
    "\ttruncation_dim=256,\n",
    "\tdistance_metric=\"mahalanobis\",\n",
    "\tdebug=True,\n",
    "):\n",
    "\tdata_matrix = np.array(raw_vectors)\n",
    "\tinput_dim = data_matrix.shape[1]\n",
    "\n",
    "\tif truncation_dim and input_dim < truncation_dim and debug:\n",
    "\t\tprint(\n",
    "\t\t\tf\"Warning: Vector dimension ({input_dim}) < limit ({truncation_dim}). No truncation.\"\n",
    "\t\t)\n",
    "\tdata_truncated = data_matrix\n",
    "\tif truncation_dim:\n",
    "\t\tdata_truncated = data_matrix[:, :truncation_dim]\n",
    "\n",
    "\tdist_output = Distance_Processors[distance_metric](data_truncated, data_truncated)\n",
    "\n",
    "\tprecision_matrix = None\n",
    "\tif distance_metric == \"mahalanobis\":\n",
    "\t\tdist_matrix, precision_matrix = dist_output\n",
    "\telse:\n",
    "\t\tdist_matrix = dist_output\n",
    "\n",
    "\t# Calculate NN indices (excluding self) for cache usage\n",
    "\td_temp = dist_matrix.copy()\n",
    "\tnp.fill_diagonal(d_temp, float(\"inf\"))\n",
    "\tnn_indices = np.argmin(d_temp, axis=1)\n",
    "\n",
    "\treturn {\n",
    "\t\t\"dist_matrix\": dist_matrix,\n",
    "\t\t\"vectors\": data_truncated,\n",
    "\t\t\"precision\": precision_matrix,\n",
    "\t\t\"semantic_data\": semantic_data,\n",
    "\t\t\"metric\": distance_metric,\n",
    "\t\t\"nn_indices\": nn_indices,\n",
    "\t}\n",
    "\n",
    "\n",
    "def prepareModelArtifacts(\n",
    "\tdata_set, vector_keys, truncation_dim=256, distance_metric=\"mahalanobis\", debug=True\n",
    "):\n",
    "\tsemantic_data = list(data_set.keys())\n",
    "\tmodel_artifacts = {}\n",
    "\traw_vectors = {}\n",
    "\tfor key in vector_keys:\n",
    "\t\traw_vectors[key] = [data_set[s][key] for s in semantic_data]\n",
    "\n",
    "\texecutor = concurrent.futures.ThreadPoolExecutor(max_workers=len(vector_keys))\n",
    "\tfutures = dict()\n",
    "\tfor key in vector_keys:\n",
    "\t\tif debug:\n",
    "\t\t\tprint(f\"Processing {key}...\")\n",
    "\t\tfutures[key] = executor.submit(\n",
    "\t\t\t_prepareModelArtifact,\n",
    "\t\t\traw_vectors[key],\n",
    "\t\t\tsemantic_data,\n",
    "\t\t\ttruncation_dim,\n",
    "\t\t\tdistance_metric,\n",
    "\t\t\tdebug,\n",
    "\t\t)\n",
    "\texecutor.shutdown(wait=True)\n",
    "\tfor key in vector_keys:\n",
    "\t\tmodel_artifacts[key] = futures[key].result()\n",
    "\treturn model_artifacts\n",
    "\n",
    "\n",
    "print(\">>> Preparing Model Artifacts...\")\n",
    "model_keys = [\"embedding_vector\", \"retrieval_embedding_vector\"]\n",
    "# artifacts = prepareModelArtifacts(\n",
    "# \tqdata, model_keys, truncation_dim=None, distance_metric=\"cosine\", debug=False\n",
    "# )\n",
    "artifacts = prepareModelArtifacts(\n",
    "\tqdata, model_keys, truncation_dim=256, distance_metric=\"mahalanobis\", debug=False\n",
    ")\n",
    "# print(\"\\n>>> Phase 1: Deriving Consensus Priors ...\")\n",
    "max_dist = max(np.max(a[\"dist_matrix\"]) for a in artifacts.values())\n",
    "tau_range = np.arange(0.1, max_dist, 0.05)\n",
    "# ==============================================================================\n",
    "# 3. ACGC CONSENSUS SEARCH (COLD START)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def clusterAndGetArtifacts(dist_matrix, threshold):\n",
    "\tmodel = AgglomerativeClustering(\n",
    "\t\tn_clusters=None,\n",
    "\t\tdistance_threshold=threshold,\n",
    "\t\tmetric=\"precomputed\",\n",
    "\t\tlinkage=\"complete\",\n",
    "\t)\n",
    "\tlabels = model.fit_predict(dist_matrix)\n",
    "\tgroups = {}\n",
    "\tfor idx, label in enumerate(labels):\n",
    "\t\tgroups.setdefault(label, []).append(idx)\n",
    "\treturn [g for g in groups.values() if len(g) > 1], labels\n",
    "\n",
    "\n",
    "def getNNPairsFromGroups(groups, nn_indices):\n",
    "\tpairs = set()\n",
    "\tfor group in groups:\n",
    "\t\tif len(group) < 2:\n",
    "\t\t\tcontinue\n",
    "\t\tgroup_set = set(group)\n",
    "\t\tfor idx in group:\n",
    "\t\t\tnn_idx = nn_indices[idx]\n",
    "\t\t\tif nn_idx in group_set:\n",
    "\t\t\t\tpairs.add(tuple(sorted((idx, nn_idx))))\n",
    "\treturn pairs\n",
    "\n",
    "\n",
    "def calculateNTrue(labels_array, target_pairs):\n",
    "\tif not target_pairs or labels_array is None:\n",
    "\t\treturn 0\n",
    "\tinvolved = {idx for pair in target_pairs for idx in pair}\n",
    "\tif not involved:\n",
    "\t\treturn 0\n",
    "\treturn len({labels_array[idx] for idx in involved})\n",
    "\n",
    "\n",
    "def createClusteringCache(model_artifacts, tau_range):\n",
    "\tcache = {name: {} for name in model_artifacts.keys()}\n",
    "\t# print(f\"Building ACGC Cache ({len(tau_range)} steps)...\")\n",
    "\tfor name, artifact in model_artifacts.items():\n",
    "\t\tprev_groups = []\n",
    "\t\tfor t in tau_range:\n",
    "\t\t\tgroups, labels = clusterAndGetArtifacts(artifact[\"dist_matrix\"], t)\n",
    "\t\t\t# Optimization: Only store if groups changed\n",
    "\t\t\tif len(groups) != len(prev_groups) or groups != prev_groups:\n",
    "\t\t\t\tnn_pairs = getNNPairsFromGroups(groups, artifact[\"nn_indices\"])\n",
    "\t\t\t\tcache[name][t] = [groups, labels, nn_pairs]\n",
    "\t\t\t\tprev_groups = groups\n",
    "\treturn cache\n",
    "\n",
    "\n",
    "# def findConsensusViaACGC(clustering_cache, model_keys):\n",
    "# \tthreshold_axes = [list(clustering_cache[m].keys()) for m in model_keys]\n",
    "# \tbest_score = -1\n",
    "# \tbest_p_true = set()\n",
    "\n",
    "# \tfor thresholds in product(*threshold_axes):\n",
    "# \t\tcurrent_config = dict(zip(model_keys, thresholds))\n",
    "# \t\tpair_sets = [clustering_cache[m][t][2] for m, t in current_config.items()]\n",
    "\n",
    "# \t\tp_true = set.intersection(*pair_sets)\n",
    "# \t\tif not p_true:\n",
    "# \t\t\tcontinue\n",
    "\n",
    "# \t\tn_true_sum = 0\n",
    "# \t\tfor m, t in current_config.items():\n",
    "# \t\t\tlabels = clustering_cache[m][t][1]\n",
    "# \t\t\tn_true_sum += calculateNTrue(labels, p_true)\n",
    "\n",
    "# \t\tcurrent_score = n_true_sum / len(model_keys)\n",
    "# \t\tif current_score > best_score:\n",
    "# \t\t\tbest_score = current_score\n",
    "# \t\t\tbest_p_true = p_true\n",
    "\n",
    "\n",
    "# \treturn best_p_true\n",
    "def findConsensusViaACGC(clustering_cache, model_keys):\n",
    "\t\"\"\"\n",
    "\tFinds the configuration maximizing the Average Consensus-Supporting Group Count.\n",
    "\tReturns full state including optimal thresholds per model.\n",
    "\t\"\"\"\n",
    "\tthreshold_axes = [list(clustering_cache[m].keys()) for m in model_keys]\n",
    "\n",
    "\tbest_score = -1\n",
    "\tbest_state = {\"score\": -1, \"P_true\": set(), \"optimal_taus\": {}, \"N_target\": 0}\n",
    "\n",
    "\t# Grid Search\n",
    "\tfor thresholds in product(*threshold_axes):\n",
    "\t\tcurrent_config = dict(zip(model_keys, thresholds))\n",
    "\n",
    "\t\t# Get pair sets\n",
    "\t\tpair_sets = [clustering_cache[m][t][2] for m, t in current_config.items()]\n",
    "\n",
    "\t\t# Intersection\n",
    "\t\tp_true = set.intersection(*pair_sets)\n",
    "\t\tif not p_true:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t# Calculate ACGC Score\n",
    "\t\tn_true_sum = 0\n",
    "\t\tfor m, t in current_config.items():\n",
    "\t\t\tlabels = clustering_cache[m][t][1]\n",
    "\t\t\tn_true_sum += calculateNTrue(labels, p_true)\n",
    "\n",
    "\t\tcurrent_score = n_true_sum / len(model_keys)\n",
    "\n",
    "\t\t# Optimization\n",
    "\t\tif current_score > best_score:\n",
    "\t\t\tbest_score = current_score\n",
    "\t\t\tbest_state[\"score\"] = best_score\n",
    "\t\t\tbest_state[\"P_true\"] = p_true\n",
    "\t\t\tbest_state[\"optimal_taus\"] = current_config\n",
    "\t\t\tbest_state[\"N_target\"] = best_score\n",
    "\n",
    "\treturn best_state\n",
    "\n",
    "\n",
    "# def findConsensusViaJaccard(clustering_cache, model_keys):\n",
    "# \t\"\"\"\n",
    "# \tFinds the 'Platinum' set: Pairs that appear in ALL models at the same\n",
    "# \tthreshold configuration that maximizes Intersection-over-Union.\n",
    "# \t\"\"\"\n",
    "# \tthreshold_axes = [list(clustering_cache[m].keys()) for m in model_keys]\n",
    "# \tbest_score = -1\n",
    "# \tbest_p_true = set()\n",
    "\n",
    "# \t# Grid Search\n",
    "# \tfor thresholds in product(*threshold_axes):\n",
    "# \t\tcurrent_config = dict(zip(model_keys, thresholds))\n",
    "# \t\tpair_sets = [clustering_cache[m][t][2] for m, t in current_config.items()]\n",
    "\n",
    "# \t\t# INTERSECTION (The Core)\n",
    "# \t\tp_true = set.intersection(*pair_sets)\n",
    "# \t\tif not p_true:\n",
    "# \t\t\tcontinue\n",
    "\n",
    "# \t\t# UNION (The Broad Net)\n",
    "# \t\tp_union = set.union(*pair_sets)\n",
    "# \t\tif not p_union:\n",
    "# \t\t\tcontinue\n",
    "\n",
    "# \t\t# Jaccard Score\n",
    "# \t\tcurrent_score = len(p_true) / len(p_union)\n",
    "\n",
    "# \t\t# We prefer high Jaccard, but also want non-trivial size\n",
    "# \t\t# (Tie-breaker: larger set size)\n",
    "# \t\tif current_score > best_score:\n",
    "# \t\t\tbest_score = current_score\n",
    "# \t\t\tbest_p_true = p_true\n",
    "# \t\telif current_score == best_score:\n",
    "# \t\t\tif len(p_true) > len(best_p_true):\n",
    "# \t\t\t\tbest_p_true = p_true\n",
    "\n",
    "# \treturn best_p_true\n",
    "\n",
    "\n",
    "def findConsensusViaJaccard(clustering_cache, model_keys):\n",
    "\t\"\"\"\n",
    "\tFinds the 'Platinum' configuration maximizing Jaccard Index.\n",
    "\tReturns the full state including the optimal thresholds per model.\n",
    "\t\"\"\"\n",
    "\tthreshold_axes = [list(clustering_cache[m].keys()) for m in model_keys]\n",
    "\n",
    "\tbest_score = -1\n",
    "\tbest_state = {\"score\": -1, \"P_true\": set(), \"optimal_taus\": {}}\t# We need this!\n",
    "\n",
    "\t# Grid Search\n",
    "\tfor thresholds in product(*threshold_axes):\n",
    "\t\tcurrent_config = dict(zip(model_keys, thresholds))\n",
    "\t\tpair_sets = [clustering_cache[m][t][2] for m, t in current_config.items()]\n",
    "\n",
    "\t\tp_true = set.intersection(*pair_sets)\n",
    "\t\tif not p_true:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tp_union = set.union(*pair_sets)\n",
    "\t\tif not p_union:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\tcurrent_score = len(p_true) / len(p_union)\n",
    "\n",
    "\t\t# Optimization Logic\n",
    "\t\tupdate = False\n",
    "\t\tif current_score > best_score:\n",
    "\t\t\tupdate = True\n",
    "\t\telif current_score == best_score:\n",
    "\t\t\t# Tie-breaker: Prefer larger consensus sets\n",
    "\t\t\tif len(p_true) > len(best_state[\"P_true\"]):\n",
    "\t\t\t\tupdate = True\n",
    "\n",
    "\t\tif update:\n",
    "\t\t\tbest_score = current_score\n",
    "\t\t\tbest_state[\"score\"] = best_score\n",
    "\t\t\tbest_state[\"P_true\"] = p_true\n",
    "\t\t\tbest_state[\"optimal_taus\"] = current_config\n",
    "\n",
    "\treturn best_state\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. PROBABILISTIC MODELING (EVT & MIXTURE MODELS)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# def computeLocalDensityRanks(dist_matrix, k_neighbors=50, **kwargs):\n",
    "# \t# method='min' ensures ties get the lower rank\n",
    "# \tranks = rankdata(dist_matrix, axis=1, method=\"min\")\n",
    "# \t# Subtract 1 so the diagonal (self) is 0.0\n",
    "# \tranks = ranks - 1.0\n",
    "# \tnormalized_ranks = ranks / k_neighbors\n",
    "# \treturn np.clip(normalized_ranks, 0.0, 1.0)\n",
    "def computeLocalDensityRanks(dist_matrix, k_neighbors=50, max_valid_dist=None):\n",
    "\t\"\"\"\n",
    "\tComputes Local Rank Density (Rank / k) with a physical distance safety gate.\n",
    "\t\"\"\"\n",
    "\n",
    "\tranks = rankdata(dist_matrix, axis=1, method=\"min\") - 1.0\n",
    "\n",
    "\tnormalized_ranks = ranks / k_neighbors\n",
    "\n",
    "\tif max_valid_dist is not None:\n",
    "\t\tnoise_mask = dist_matrix > max_valid_dist\n",
    "\t\tnormalized_ranks[noise_mask] = 1.0\n",
    "\n",
    "\t# 4. Clip to [0, 1] for Beta distribution compatibility\n",
    "\treturn np.clip(normalized_ranks, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def getCharacteristicLength(dist_matrix, nn_indices):\n",
    "\t\"\"\"\n",
    "\tReturns the scalar distance to the nearest neighbor (r).\n",
    "\t\"\"\"\n",
    "\tN = dist_matrix.shape[0]\n",
    "\tsigmas = dist_matrix[np.arange(N), nn_indices]\n",
    "\tsigmas[sigmas == 0] = 1e-9\n",
    "\treturn sigmas.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# def computeLocalDensityRanks(dist_matrix, k_neighbors=None, max_valid_dist=None):\n",
    "# \t\"\"\"\n",
    "# \tComputes Empirical CDF (Rank/N) with strictly enforced horizons.\n",
    "# \tDenominator is N (Corpus Size), making probabilities scale-invariant to k.\n",
    "# \t\"\"\"\n",
    "# \tn_samples = dist_matrix.shape[0]\n",
    "\n",
    "# \t# 1. Compute Rank (0 to N-1)\n",
    "# \tranks = rankdata(dist_matrix, axis=1, method=\"min\") - 1.0\n",
    "\n",
    "# \t# 2. Normalize by TOTAL CORPUS SIZE (N)\n",
    "# \t#    This keeps the scale constant. Rank 10 is always 10/N.\n",
    "# \tnormalized_ranks = ranks / (n_samples - 1 + 1e-10)\n",
    "\n",
    "# \t# 3. Apply K-Horizon (Masking)\n",
    "# \t#    If a point is outside the top K, force it to Background (1.0).\n",
    "# \t#    It does not stretch the scale; it just cuts off the tail.\n",
    "# \tif k_neighbors is not None:\n",
    "# \t\tk_mask = ranks > k_neighbors\n",
    "# \t\tnormalized_ranks[k_mask] = 1.0\n",
    "\n",
    "# \t# 4. Apply Distance-Horizon (Masking)\n",
    "# \tif max_valid_dist is not None:\n",
    "# \t\tdist_mask = dist_matrix > max_valid_dist\n",
    "# \t\tnormalized_ranks[dist_mask] = 1.0\n",
    "\n",
    "# \treturn np.clip(normalized_ranks, 0.0, 1.0)\n",
    "\n",
    "\n",
    "# def computeLocalDensityRanks(dist_matrix, k_neighbors=None, max_valid_dist=None):\n",
    "# \t\"\"\"\n",
    "# \tComputes Empirical CDF (Rank/N) with a Safety Distance Gate.\n",
    "# \t\"\"\"\n",
    "# \tn_samples = dist_matrix.shape[0]\n",
    "\n",
    "# \t# 1. Compute Rank over the FULL corpus (N), not just k.\n",
    "# \t#    This ensures the 'Background' is Uniformly distributed [0,1],\n",
    "# \t#    which allows the Beta distribution to fit it correctly.\n",
    "# \tranks = rankdata(dist_matrix, axis=1, method=\"min\") - 1.0\n",
    "# \tnormalized_ranks = ranks / (n_samples - 1 + 1e-10)\n",
    "\n",
    "# \t# 2. Distance Gating (The Safety Horizon)\n",
    "# \t#    If a point is physically too far (e.g. Mahalanobis > 20),\n",
    "# \t#    we force it to be \"Background\" (Rank 1.0), even if it was the closest neighbor.\n",
    "# \tif max_valid_dist is not None:\n",
    "# \t\tnoise_mask = dist_matrix > max_valid_dist\n",
    "# \t\tnormalized_ranks[noise_mask] = 1.0\n",
    "\n",
    "# \treturn np.clip(normalized_ranks, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def computeFusedEVTStatistic(rank_matrices_dict):\n",
    "\tmatrices = list(rank_matrices_dict.values())\n",
    "\tstacked_matrices = np.stack(matrices, axis=0)\n",
    "\treturn np.min(stacked_matrices, axis=0)\n",
    "\n",
    "\n",
    "def _get_off_diagonal_samples(matrix):\n",
    "\tmask = ~np.eye(matrix.shape[0], dtype=bool)\n",
    "\treturn matrix[mask]\n",
    "\n",
    "\n",
    "def _initializeParametersViaNaiveQuantile(fused_statistic_matrix, signal_quantile=0.01):\n",
    "\tdata = _get_off_diagonal_samples(fused_statistic_matrix)\n",
    "\tthreshold = np.quantile(data, signal_quantile)\n",
    "\tsignal_data = data[data <= threshold]\n",
    "\tnoise_data = data[data > threshold]\n",
    "\n",
    "\tlamb = 1.0 / (np.mean(signal_data) + 1e-6)\n",
    "\tmu, var = np.mean(noise_data), np.var(noise_data)\n",
    "\n",
    "\tif var >= mu * (1 - mu):\n",
    "\t\talpha, b_param = 1.0, 1.0\n",
    "\telse:\n",
    "\t\tcommon = (mu * (1 - mu) / (var + 1e-9)) - 1\n",
    "\t\talpha = max(mu * common, 1.0)\n",
    "\t\tb_param = max((1 - mu) * common, 1.0)\n",
    "\n",
    "\treturn {\"pi\": signal_quantile, \"lambda\": lamb, \"alpha\": alpha, \"beta\": b_param}\n",
    "\n",
    "\n",
    "def _initializeParametersViaConsensus(fused_statistic_matrix, consensus_pairs):\n",
    "\tif not consensus_pairs:\n",
    "\t\tprint(\"Warning: ACGC found no pairs. Falling back to naive quantile.\")\n",
    "\t\treturn _initializeParametersViaNaiveQuantile(fused_statistic_matrix)\n",
    "\n",
    "\tsignal_values = [fused_statistic_matrix[i, j] for i, j in consensus_pairs]\n",
    "\tflat_all = _get_off_diagonal_samples(fused_statistic_matrix)\n",
    "\n",
    "\tnoise_mean = np.mean(flat_all)\n",
    "\tnoise_var = np.var(flat_all)\n",
    "\n",
    "\tpi = len(consensus_pairs) / len(flat_all)\n",
    "\tlamb = 1.0 / (np.mean(signal_values) + 1e-6)\n",
    "\n",
    "\tcommon = (noise_mean * (1 - noise_mean) / (noise_var + 1e-9)) - 1\n",
    "\talpha = max(noise_mean * common, 1.0)\n",
    "\tbeta_param = max((1 - noise_mean) * common, 1.0)\n",
    "\n",
    "\treturn {\"pi\": pi, \"lambda\": lamb, \"alpha\": alpha, \"beta\": beta_param}\n",
    "\n",
    "\n",
    "# def fitWeibullBetaMixture(\n",
    "# \tfused_statistic_matrix, initial_params=None, max_iter=100, tol=1e-4\n",
    "# ):\n",
    "# \tdata = _get_off_diagonal_samples(fused_statistic_matrix)\n",
    "# \tdata = np.clip(data, 1e-6, 1.0 - 1e-6)\n",
    "\n",
    "# \tif initial_params is None:\n",
    "# \t\tparams = _initializeParametersViaNaiveQuantile(fused_statistic_matrix)\n",
    "# \telse:\n",
    "# \t\tparams = initial_params\n",
    "\n",
    "# \tpi, lamb, a, b_p = params[\"pi\"], params[\"lambda\"], params[\"alpha\"], params[\"beta\"]\n",
    "# \tlog_likelihood_old = -np.inf\n",
    "\n",
    "# \tfor _ in range(max_iter):\n",
    "# \t\t# E-Step\n",
    "# \t\tpdf_signal = expon.pdf(data, scale=1.0 / lamb)\n",
    "# \t\tpdf_noise = beta.pdf(data, a, b_p)\n",
    "\n",
    "# \t\tweighted_signal = pi * pdf_signal\n",
    "# \t\tweighted_noise = (1 - pi) * pdf_noise\n",
    "# \t\ttotal_evidence = weighted_signal + weighted_noise + 1e-10\n",
    "# \t\tgamma = weighted_signal / total_evidence\n",
    "\n",
    "# \t\t# M-Step\n",
    "# \t\tN_s = np.sum(gamma)\n",
    "# \t\tpi = N_s / len(data)\n",
    "\n",
    "# \t\tweighted_sum_x = np.sum(gamma * data)\n",
    "# \t\tlamb = 1.0 / (weighted_sum_x / (N_s + 1e-10))\n",
    "\n",
    "# \t\tw_noise = 1 - gamma\n",
    "# \t\tw_noise_sum = np.sum(w_noise) + 1e-10\n",
    "# \t\tmu_n = np.sum(w_noise * data) / w_noise_sum\n",
    "# \t\tvar_n = np.sum(w_noise * (data - mu_n) ** 2) / w_noise_sum\n",
    "\n",
    "# \t\tif var_n < mu_n * (1 - mu_n):\n",
    "# \t\t\tcommon = (mu_n * (1 - mu_n) / (var_n + 1e-10)) - 1\n",
    "# \t\t\ta = max(mu_n * common, 1.0)\n",
    "# \t\t\tb_p = max((1 - mu_n) * common, 1.0)\n",
    "\n",
    "# \t\tlog_likelihood_new = np.sum(np.log(total_evidence))\n",
    "# \t\tif abs(log_likelihood_new - log_likelihood_old) < tol:\n",
    "# \t\t\tbreak\n",
    "# \t\tlog_likelihood_old = log_likelihood_new\n",
    "\n",
    "# \tfinal_params = {\"pi\": pi, \"lambda\": lamb, \"alpha\": a, \"beta\": b_p}\n",
    "\n",
    "# \t# Compute Full Posterior\n",
    "# \tpdf_S_full = expon.pdf(fused_statistic_matrix, scale=1.0 / lamb)\n",
    "# \tpdf_N_full = beta.pdf(np.clip(fused_statistic_matrix, 1e-6, 1 - 1e-6), a, b_p)\n",
    "# \tnumerator = pi * pdf_S_full\n",
    "# \tposterior = numerator / (numerator + (1 - pi) * pdf_N_full + 1e-10)\n",
    "\n",
    "# \treturn posterior, final_params\n",
    "\n",
    "\n",
    "# def fitWeibullBetaMixture(\n",
    "# \tfused_statistic_matrix, initial_params=None, max_iter=20, tol=1e-4\n",
    "# ):\n",
    "# \tdata = _get_off_diagonal_samples(fused_statistic_matrix)\n",
    "# \tdata = np.clip(data, 1e-6, 1.0 - 1e-6)\n",
    "\n",
    "# \tif initial_params is None:\n",
    "# \t\tparams = _initializeParametersViaNaiveQuantile(fused_statistic_matrix)\n",
    "# \telse:\n",
    "# \t\tparams = initial_params\n",
    "\n",
    "# \tpi, lamb, a, b_p = params[\"pi\"], params[\"lambda\"], params[\"alpha\"], params[\"beta\"]\n",
    "# \tlog_likelihood_old = -np.inf\n",
    "\n",
    "# \t# --- EM LOOP (Learning Shapes with True Priors) ---\n",
    "# \tfor _ in range(max_iter):\n",
    "# \t\t# E-Step\n",
    "# \t\tpdf_signal = expon.pdf(data, scale=1.0 / lamb)\n",
    "# \t\tpdf_noise = beta.pdf(data, a, b_p)\n",
    "\n",
    "# \t\tweighted_signal = pi * pdf_signal\n",
    "# \t\tweighted_noise = (1 - pi) * pdf_noise\n",
    "# \t\ttotal_evidence = weighted_signal + weighted_noise + 1e-10\n",
    "# \t\tgamma = weighted_signal / total_evidence\n",
    "\n",
    "# \t\t# M-Step\n",
    "# \t\tN_s = np.sum(gamma)\n",
    "# \t\tpi = N_s / len(data)\n",
    "\n",
    "# \t\tweighted_sum_x = np.sum(gamma * data)\n",
    "# \t\tlamb = 1.0 / (weighted_sum_x / (N_s + 1e-10))\n",
    "\n",
    "# \t\tw_noise = 1 - gamma\n",
    "# \t\tw_noise_sum = np.sum(w_noise) + 1e-10\n",
    "# \t\tmu_n = np.sum(w_noise * data) / w_noise_sum\n",
    "# \t\tvar_n = np.sum(w_noise * (data - mu_n) ** 2) / w_noise_sum\n",
    "\n",
    "# \t\tif var_n < mu_n * (1 - mu_n):\n",
    "# \t\t\tcommon = (mu_n * (1 - mu_n) / (var_n + 1e-10)) - 1\n",
    "# \t\t\ta = max(mu_n * common, 1.0)\n",
    "# \t\t\tb_p = max((1 - mu_n) * common, 1.0)\n",
    "\n",
    "# \t\tlog_likelihood_new = np.sum(np.log(total_evidence))\n",
    "# \t\tif abs(log_likelihood_new - log_likelihood_old) < tol:\n",
    "# \t\t\tbreak\n",
    "# \t\tlog_likelihood_old = log_likelihood_new\n",
    "\n",
    "# \tfinal_params = {\"pi\": pi, \"lambda\": lamb, \"alpha\": a, \"beta\": b_p}\n",
    "\n",
    "# \t# --- FINAL INFERENCE (Balanced Prior) ---\n",
    "# \t# We calculate the probability assuming P(Signal) = 0.5 vs P(Noise) = 0.5\n",
    "# \t# This turns the output into a Likelihood Ratio test normalized to [0,1].\n",
    "\n",
    "# \tpdf_S_full = expon.pdf(fused_statistic_matrix, scale=1.0 / lamb)\n",
    "# \tpdf_N_full = beta.pdf(np.clip(fused_statistic_matrix, 1e-6, 1 - 1e-6), a, b_p)\n",
    "\n",
    "# \t# Use 0.5 instead of 'pi' here to ignore class imbalance for detection\n",
    "# \tnumerator = 0.5 * pdf_S_full\n",
    "# \tposterior = numerator / (numerator + 0.5 * pdf_N_full + 1e-10)\n",
    "\n",
    "# \treturn posterior, final_params\n",
    "\n",
    "\n",
    "# def fitWeibullBetaMixture(\n",
    "# \tfused_statistic_matrix, initial_params=None, max_iter=100, tol=1e-4\n",
    "# ):\n",
    "# \t\"\"\"\n",
    "# \tFits EVT Mixture Model using Balanced-EM.\n",
    "# \tCrucial Fix: Forces 50/50 prior during training to prevent\n",
    "# \tclass imbalance from collapsing the signal distribution.\n",
    "# \t\"\"\"\n",
    "# \tdata = _get_off_diagonal_samples(fused_statistic_matrix)\n",
    "# \tdata = np.clip(data, 1e-6, 1.0 - 1e-6)\n",
    "\n",
    "# \tif initial_params is None:\n",
    "# \t\tparams = _initializeParametersViaNaiveQuantile(fused_statistic_matrix)\n",
    "# \telse:\n",
    "# \t\tparams = initial_params\n",
    "\n",
    "# \t# We ignore the 'pi' from init because it is too small (e.g. 0.00002).\n",
    "# \t# We will trust the Lambda/Alpha/Beta shapes, but ignore the frequency.\n",
    "# \tlamb = params[\"lambda\"]\n",
    "# \ta = params[\"alpha\"]\n",
    "# \tb_p = params[\"beta\"]\n",
    "\n",
    "# \tlog_likelihood_old = -np.inf\n",
    "\n",
    "# \t# --- BALANCED EM LOOP ---\n",
    "# \tfor _ in range(max_iter):\n",
    "# \t\t# E-Step: Calculate Responsibilities assuming Balanced Priors (0.5 / 0.5)\n",
    "# \t\t# This asks: \"Based on SHAPE alone, which distribution fits best?\"\n",
    "# \t\tpdf_signal = expon.pdf(data, scale=1.0 / lamb)\n",
    "# \t\tpdf_noise = beta.pdf(data, a, b_p)\n",
    "\n",
    "# \t\t# Force 50/50 weights\n",
    "# \t\tweighted_signal = 0.5 * pdf_signal\n",
    "# \t\tweighted_noise = 0.5 * pdf_noise\n",
    "\n",
    "# \t\ttotal_evidence = weighted_signal + weighted_noise + 1e-10\n",
    "# \t\tgamma = weighted_signal / total_evidence\t# P(Signal | Data, Balanced)\n",
    "\n",
    "# \t\t# M-Step: Update Shape Parameters based on these responsibilities\n",
    "# \t\tN_s = np.sum(gamma)\n",
    "\n",
    "# \t\t# Update Lambda (Weighted MLE)\n",
    "# \t\tweighted_sum_x = np.sum(gamma * data)\n",
    "# \t\t# Protect against N_s vanishing\n",
    "# \t\tlamb = 1.0 / (weighted_sum_x / (N_s + 1e-10))\n",
    "\n",
    "# \t\t# Update Noise Params\n",
    "# \t\tw_noise = 1 - gamma\n",
    "# \t\tw_noise_sum = np.sum(w_noise) + 1e-10\n",
    "# \t\tmu_n = np.sum(w_noise * data) / w_noise_sum\n",
    "# \t\tvar_n = np.sum(w_noise * (data - mu_n) ** 2) / w_noise_sum\n",
    "\n",
    "# \t\tif var_n < mu_n * (1 - mu_n):\n",
    "# \t\t\tcommon = (mu_n * (1 - mu_n) / (var_n + 1e-10)) - 1\n",
    "# \t\t\ta = max(mu_n * common, 1.0)\n",
    "# \t\t\tb_p = max((1 - mu_n) * common, 1.0)\n",
    "\n",
    "# \t\t# Check convergence on the balanced likelihood\n",
    "# \t\tlog_likelihood_new = np.sum(np.log(total_evidence))\n",
    "# \t\tif abs(log_likelihood_new - log_likelihood_old) < tol:\n",
    "# \t\t\tbreak\n",
    "# \t\tlog_likelihood_old = log_likelihood_new\n",
    "\n",
    "# \t# We do NOT return a learned 'pi'. We return the shapes.\n",
    "# \tfinal_params = {\"lambda\": lamb, \"alpha\": a, \"beta\": b_p}\n",
    "\n",
    "# \t# --- FINAL INFERENCE ---\n",
    "# \t# Again, use Balanced Inference. P > 0.5 means \"More likely Signal than Noise\".\n",
    "# \tpdf_S_full = expon.pdf(fused_statistic_matrix, scale=1.0 / lamb)\n",
    "# \tpdf_N_full = beta.pdf(np.clip(fused_statistic_matrix, 1e-6, 1 - 1e-6), a, b_p)\n",
    "\n",
    "# \tnumerator = 0.5 * pdf_S_full\n",
    "# \tposterior = numerator / (numerator + 0.5 * pdf_N_full + 1e-10)\n",
    "\n",
    "# \treturn posterior, final_params\n",
    "\n",
    "\n",
    "def fitWeibullBetaMixture(\n",
    "\tfused_statistic_matrix, initial_params=None, max_iter=20, tol=1e-6\n",
    "):\n",
    "\t\"\"\"\n",
    "\tFits EVT Mixture Model using Balanced-EM.\n",
    "\tCrucial Fix: Forces 50/50 prior during training steps.\n",
    "\t\"\"\"\n",
    "\tdata = _get_off_diagonal_samples(fused_statistic_matrix)\n",
    "\tdata = np.clip(data, 1e-8, 1.0 - 1e-8)\n",
    "\n",
    "\tif initial_params is None:\n",
    "\t\tparams = _initializeParametersViaNaiveQuantile(fused_statistic_matrix)\n",
    "\telse:\n",
    "\t\tparams = initial_params\n",
    "\n",
    "\tlamb = params[\"lambda\"]\n",
    "\ta = params[\"alpha\"]\n",
    "\tb_p = params[\"beta\"]\n",
    "\n",
    "\tlog_likelihood_old = -np.inf\n",
    "\n",
    "\t# --- BALANCED EM LOOP ---\n",
    "\tfor _ in range(max_iter):\n",
    "\t\t# E-Step: Force 50/50 weights\n",
    "\t\tpdf_signal = expon.pdf(data, scale=1.0 / lamb)\n",
    "\t\tpdf_noise = beta.pdf(data, a, b_p)\n",
    "\t\tpdf_signal = np.nan_to_num(pdf_signal, nan=0.0, posinf=0.0)\n",
    "\t\tpdf_noise = np.nan_to_num(pdf_noise, nan=0.0, posinf=0.0)\n",
    "\t\t# Use 0.5 prior to learn SHAPE, ignoring FREQUENCY\n",
    "\t\tweighted_signal = 0.5 * pdf_signal\n",
    "\t\tweighted_noise = 0.5 * pdf_noise\n",
    "\n",
    "\t\ttotal_evidence = weighted_signal + weighted_noise + 1e-10\n",
    "\t\tgamma = weighted_signal / total_evidence\n",
    "\n",
    "\t\t# M-Step\n",
    "\t\tN_s = np.sum(gamma)\n",
    "\t\tweighted_sum_x = np.sum(gamma * data)\n",
    "\t\tlamb = 1.0 / (weighted_sum_x / (N_s + 1e-10))\n",
    "\n",
    "\t\tw_noise = 1 - gamma\n",
    "\t\tw_noise_sum = np.sum(w_noise) + 1e-10\n",
    "\t\tmu_n = np.sum(w_noise * data) / w_noise_sum\n",
    "\t\tvar_n = np.sum(w_noise * (data - mu_n) ** 2) / w_noise_sum\n",
    "\n",
    "\t\tif var_n < mu_n * (1 - mu_n):\n",
    "\t\t\tcommon = (mu_n * (1 - mu_n) / (var_n + 1e-10)) - 1\n",
    "\t\t\ta = max(mu_n * common, 1.0)\n",
    "\t\t\tb_p = max((1 - mu_n) * common, 1.0)\n",
    "\n",
    "\t\tif abs(np.sum(np.log(total_evidence)) - log_likelihood_old) < tol:\n",
    "\t\t\tbreak\n",
    "\t\tlog_likelihood_old = np.sum(np.log(total_evidence))\n",
    "\n",
    "\tfinal_params = {\"lambda\": lamb, \"alpha\": a, \"beta\": b_p}\n",
    "\n",
    "\t# --- FINAL INFERENCE (Balanced) ---\n",
    "\tpdf_S_full = expon.pdf(fused_statistic_matrix, scale=1.0 / lamb)\n",
    "\tpdf_N_full = beta.pdf(np.clip(fused_statistic_matrix, 1e-6, 1 - 1e-6), a, b_p)\n",
    "\tpdf_S_full = np.nan_to_num(pdf_S_full, nan=0.0)\n",
    "\tpdf_N_full = np.nan_to_num(pdf_N_full, nan=0.0)\n",
    "\tnumerator = 0.5 * pdf_S_full\n",
    "\tposterior = numerator / (numerator + 0.5 * pdf_N_full + 1e-10)\n",
    "\n",
    "\treturn posterior, final_params\n",
    "\n",
    "\n",
    "def fitGaussianMixture(fused_statistic_matrix, n_components=2, initial_params=None):\n",
    "\tdata = _get_off_diagonal_samples(fused_statistic_matrix).reshape(-1, 1)\n",
    "\tgmm = GaussianMixture(\n",
    "\t\tn_components=n_components, covariance_type=\"full\", random_state=42\n",
    "\t)\n",
    "\tgmm.fit(data)\n",
    "\n",
    "\tsignal_idx = np.argmin(gmm.means_.flatten())\n",
    "\tprobs = gmm.predict_proba(fused_statistic_matrix.reshape(-1, 1))\n",
    "\tposterior = probs[:, signal_idx].reshape(fused_statistic_matrix.shape)\n",
    "\n",
    "\tparams = {\n",
    "\t\t\"means\": gmm.means_.tolist(),\n",
    "\t\t\"signal_idx\": int(signal_idx),\n",
    "\t\t\"weights\": gmm.weights_.tolist(),\n",
    "\t}\n",
    "\treturn posterior, params\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. TOPOLOGY & CLUSTERING\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def computeTransitivityPrior(likelihood_matrix):\n",
    "\traw_support = likelihood_matrix @ likelihood_matrix\n",
    "\tmax_val = np.max(raw_support)\n",
    "\treturn (raw_support / max_val) if max_val > 0 else raw_support\n",
    "\n",
    "\n",
    "# def computeTransitivityPrior(likelihood_matrix):\n",
    "# \t# 1. Only allow \"High Confidence\" bridges to propagate info\n",
    "# \t#    Zero out weak links before matrix multiplication\n",
    "# \tclean_likelihood = likelihood_matrix.copy()\n",
    "# \tclean_likelihood[clean_likelihood < 0.5] = 0.0\n",
    "\n",
    "# \t# 2. Compute the flow\n",
    "# \traw_support = clean_likelihood @ clean_likelihood\n",
    "\n",
    "# \t# 3. Normalize locally (don't just divide by global max)\n",
    "# \t#    We want the support to be relative to the number of paths\n",
    "# \t#    This prevents dense noisy clusters from self-amplifying\n",
    "# \trow_sums = np.sum(clean_likelihood, axis=1, keepdims=True) + 1e-9\n",
    "# \tnormalized_support = raw_support / row_sums\n",
    "\n",
    "# \treturn np.clip(normalized_support, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def computeFinalPosterior(likelihood_matrix, prior_matrix, weight_factor=0.5):\n",
    "\tsafe_prior = np.clip(prior_matrix, 0, 1.0)\n",
    "\treturn likelihood_matrix * (safe_prior**weight_factor)\n",
    "\n",
    "\n",
    "def extractAndPrintClusters(posterior_matrix, semantic_data, threshold=0.5):\n",
    "\tdist_matrix = np.clip(1.0 - posterior_matrix, 0.0, 1.0)\n",
    "\tmodel = AgglomerativeClustering(\n",
    "\t\tn_clusters=None,\n",
    "\t\tdistance_threshold=(1.0 - threshold),\n",
    "\t\tmetric=\"precomputed\",\n",
    "\t\tlinkage=\"complete\",\n",
    "\t)\n",
    "\tlabels = model.fit_predict(dist_matrix)\n",
    "\n",
    "\tgroups = {}\n",
    "\tfor idx, label in enumerate(labels):\n",
    "\t\tgroups.setdefault(label, []).append(idx)\n",
    "\n",
    "\tsignificant_groups = [g for g in groups.values() if len(g) > 1]\n",
    "\tsignificant_groups.sort(key=len, reverse=True)\n",
    "\n",
    "\tprint(f\"\\n{'='*80}\")\n",
    "\tprint(f\"PROBABILISTIC CLUSTERING RESULTS (Threshold P > {threshold})\")\n",
    "\tprint(f\"{'='*80}\")\n",
    "\tprint(f\"Found {len(significant_groups)} significant groups.\\n\")\n",
    "\n",
    "\tfor i, indices in enumerate(significant_groups):\n",
    "\t\tsub_probs = posterior_matrix[np.ix_(indices, indices)]\n",
    "\t\tmask = ~np.eye(len(indices), dtype=bool)\n",
    "\t\tavg_prob = np.mean(sub_probs[mask]) if len(indices) > 1 else 1.0\n",
    "\n",
    "\t\tprint(f\"GROUP {i+1} (Size: {len(indices)}) [Cohesion: {avg_prob:.4f}]\")\n",
    "\t\tfor idx in indices:\n",
    "\t\t\tprint(f\" - {semantic_data[idx]}\")\n",
    "\t\tprint(\"-\" * 80)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. MAIN EXECUTION\n",
    "# ==============================================================================\n",
    "\n",
    "cache = createClusteringCache(artifacts, tau_range)\n",
    "\n",
    "\n",
    "# consensus_pairs = findConsensusViaACGC(cache, model_keys)\n",
    "# consensus_data = findConsensusViaJaccard(cache, model_keys)\t# doesnt work\n",
    "\n",
    "consensus_data = findConsensusViaJaccard(cache, model_keys)\n",
    "print(consensus_data[\"optimal_taus\"])\n",
    "consensus_pairs = consensus_data[\"P_true\"]\n",
    "optimal_taus = consensus_data[\"optimal_taus\"]\n",
    "# gives AgglomerativeClustering does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
    "# importantly ACGC isnt the precise labels, just most consensus sets\n",
    "# print(f\"Consensus Anchors Found: {len(consensus_pairs)}\")\n",
    "\n",
    "\n",
    "# # --- STEP 1.5: Derive Strict Safety Horizon ---\n",
    "# safety_horizon = 20\n",
    "# safety_horizons = {}\n",
    "# # consensus_dict = {}\n",
    "# consensus_data = findConsensusViaJaccard(cache, model_keys)\t# doesnt work\n",
    "# safety_horizons = {}\n",
    "\n",
    "# for key in model_keys:\n",
    "\n",
    "# \tsafety_horizons[key] = consensus_dists[key][\"optimal_threshold\"]\n",
    "\n",
    "# # if consensus_dists:\n",
    "# # \tmax_consensus_dist = np.max(consensus_dists)\n",
    "\n",
    "# # \tsafety_horizon = max_consensus_dist * 1.5\n",
    "# # \tsafety_horizons\n",
    "# # \tprint(\n",
    "# # \t\tf\"Safety Horizon Derived: {safety_horizon:.4f} (Max Seed: {max_consensus_dist:.4f})\"\n",
    "# # \t)\n",
    "# # else:\n",
    "# # \t# Fallback if Jaccard finds nothing (rare)\n",
    "# # \tsafety_horizon = None\n",
    "# # \tprint(\"Warning: Consensus seeds. Safety Horizon disabled.\")\n",
    "\n",
    "# # --- STEP 2: Probabilistic Modeling ---\n",
    "print(\"\\n>>> Phase 2: Running EVT Pipeline...\")\n",
    "k_neighbors = 100\n",
    "\n",
    "rank_matrices = {}\n",
    "for key, artifact in artifacts.items():\n",
    "\t# PASS THE SAFETY HORIZON HERE\n",
    "\trank_matrices[key] = computeLocalDensityRanks(\n",
    "\t\tartifact[\"dist_matrix\"],\n",
    "\t\tk_neighbors=k_neighbors,\n",
    "\t\tmax_valid_dist=optimal_taus[key] * 2,\n",
    "\t)\n",
    "\tprint(f\"safety: {optimal_taus[key]* 2}\")\n",
    "\n",
    "fused_matrix = computeFusedEVTStatistic(rank_matrices)\n",
    "\n",
    "# Re-init params since the rank inputs have changed (gated items are now 1.0)\n",
    "# init_params = _initializeParametersViaConsensus(fused_matrix, consensus_pairs)\n",
    "# print(f\"Bootstrapped Params: {init_params}\")\n",
    "\n",
    "likelihood_matrix, final_params = fitWeibullBetaMixture(\n",
    "\tfused_matrix, initial_params=None\n",
    ")\n",
    "print(\"EM Model Converged.\")\n",
    "\n",
    "\n",
    "# --- STEP 3: Topology & Results ---\n",
    "print(\"\\n>>> Phase 3: Final Inference\")\n",
    "prior_matrix = computeTransitivityPrior(likelihood_matrix)\n",
    "posterior_matrix = computeFinalPosterior(likelihood_matrix, prior_matrix)\n",
    "\n",
    "\n",
    "final_matrix = posterior_matrix * prior_matrix\n",
    "# print(\"Applying Conservative Symmetrization...\")\n",
    "# final_matrix = np.minimum(final_matrix, final_matrix.T)\n",
    "# final_matrix = np.sqrt(final_matrix * final_matrix.T)\n",
    "extractAndPrintClusters(\n",
    "\tfinal_matrix, artifacts[model_keys[0]][\"semantic_data\"], threshold=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1f9a33",
   "metadata": {},
   "source": [
    "## Full rank cosine\n",
    "{'embedding_vector': np.float64(0.30000000000000004), 'retrieval_embedding_vector': np.float64(0.25000000000000006)}\n",
    "\n",
    ">>> Phase 2: Running EVT Pipeline...\n",
    "safety: 0.33000000000000007\n",
    "safety: 0.2750000000000001\n",
    "EM Model Converged.\n",
    "\n",
    ">>> Phase 3: Final Inference\n",
    "\n",
    "================================================================================\n",
    "PROBABILISTIC CLUSTERING RESULTS (Threshold P > 0.5)\n",
    "================================================================================\n",
    "Found 4 significant groups.\n",
    "\n",
    "GROUP 1 (Size: 3) [Cohesion: 0.4753]\n",
    " - Does the privacy policy affirm that the company collects information about the types of content the user views or engages with?\n",
    " - Does the privacy policy affirm that the company collects information about the features the user uses?\n",
    " - Does the privacy policy affirm that the company collects information about the actions the user takes?\n",
    "--------------------------------------------------------------------------------\n",
    "GROUP 2 (Size: 2) [Cohesion: 0.4782]\n",
    " - Does the privacy policy affirm that it describes practices regarding Personal Data collected when using the company's applications?\n",
    " - Does the privacy policy affirm that it describes practices regarding Personal Data collected when using the company's services?\n",
    "--------------------------------------------------------------------------------\n",
    "GROUP 3 (Size: 2) [Cohesion: 0.5094]\n",
    " - Does the privacy policy affirm that Personal Data is used to provide the company's services?\n",
    " - Does the privacy policy affirm that Personal Data is used to maintain the company's services?\n",
    "--------------------------------------------------------------------------------\n",
    "GROUP 4 (Size: 2) [Cohesion: 0.4884]\n",
    " - Does the privacy policy affirm that users have the right to receive confirmation on whether the company processes their data?\n",
    " - Does the privacy policy affirm that users have the right to know what personal data the company processes about them?\n",
    "--------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f05497d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding_vector: 0.36484206288831467\n",
      "retrieval_embedding_vector: 0.35864071718415347\n"
     ]
    }
   ],
   "source": [
    "for k, v in artifacts.items():\n",
    "\n",
    "\tprint(f\"{k}: {consensus_data[\"optimal_taus\"][k]/np.max(v[\"dist_matrix\"])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3426d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019230769230769232"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10 / 520"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "49a9c8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.983002656944028"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.35864071718415347 / 0.36484206288831467"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
